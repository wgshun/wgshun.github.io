<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>心安便是归处</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wgshun.github.io/"/>
  <updated>2019-04-10T18:04:58.158Z</updated>
  <id>http://wgshun.github.io/</id>
  
  <author>
    <name>王大顺</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hexo 提交命令时的错误</title>
    <link href="http://wgshun.github.io/2019/04/Hexo-s-a-bug/"/>
    <id>http://wgshun.github.io/2019/04/Hexo-s-a-bug/</id>
    <published>2019-04-10T17:42:43.000Z</published>
    <updated>2019-04-10T18:04:58.158Z</updated>
    
    <content type="html"><![CDATA[<p>参考：<a href="https://www.jianshu.com/p/738ebe02029b" target="_blank" rel="noopener">https://www.jianshu.com/p/738ebe02029b</a></p><h5 id="错误描述"><a href="#错误描述" class="headerlink" title="错误描述"></a>错误描述</h5><p>在执行<code>hexo s</code>命令时，报错如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">FATAL Something<span class="string">'s wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html</span></span><br><span class="line"><span class="string">Template render error: (unknown path) [Line 31, Column 46]</span></span><br><span class="line"><span class="string">  expected variable end</span></span><br></pre></td></tr></table></figure></p><a id="more"></a><h5 id="错误原因"><a href="#错误原因" class="headerlink" title="错误原因"></a>错误原因</h5><p>提交的文章中有两个大括号连在一起，而且不是在代码块里的情况。如下图：</p><img src="/2019/04/Hexo-s-a-bug/bug.jpg"><h5 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h5><p>在 <code>{ {</code> 或者 <code>} }</code> 中间加上空格键，别让上述原因里的情况出现就行了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考：&lt;a href=&quot;https://www.jianshu.com/p/738ebe02029b&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.jianshu.com/p/738ebe02029b&lt;/a&gt;&lt;/p&gt;
&lt;h5 id=&quot;错误描述&quot;&gt;&lt;a href=&quot;#错误描述&quot; class=&quot;headerlink&quot; title=&quot;错误描述&quot;&gt;&lt;/a&gt;错误描述&lt;/h5&gt;&lt;p&gt;在执行&lt;code&gt;hexo s&lt;/code&gt;命令时，报错如下：&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;FATAL Something&lt;span class=&quot;string&quot;&gt;&#39;s wrong. Maybe you can find the solution here: http://hexo.io/docs/troubleshooting.html&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;Template render error: (unknown path) [Line 31, Column 46]&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;string&quot;&gt;  expected variable end&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="博客技术" scheme="http://wgshun.github.io/categories/%E5%8D%9A%E5%AE%A2%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="hexo" scheme="http://wgshun.github.io/tags/hexo/"/>
    
      <category term="bug" scheme="http://wgshun.github.io/tags/bug/"/>
    
  </entry>
  
  <entry>
    <title>《PlaneRCNN-单幅图像的三维平面检测与重建》论文中英文对照解读</title>
    <link href="http://wgshun.github.io/2019/04/PlaneRCNN-translation/"/>
    <id>http://wgshun.github.io/2019/04/PlaneRCNN-translation/</id>
    <published>2019-04-10T16:40:40.000Z</published>
    <updated>2019-04-10T17:38:30.682Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/pdf/1812.04072.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1812.04072.pdf</a><br>代码地址：<a href="https://github.com/art-programmer/PlaneNet" target="_blank" rel="noopener">https://github.com/art-programmer/PlaneNet</a></p><hr><h3 id="PlaneRCNN-3D-Plane-Detection-and-Reconstruction-from-a-Single-Image"><a href="#PlaneRCNN-3D-Plane-Detection-and-Reconstruction-from-a-Single-Image" class="headerlink" title="PlaneRCNN: 3D Plane Detection and Reconstruction from a Single Image"></a>PlaneRCNN: 3D Plane Detection and Reconstruction from a Single Image</h3><h3 id="PlaneRCNN：单幅图像的三维平面检测与重建"><a href="#PlaneRCNN：单幅图像的三维平面检测与重建" class="headerlink" title="PlaneRCNN：单幅图像的三维平面检测与重建"></a>PlaneRCNN：单幅图像的三维平面检测与重建</h3><img src="/2019/04/PlaneRCNN-translation/planercnn1.png"><a id="more"></a><p><strong><em>EN:</em></strong> Figure 1. This paper proposes a deep neural architecture, PlaneRCNN, that detects planar regions and reconstructs a piecewise planar depthmap from a single RGB image. From left to right, an input image, segmented planar regions, estimated depthmap, and reconstructed planar surfaces.<br><strong><em>CH:</em></strong> 图1.本论文提出了一种深度神经结构 PlaneRCNN，它可以检测平面区域并从单个RGB图像重建分段平面深度图。 从左到右，依次为输入图像，分段平面区域，估计深度图和重建平面表面。</p><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><h4 id="论文摘要"><a href="#论文摘要" class="headerlink" title="论文摘要"></a>论文摘要</h4><p><strong><em>EN:</em></strong> This paper proposes a deep neural architecture, PlaneRCNN, that detects and reconstructs piecewise planar surfaces from a single RGB image. PlaneRCNN employs a variant of Mask R-CNN to detect planes with their plane parameters and segmentation masks. PlaneRCNN then jointly refines all the segmentation masks with a novel loss enforcing the consistency with a nearby view during training. The paper also presents a new benchmark with more finegrained plane segmentations in the ground-truth, in which, PlaneRCNN outperforms existing state-of-the-art methods with significant margins in the plane detection, segmentation, and reconstruction metrics. PlaneRCNN makes an important step towards robust plane extraction, which would have an immediate impact on a wide range of applications including Robotics, Augmented Reality, and Virtual Reality.<br><strong><em>CH:</em></strong> 本论文提出了一种深度神经网络结构 PlaneRCNN，它可以从单个RGB图像中检测和重建分段平面。PlaneRCNN 为了检测出平面的平面参数和分割掩膜而采用了 Mask R-CNN 的变种算法。然后，PlaneRCNN 联合细化所有的分割掩膜，在训练期间形成一个新的 loss，强制得与该 loss 就近的视图保持一致。本文还提出了一个新的基准用于在真实样本中能有更细粒度的平面分割；其中，在平面检测，平面分割，重建平面的指标上，PlaneRCNN 性能要远优于现有的最先进的方法。而且 PlaneRCNN 向成熟稳健的平面检测迈出了重要的一步，这将对包括机器人技术，增强现实技术和虚拟现实在内的广泛应用产生直接影响。</p><h5 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h5><h5 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h5><p><strong><em>EN:</em></strong> Planar regions in 3D scenes offer important geometric cues in a variety of 3D perception tasks such as scene understanding , scene reconstruction , and robot navigation. Accordingly, piecewise planar scene reconstruction has been a focus of computer vision research for many years, for example, plausible recovery of planar structures from a single image, volumetric piecewise planar reconstruction from point clouds, and Manhattan depthmap reconstruction from multiple images.<br><strong><em>CH:</em></strong> 3D场景中的平面区域为各种3D类的感知任务，例如场景理解，场景重建，和机器人导航等，提供了重要的几何信息；因此，分段平面场景重建一直是计算机视觉研究的焦点；比如，从单个图像中合理的恢复平面结构，从点云数据中进行体积分段平面重建，以及根据多个图像重建曼哈顿深度图。</p><p><strong><em>EN:</em></strong> A difficult yet fundamental task is the inference of a piecewise planar structure from a single RGB image, posing two key challenges. First, 3D plane reconstruction from a single image is an ill-posed problem, requiring rich scene priors. Second, planar structures abundant in man-made environments often lack textures, requiring global image understanding as opposed to local texture analysis. Recently, PlaneNet and PlaneRecover made a breakthrough by introducing the use of Convolutional Neural Networks (CNNs) and formulating the problem as a plane segmentation task. While generating promising results, they suffer from three major limitations: 1) Missing small surfaces; 2) Requiring the maximum number of planes in a single image a priori; and 3) Poor generalization across domains (e.g., trained for indoors images and tested outdoors).<br><strong><em>CH:</em></strong> 一个困难但基本的任务是从单个RGB图像中推断出来分段平面结构，这产生了两个关键的挑战。首先是：从单个图像重建3D平面是一个不稳定的命题，需要丰富的场景先验信息。其次是：在人造环境中丰富的平面结构通常缺乏质地纹理，需要全局图像理解而不是局部纹理分析。最近，PlaneNet 和 PlaneRecover 通过使用卷积神经网络（CNN）取得了突破性的进展，并将该问题描述为一个平面分割任务；虽然有了一些希望，但仍受到三个主要的制约：1.缺少小的平面，2.需要事先给出单个图像中的最大平面数，3.跨域的泛化能力比较差（比如室内图像的训练和室外的测试）。</p><p><strong><em>EN:</em></strong> This paper proposes a novel deep neural architecture, PlaneRCNN, that addresses these issues and more effectively infers piecewise planar structure from a single RGB image (Fig. 1). PlaneRCNN consists of three components.<br><strong><em>CH:</em></strong> 本论文提出了一种新型的深度神经网络结构-PlaneRCNN，它解决了以上问题，并能更加有效的从单个RGB图像中推断出该图像的分段平面结构（如图1）。PlaneRCNN由三部分组成。</p><p><strong><em>EN:</em></strong> The first component is a plane detection network built upon Mask R-CNN [14]. Besides an instance mask for each planar region, we also estimate the plane normal and perpixel depth values. With known camera intrinsics, we can further reconstruct the 3D planes from the detected planar regions. This detection framework is more flexible and can handle an arbitrary number of planar regions in an image. To the best of our knowledge, this paper is the first to introduce a detection network, common in object recognition, to the depthmap reconstruction task. The second component is a segmentation refinement network that jointly optimizes extracted segmentation masks to more coherently explain a scene as a whole. The refinement network is designed to handle an arbitrary number of regions via a simple yet effective neural module. The third component, the warping-loss module, enforces the consistency of reconstructions with another view observing the same scene during training and improves the plane parameter and depthmap accuracy in the detection network via end-to-end training.<br><strong><em>CH:</em></strong> 第一部分是建立在 Mask R-CNN 基础上的平面检测网络。除了每个平面区域的实例掩膜之外，我们还估计平面法线和每个像素的深度值。利用已知的相机内在函数，我们可以从检测到的平面区域进一步的重建3D平面。这个检测框架更加的灵活，可以处理一张图片里任意数量的平面区域。据我们所知，本论文是第一个将目标识别中常见的检测网络引入深度图重构任务中的。第二部分是一个分割细化网络，联合优化提取到的分割掩膜，以便于更加连贯一致的解释整个场景。这个细化网络通过设计一个简单但有效的神经模块来处理任意数量的平面。第三部分是翘曲损失模块，在训练中强制让同一个场景中不同视角观察的重建平面保持一致，在端到端的检测网络中提高平面参数和深度图的精度。</p><p><strong><em>EN:</em></strong> The paper also presents a new benchmark for the piecewise planar depthmap reconstruction task. We collected 100,000 images from ScanNet and generated the corresponding ground-truth by utilizing the associated 3D scans. The new benchmark offers 14.7 plane instances per image on the average, in contrast to roughly 6 instances per image in the existing benchmark.<br><strong><em>CH:</em></strong> 本论文还为分段平面深度图重建任务提出了一个新的基准。我们从 ScanNet （一个拥有标注过 3D 室内场景重构信息的大规模 RGB-D 数据集。）中搜集了100000张图像，并利用相关的3D扫描来生成对应的真实样本。这个新的基准平均每张图像可以提供14.7个平面实例，而现有的基准大致上一张图像才6个实例。</p><p><strong><em>EN:</em></strong>  The performance is evaluated via plane detection, segmentation, and reconstruction metrics, in which PlaneRCNN outperforms the current state-of-the-art with significant margins. Especially, PlaneRCNN is able to detect small planar surfaces and generalize well to new scene types.<br><strong><em>CH:</em></strong>  通过平面检测、分割、重建作为评估性能的标准，PlaneRCNN 的效果要优于当下最新的技术水平，特别是，PlaneRCNN 能检测到小的平面，并能很好地应用到新的场景类型。</p><p><strong><em>EN:</em></strong> The contributions of the paper are two-fold:<br>Technical Contribution: The paper proposes a novel neural architecture PlaneRCNN, where 1) a detection network extracts an arbitrary number of planar regions; 2) a refinement network jointly improves all the segmentation masks; and 3) a warping loss improves plane-parameter and depthmap accuracy via end-to-end training.<br>System Contribution: The paper provides a new benchmark for the piecewise planar depthmap reconstruction task with much more fine-grained annotations than before, in which PlaneRCNN makes significant improvements over the current state-of-the-art.<br><strong><em>CH:</em></strong>  本论文的主要贡献有两方面：<br>技术贡献：本论文提出了一个新的神经网络结构：PlaneRCNN，分为三个模块：1.检测神经网络用来检测任意数量的平面区域，2.细化网络联合优化提取到的分割掩膜，3.翘曲损失模块通过端到端的训练提高平面参数和深度图的精度。<br>系统贡献：本论文为分段平面深度图重建任务提供了一个更加精细的评估基准，并且 PlaneRCNN 在当下最先进技术的基础上取得了重大提升。</p><h5 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h5><h5 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h5><p><strong><em>EN:</em></strong> For 3D plane detection and reconstruction, most traditional approaches require multiple views or depth information as input. They generate plane proposals by fitting planes to 3D points, then assign a proposal to each pixel via a global inference. Deng et al. proposed a learning-based approach to recover planar regions, while still requiring depth information as input.<br><strong><em>CH:</em></strong> 对于3D平面检测及重建，大多数传统的处理方法需要多个视图或深度信息作为输入；他们将多个平面拟合成3D点来生成平面候选区域，然后通过一个全局推断来为每个像素点分配候选区域。Deng等人提出了一个基于学习的方法用来恢复平面区域，但该方法仍然需要深度信息的输入。</p><p><strong><em>EN:</em></strong> Recently, PlaneNet revisited the piecewise planar depthmap reconstruction problem with an end-to-end learning framework from a single indoor RGB image. PlaneRecover later proposed an un supervised learning approach for outdoor scenes. Both PlaneNet and PlaneRecover formulated the task as a pixel-wise segmentation problem with a fixed number of planar regions (i.e., 10 in PlaneNet and 5 in PlaneRecover), which severely limits the expressiveness of their reconstructions and generalization capabilities to different scene types. We address these limitations by utilizing a detection network, commonly used for object recognition.<br><strong><em>CH:</em></strong> 最近，PlaneNet 通过从单个的室内RGB图像得到的端到端的学习框架重新看待了分段平面深度图重建问题。PlaneRecover 后来提供了一种针对室外场景的非监督学习方法。PlaneRecover 和 PlaneNet 都将任务描述成一个有固定平面数量的像素分割问题，（i.e.：PlaneNet 的10个平面和 PlaneRecover 的5个平面）严重限制了不同场景类型重建和泛化能力的表现。我们用一般物体识别的检测网络处理了这些限制。</p><p><strong><em>EN:</em></strong> Detection-based framework has been successfully applied to many 3D understanding tasks for objects, for example, predicting object shapes in the form of bounding boxes, wire-frames, or template-based shape compositions. However, the coarse representation employed in these methods lack the ability to accurately model complex and cluttered indoor scenes.<br><strong><em>CH:</em></strong> 基于检测的框架现在已经成功得应用于许多3D理解得任务，比如：以边界框、线框、或基于组成模板得形状来预测物体形状。然而，这些方法中粗略得表示缺乏精确建模复杂混乱室内场景得能力。</p><p><strong><em>EN:</em></strong> In addition to the detection, joint refinement of segmentation masks is also a key to many applications that require precise plane parameters or boundaries. In recent semantic segmentation techniques, fully connected conditional random field (CRF) is proven to be effective for localizing segmentation boundaries. CRFasRNN further makes it differentiable for end-to-end training. CRF only utilizes low-level information, and global context is further exploited via RNNs, more general graphical models, or novel neural architectural designs. These segmentation refinement techniques are NOT instance-aware, merely inferring a semantic label at each pixel and cannot distinguish multiple instances belonging to the same semantic category.<br><strong><em>CH:</em></strong> 除了检测以外，分割掩膜得联合细化，对于一些需要精确平面参数和边界得应用来说，也很关键。在最近得语义分割技术中，全连接条件随机场（CRF）被证明了对于局部得分割边界是有效的，CRFasRNN 在端到端的训练中促进了它的可辨别性。CRF 使用的只是低级信息，通过 RNNs，更通用的图形模型，或者新型的神经架构设计进一步使用全局的上下文信息。这些分割细化技术不是实例感知的，仅仅是在每个像素上进行的推断，并不能区别多个实例属于同一个语义的种类。</p><p><strong><em>EN:</em></strong> Instance-aware joint segmentation refinement poses more challenges. Traditional methods model the scene as a graph and use graphical model inference techniques to jointly optimize all instance masks. With a sequence of heuristics, these methods are often not robust. To this end, we will propose a segmentation refinement network that jointly optimizes an arbitrary number of segmentation masks on top of a detection network.<br><strong><em>CH:</em></strong> 实例感知的联合分割细化造成了更多的挑战。传统的方法将场景当作图形来建模，然后使用图形的模型推理技术来联合优化所有的实例任务。通过一系列的启发式算法，这些方法经常表现得不够稳健。为此，我们提出了一个能在检测网络上联合优化任意数量分割掩码的分割细化网络。</p><h5 id="3-Approach"><a href="#3-Approach" class="headerlink" title="3. Approach"></a>3. Approach</h5><h5 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h5><p><strong><em>EN:</em></strong> PlaneRCNN consists of three main components (See Fig. 2): a plane detection network, a segmentation refinement network, and a warping loss module. Built upon Mask R-CNN, the plane proposal network (Sec. 3.1) detects planar regions given a single RGB image and predicts 3D plane parameters together with a segmentation mask for each planar region. The refinement network (Sec. 3.2) takes all detected planar regions and jointly optimizes their masks. The warping loss module (Sec. 3.3) enforces the consistency of reconstructed planes with another view observing the same scene to further improve the accuracy of plane parameters and depthmap during training.<br><strong><em>CH:</em></strong> PlaneRCNN 由三个模块构成：一个检测网络，一个分割细化网络和一个翘曲损失模块。基于 Mask R-CNN，这个平面候选网络（3.1节）给一个给定的RGB图像中检测平面区域，并且一起预测每个平面的3D平面参数和分割掩膜。这个细化网络（3.2节）拿到所有检测出来的平面区域联合优化它们的分割掩膜。翘曲损失模块（3.3节）通过在训练中强制保持同一场景观察到的不同视图的重建平面保持一致，进一步提高了平面参数和深度图的精度。</p><h5 id="3-1-Plane-Detection-Network"><a href="#3-1-Plane-Detection-Network" class="headerlink" title="3.1. Plane Detection Network"></a>3.1. Plane Detection Network</h5><h5 id="3-1-平面检测网络"><a href="#3-1-平面检测网络" class="headerlink" title="3.1. 平面检测网络"></a>3.1. 平面检测网络</h5><p><strong><em>EN:</em></strong> Mask R-CNN was originally designed for semantic segmentation, where images contain instances of varying categories (e.g., person, car, train, bicycle and more). Our problem has only two categories ”planar” or ”non-planar”, defined in a geometric sense. Nonetheless, Mask R-CNN works surprisingly well in detecting planes in our experiments. It also enables us to handle an arbitrary number of planes, where existing approaches need the maximum number of planes in an image a priori (i.e., 10 for PlaneNet and 5 for PlaneRecover ).<br><strong><em>CH:</em></strong> Mask R-CNN 是被设计用来进行图像语义分割的，这些图像包含不同类别的实例（e.g.person, car, train, bicycle and more）。我们的问题中只含有两个类别：平面和非平面，是在几何意义下的。尽管如此，在我们的实验中 Mask R-CNN 在检测平面的表现依然好得出奇。目前其他的方法需要事先给出一个图像平面的最大数量(i.e., PlaneNet需要输入十个，PlaneRecover需要输五个 )，Mask R-CNN 能让我们处理任意数量的平面。</p><img src="/2019/04/PlaneRCNN-translation/planercnn2.png"><p><strong><em>EN:</em></strong> Figure 2. Our framework consists of three building blocks: 1) a plane detection network based on Mask R-CNN, 2) a segmentation refinement network that jointly optimizes extracted segmentation masks, and 3) a warping loss module that enforces the consistency of reconstructions with a nearby view during training.<br><strong><em>CH:</em></strong> 图二：我们的框架由三部分组成：1）一个基于 Mask R-CNN 的平面检测网络，2）一个用来联合优化提取的分割掩膜的分割细化网络，3）一个用来在训练中强制保持就近视图的重建平面一致的翘曲损失模块。</p><p><strong><em>EN:</em></strong> We treat each planar region as an object instance and let Mask R-CNN detect such instances and estimate their segmentation masks. The remaining task is to infer 3D plane parameters, which consists of the normal and the offset information. While CNNs have been successful for depthmap and surface normal estimation, direct regression of plane offset turns out to be a challenge (even with the use of CoordConv). Instead of direct regression, we solve it in three steps: (1) predict a normal per planar instance, (2) estimate a depthmap for an entire image, and (3) use a simple algebraic formula (Eq. 1) to calculate the plane offset (which is differentiable for end-to-end training). We now explain how we modify Mask-RCNN to perform these three steps.<br><strong><em>CH:</em></strong> 我们将每个平面区域当作可检测的对象实例，让 Mask R-CNN 去检测并估算它们的分割掩膜。剩下的任务是推断3D平面参数，包括法线和偏移信息。虽然 CNN 已经成功的用于深度图和平面法线的估算，但平面偏移的直接回归结果仍是个挑战（即使使用 CoordConv），不使用直接回归，我们用三步来解决它：1）预测每个平面实例的法线，2）估算整个图像的深度图，3）使用简单的代数公式（Eq.1）去计算平面偏移（在端到端的训练中是可区分的）。现在就来解释如何修改 Mask R-CNN 来实现这三步。</p><p><strong><em>EN:</em></strong> Plane normal estimation: Directly attaching a parameter regression module after the ROI pooling produces reasonable results, but we borrow the idea of 2D anchor boxes for bounding box regression to further improve accuracy. More precisely, we consider anchor normals and estimate a plane normal in the local camera coordinate frame by 1) picking an anchor normal, 2) regressing the residual 3D vector, and 3) normalizing the sum to a unit-length vector.<br><strong><em>CH:</em></strong> 平面法线的估算：在 ROL pooling 层后面附加上一个参数回归模块得到一个相对合理的结果，除此之外，我们还针对边界框回归借用了2D anchor box 的思想来进一步提高准确率。更确切的说我们认为锚定法线和在局部相机坐标系中估算平面法线要通过三步：1）选择一个锚定法线，2）3D残差向量的回归，3）将和标准化为单位长度向量。</p><p><strong><em>EN:</em></strong> Anchor normals are defined by running the K-means clustering algorithm on the plane normals in 10, 000 randomly sampled training images. We use k = 7 and the cluster centers become anchor normals, which are up-facing, down-facing, and horizontal vectors roughly separated by 45◦ in our experiments (See Fig. 3).<br><strong><em>CH:</em></strong> 通过在10000个随机采样的训练图像的平面法线上运行 K-means 聚类算法确定了锚定法线。我们设置 k 值为7，聚类中心为锚定法线，在我们的实验中，它们的方向是向上，向下还有和水平向量的夹角大致为45°。（看图三）</p><p><strong><em>EN:</em></strong> We replace the object category prediction in the original Mask R-CNN with the anchor ID prediction, and append one separate fully connected layer to regress the 3D residual vector for each anchor normal (i.e., 21 = 3 × 7 output values). To generate supervision for each ground-truth plane normal, we find the closest anchor normal and compute the residual vector. We use the cross-entropy loss for the anchor normal selection, and the smooth L1 loss for the residual vector regression as in the bounding box regression of Mask R-CNN.<br><strong><em>CH:</em></strong> 我们将原来 Mask R-CNN 中的目标类别检测替换为现在的锚点 ID 预测，并且为了每个锚定法线的3D残差向量回归添加了一个单独的全连接层。（i.e.21=3x7输出值）为了每个真实样本平面法线生成的有效性，我们找到最靠近的锚定法线并计算残差向量。我们用交叉熵损失来进行锚定法线的选择，用 L1 损失来进行残差向量的回归，就像 Mask R-CNN 网络里 bounding box 的回归。</p><img src="/2019/04/PlaneRCNN-translation/planercnn3.png"><p><strong><em>EN:</em></strong> Figure 3. We estimate a plane normal by first picking one of the 7 anchor normals and then regressing the residual 3D vector. Anchor normals are defined by running the K-means clustering algorithm on the ground-truth plane normal vectors.<br><strong><em>CH:</em></strong> 图三：我们首先通过选取7个锚定法线中的一个来估算平面法线，然后回归3D残差向量。通过在真实样本的平面法线向量上运行 K-means 聚类算法确定锚定法线。</p><p><strong><em>EN:</em></strong> Depthmap estimation: While local image analysis per region suffices for surface normal prediction, global image analysis is crucial for depthmap inference. We add a decoder after the feature pyramid network (FPN) in Mask R-CNN to estimate the depthmap D for an entire image. For the depthmap decoder, we use a block of 3 × 3 convolution with stride 1 and 4 × 4 deconvolution with stride 2 at each level. Lastly, bilinear upsampling is used to generate a depthmap in the same resolution as the input image (640 × 640).<br><strong><em>CH:</em></strong> 深度图的估算：虽然局部图像分析的每个区域可以满足表面法线的预测，但全局图像分析对于深度图信息是很重要的。为了估算整个图像得深度图 D，我们在 Mask R-CNN 的 FPN（特征金字塔提取网络）后面加上了一个解码器。我们在这个深度图解码器的每一层都用了一个尺寸为 3x3，步长为 1 的卷积核和一个尺寸为 4x4，步长为 2 的卷积核。最后，利用双线性上采样的方法得到和输入图像尺寸（640 × 640）相同的深度图。</p><p><strong><em>EN:</em></strong> Plane offset estimation: Given a plane normal $n$, it is straightforward to estimate the plane offset $d$:</p><script type="math/tex; mode=display">d={ {\sum_{i}m_i(n^T(z_iK^{-1}x_i))}\over{\sum_{i}m_i} }</script><p>where $K$ is the 3 × 3 camera intrinsic matrix, $x_i$ is the $i_{th}$ pixel coordinate in a homogeneous representation, $z_i$ is its predicted depth value, and $m_i$ is an indicator variable which becomes 1 if the pixel belongs to the plane. The summation is over all the pixels in the image. Note that we do not have a loss on the plane offset parameter, which did not make differences in the results. However, the plane offset influences the warping loss module below.<br><strong><em>CH:</em></strong> 平面偏移估计：给定一个平面法线 $n$，简单的去估算平面偏移 $d$，公式如上；其中 $K$ 是尺寸为 3x3 的相机内置矩阵，$x_i$ 是齐次表示中的第$i_{th}$个像素坐标，$z_i$ 是预测出来的深度值，$m_i$ 是一个状态变量，就是如果这个像素属于这个平面就为 1。 <strong>这个总和要超过这个图像的所有像素。</strong> 注意，我们的平面偏移参数没有损失，不会对结果造成影响。但是平面偏移会对下面的翘曲损失产生影响。</p><h5 id="3-2-Segmentation-Refinement-Network"><a href="#3-2-Segmentation-Refinement-Network" class="headerlink" title="3.2. Segmentation Refinement Network"></a>3.2. Segmentation Refinement Network</h5><h5 id="3-2-分割细化网络"><a href="#3-2-分割细化网络" class="headerlink" title="3.2. 分割细化网络"></a>3.2. 分割细化网络</h5><p><strong><em>EN:</em></strong> The plane detection network predicts segmentation masks independently. The segmentation refinement network jointly optimizes all the masks, where the major challenge lies in the varying number of detected planes. One solution is to assume the maximum number of planes in an image, concatenate all the masks, and pad zero in the missing entries. However, this does not scale to a large number of planes, and is prone to missing small planes.<br><strong><em>CH:</em></strong> 平面检测网络独立检测分割掩膜。这个分割细化网络联合优化所有的掩膜，在这里主要的挑战是检测出来的平面数量不同。有一种解决方案是假定一张图像中平面的最大数量，然后联结所有的掩膜，把缺少的那部分用 0 填充。但是，这样做不能拓展到更多的平面，也容易丢失掉小的平面。</p><p><strong><em>EN:</em></strong> Instead, we propose a simple yet effective module, ConvAccu, based on the idea of non-local module. ConvAccu processes each plane segmentation mask represented in the entire image window with a convolution layer. We then calculate and concatenate the mean feature volumes over all the other planes at the same level before passing to the next level (See Fig. 2). This resembles the non-local module and can effectively aggregate information from all the masks. We built an U-Net architecture using ConvAccu modules with details illustrated in Appendix A.<br><strong><em>CH:</em></strong> 相反，我们基于 non-local 模块的思想。提出了一个简单并且有效的方法，ConvAccu。ConvAccu 用卷积层处理在整个图像中表示的每个平面分割掩膜。然后，我们计算并联结同一层面上的所有其他平面的平均特征量，并进入下一层面（见图二）。这种方式类似于 non-local 模块的思想，可以有效的聚合所有掩膜的信息。我们使用 ConvAccu 模块构造了 U-Net 结构，详细信息在附录A。</p><p><strong><em>EN:</em></strong> Refined plane masks are concatenated at the end and compared against ground-truth with a cross-entropy loss. Note that besides the plane mask, the refinement network also takes the original image, the union of all the other plane masks, the reconstructed depthmap (for planar and non-planar regions), and a 3D coordinate map for the specific plane as input. The target segmentation mask is generated on the fly during training by assigning a ground-truth mask with the largest overlap. Planes without any assigned ground-truth masks do not receive supervision.<br><strong><em>CH:</em></strong> 最后将精确的平面掩膜联结起来，使用交叉熵损失和真实样本进行比较。注意除了平面掩膜，分割细化网络还使用了原始图像，其他所有平面掩膜的并集，重建的深度图（对于平面和非平面区域），和特定平面的3D坐标图作为输入。在训练中通过分派有最大重叠的真实样本生成目标分割掩膜。没有分派真实样本掩膜的平面不会接受联结。</p><h5 id="3-3-Warping-Loss-Module"><a href="#3-3-Warping-Loss-Module" class="headerlink" title="3.3. Warping Loss Module"></a>3.3. Warping Loss Module</h5><h5 id="3-3-翘曲损失模块"><a href="#3-3-翘曲损失模块" class="headerlink" title="3.3. 翘曲损失模块"></a>3.3. 翘曲损失模块</h5><p><strong><em>EN:</em></strong> The warping loss module enforces the consistency of reconstructed 3D planes with a nearby view during training. Specifically, our training samples come from RGB-D videos in ScanNet, and the nearby view is defined to be the one 20 frames ahead from the current. The module first builds a depthmap for each frame by 1) computing depth values from the plane equations for planar regions and 2) using pixel-wise depth values predicted inside the plane detection network for the remaining pixels. Depthmaps are converted to 3D coordinate maps in the local camera coordinate frames (i.e., a 3D coordinate instead of a depth value per pixel) by using the camera intrinsic information.<br><strong><em>CH:</em></strong> 翘曲损失模块在训练期间强制使附近视图的重建3D平面保持一致。我们的训练样本是 ScanNet 中的 RGB-D 视频，这个附近视图被明确规定为是当下一帧的前二十帧。这个模块首先通过两点（1.从平面等式中计算平面区域的深度值，2.在平面检测网络里给剩余的像素点用像素点的深度信息预测）给每一帧图像构造一个深度图。在局部相机坐标系中（i.e.每个像素点的3D坐标而不是深度信息），使用相机的内在信息，将深度图转化为3D坐标图。</p><p><strong><em>EN:</em></strong> The warping loss is then computed as follows. Let $M_c$ and $M_n$ denote the 3D coordinate maps of the current and the nearby frames, respectively. For every 3D point $P_n$(∈ $M_n$) in the nearby view, we use the camera pose information to project to the current frame, and use a bilinear interpolation to read the 3D coordinate $P_c$ from $M_c$. We then transform $P_c$ to the coordinate frame of the nearby view based on the camera pose and compute the 3D distance between the transformed coordinate $P^t_c$ and $P_n$. L2 norm of all such 3D distances divided by the number of pixels is the loss. We ignore pixels that project outside the current image frame during bilinear interpolation.<br><strong><em>CH:</em></strong> 然后如下计算这个翘曲损失模块。让 $M_c$ 和 $M_n$ 分别表示当前帧和临近帧的3D坐标图。对于每个临近视图中的3D点 $P_n$(∈ $M_n$)，我们使用相机姿态信息来投射到当前帧，并且使用双线性插值从 $M_c$ 中读取3D坐标 $P_c$ 。然后我们基于相机姿态将 $P_c$ 转换到临近视图的坐标系，并且计算变换后的坐标 $P^t_c$ 和 $P_n$ 的3D距离。所有这种3D距离的 L2 范数除以像素点的数量就是这个损失。我们忽略在双线性插值期间投射到当前帧之外的像素点。</p><p><strong><em>EN:</em></strong> The projection, un-projection, and coordinate frame transformation are all simple algebraic operations, whose gradients can be passed for training. Note that the warping loss module and the nearby view is utilized only during training to boost geometric reconstruction accuracy, and the system runs on a single image at test time.<br><strong><em>CH:</em></strong> 投射，非投射和坐标系变换都是简单的代数运算，梯度是通过训练的。注意这个翘曲损失模块和临近视图仅仅是在训练中为了提高几何重建精度而使用的，而且，这个系统在测试的时候是在单个图像上运行的。</p><h5 id="4-Benchmark-construction"><a href="#4-Benchmark-construction" class="headerlink" title="4. Benchmark construction"></a>4. Benchmark construction</h5><h5 id="4-基准构造"><a href="#4-基准构造" class="headerlink" title="4. 基准构造"></a>4. 基准构造</h5><p><strong><em>EN:</em></strong> Following steps described in PlaneNet, we build a new benchmark from RGB-D videos in ScanNet. We add the following three modifications to recover more fine-grained planar regions, yielding 14.7 plane instances per image on the average, which is more than double the PlaneNet dataset containing 6.0 plane instances per image.<br><strong><em>CH:</em></strong> 根据在 PlaneNet 中的步骤描述，我们从 ScanNet 的 RGB-D 视频中构造了一个新的基准。我们加了如下三个修改来恢复更细粒度的平面区域，平均每张图有 14.7 个平面实例，比 PlaneNet 中提到的每张图的 6 个平面的二倍还要多。    </p><p><strong><em>EN:</em></strong> First, we keep more small planar regions by reducing the plane area threshold from 1% of the image size to 0.16% (i.e., 500 pixels) and not dropping small planes when the total number is larger than 10.<br>Second, PlaneNet merges co-planar planes into a single region as they share the same plane label. The merging of two co-planar planes from different objects causes loss of semantics. We skip the merging process and keep all instance segmentation masks.<br>Third, the camera pose quality in ScanNet degrades in facing 3D tracking failures, which causes misalignment between image and the projected ground-truth planes. Since we use camera poses and aligned 3D models to generate ground-truth planes, we detect such failures by the discrepancy between our ground-truth 3D planes and the raw depthmap from a sensor. More precisely, we do not use images if the average depth discrepancy over planar regions is larger than 0.1m. This simple strategy removes approximately 10% of the images.<br><strong><em>CH:</em></strong> 第一：我们通过将平面区域的阈值从图片尺寸的 1% 降低到了 0.16%（i.e.500个像素点）来保留更多的小平面区域，并且当平面数大于10时，不丢弃小的平面。<br>第二：PlaneNet 把一些共平面合并为一个单独的平面为了让它们有相同的平面类别。    这种从不同实例合并平面的方式会丢失掉语义信息。我们跳过了这个合并过程，保留了所有的实例分割掩膜。<br>第三：ScanNet 里的相机姿态质量在3D追踪失败的时候会降低，这就导致了图像与真实实例平面的投射会发生不重合的情况。自从我们使用相机姿态和对齐的3D模型来生成真实实例平面，我们通过比较生成的真实实例平面和来自传感器的原始深度图的差异来检测这种失败。更确切的来说，如果在平面区域上的平均深度差值大于 0.1m 我们就不会使用这张图片。这个简单的策略大约可以删除 10% 的图片。</p><h5 id="5-Experimental-results"><a href="#5-Experimental-results" class="headerlink" title="5. Experimental results"></a>5. Experimental results</h5><h5 id="5-实验结果"><a href="#5-实验结果" class="headerlink" title="5. 实验结果"></a>5. 实验结果</h5><p><strong><em>EN:</em></strong> We have implemented our network in PyTorch. We use pre-trained Mask R-CNN and initialize the segmentation refinement network with the existing model. We train the network end-to-end on an NVIDIA TitanX GPU for 10 epochs with 100,000 randomly sampled images from training scenes in ScanNet. We use the same scale factor for all losses. For the detection network, we scale the image to 640 × 480 and pad zero values to get a 640 × 640 input image. For the refinement network, we scale the image to 256 × 192 and align the detected instance masks with the image based on the predicted bounding boxes.<br><strong><em>CH:</em></strong> 我们已经用 PyTorch 实现了我们的网络结构。我们使用 Mask R-CNN 的预训练模型进行训练，并使用现有的模型初始化分割细化网络。我们使用 ScanNet 训练场景里的随机100,000张实例图片在 NVIDIA TitanX GPU 上进行了十轮端到端的训练。我们对所有的 loss 采用同样的比例系数。对于检测网络，我们将图像缩放成 640 × 480 的尺寸，并用 0 填充成 640 × 640 的尺寸用作网络输入。对于分段网络，我们将图像缩放成 256 × 192 的尺寸，并基于预测的 bounding boxes 将检测到的实例掩膜和图像对齐。</p><img src="/2019/04/PlaneRCNN-translation/planercnn4.png"><p><strong><em>EN:</em></strong> Figure 4. Plane-wise accuracy against baselines. PlaneRCNN outperforms all the competing methods except when the depth threshold is very small and MWS-G can fit 3D planes extremely accurately by utilizing the ground truth depth values.<br><strong><em>CH:</em></strong> 图四：平面检测的基准对比。PlaneRCNN 要优于所有的竞赛方法，除了当深度阈值非常小的时候 MWS-G 能利用真实实例深度值非常准确的拟合3D平面。</p><h5 id="5-1-Qualitative-evaluations"><a href="#5-1-Qualitative-evaluations" class="headerlink" title="5.1. Qualitative evaluations"></a>5.1. Qualitative evaluations</h5><h5 id="5-1-实验评估"><a href="#5-1-实验评估" class="headerlink" title="5.1. 实验评估"></a>5.1. 实验评估</h5><p><strong><em>EN:</em></strong> Fig. 5 demonstrates our reconstructions results for ScanNet testing scenes. PlaneRCNN is able to recover planar surfaces even for small objects. We include more examples in Appendix B.<br><strong><em>CH:</em></strong>  图五：在 ScanNet 的测试场景上演示我们模型的3D重建结果。PlaneRCNN 即便是小的实例也能恢复平面信息。在附录B中有更多的示例。</p><img src="/2019/04/PlaneRCNN-translation/planercnn5.png"><p><strong><em>EN:</em></strong> Figure 5. Piecewise planar reconstruction results by PlaneRCNN.<br><strong><em>CH:</em></strong> 图五：PlaneRCNN 的分段平面重建结果。</p><p><strong><em>EN:</em></strong> Fig. 6 compares PlaneRCNN against two competing methods, PlaneNet and PlaneRecover, on a variety of scene types from unseen datasets (except the SYNTHIA dataset is used for training by PlaneRecover). Note that PlaneRCNN and PlaneNet are trained on the ScanNet which contains indoor scenes, while PlaneRecover is trained on the SYNTHIA dataset (i.e., the 7th and 8th rows in the figure) which consist of synthetic outdoor scenes. The figure shows that PlaneRCNN is able to reconstruct most planes in varying scene types from unseen datasets regardless of their sizes, shapes, and textures. In particular, our results on the KITTI dataset are surprisingly better than PlaneRecover for planes close to the camera. In indoor scenes, our results are consistently better than both PlaneNet and PlaneRecover. We include more examples in Appendix B.<br><strong><em>CH:</em></strong>  图六：在未知的数据集的各种场景类型下（除了 SYNTHIA 数据集被 PlaneRecover 用来训练）比较 PlaneRCNN 和两个竞赛方法（PlaneNet 和 PlaneRecover）的检测效果。注意 PlaneRCNN 和PlaneNet 是在 ScanNet 的室内数据集训练的，PlaneRecover 是用SYNTHIA 室外数据集训练的。（i.e.在图的第七行和第八行）这个图展示的是在未知的数据集上，不管它们的尺寸，形状和内容，PlaneRCNN 都能够重建不同场景类型的大部分平面。特别是，在 KITTI 数据集上，靠近相机的平面，我们的结果要出乎意料的比 PlaneRecover 的效果好。在室内的场景中，我们的效果也比 PlaneNet 和 PlaneRecover都要好。在附录B中有更多的示例。</p><img src="/2019/04/PlaneRCNN-translation/planercnn6.png"><p><strong><em>EN:</em></strong> Figure 6. Plane segmentation results on unseen datasets without fine-tuning.<br><strong><em>CH:</em></strong> 图六：未知的数据集的识别结果。</p><h5 id="5-2-Plane-reconstruction-accuracy"><a href="#5-2-Plane-reconstruction-accuracy" class="headerlink" title="5.2. Plane reconstruction accuracy"></a>5.2. Plane reconstruction accuracy</h5><h5 id="5-2-平面重建准确率"><a href="#5-2-平面重建准确率" class="headerlink" title="5.2. 平面重建准确率"></a>5.2. 平面重建准确率</h5><p><strong><em>EN:</em></strong> Following PlaneNet, we evaluate plane detection accuracy by measuring the plane recall with a fixed Intersection over Union (IOU) threshold 0.5 and a varying depth error threshold (from 0 to 1m with an increment of 0.05m). The accuracy is measured inside the overlapping regions between the ground-truth and inferred planes. Besides PlaneNet, we compare against Manhattan World Stereo (MWS), which is the most competitive tradi-tional MRF-based approach as demonstrated in prior evaluations. MWS requires a 3D point cloud as an input, and we either use the point cloud from the groundtruth 3D planes (MWS-G) or the point cloud inferred by our depthmap estimation module in the plane detection network (MWS). PlaneRecover was originally trained with the assumption of at most 5 planes in an image. We find it difficult to train PlaneRecover successfully for cluttered indoor scenes by simply increasing the threshold. We believe that PlaneNet, which is explicitly trained on ScanNet, serves as a stronger competitor for the evaluation.<br><strong><em>CH:</em></strong> 在 PlaneNet 之后，我们通过测量固定 IOU 阈值为0.5和非固定深度误差阈值（0-1m，增量为0.05m）的平面召回量来评判平面检测的准确性。这个准确性是在真实样本和检测出样本的重叠区域下计算测量的。除了 PlaneNet 之外，我们还和 Manhattan World Stereo (MWS) 进行了比较，作为之前评估演示的，这是最具竞争力的传统的 MRF 算法。MWS 需要3D点云数据作为输入，我们可以用真实样本的3D点云数据，也可以用我们平面检测网络中的深度图估计模块推算的点云数据。PlaneRecover 最初训练的是假设一张图5个平面。我们发现很难通过简单的提高阈值来使 PlaneRecover 在杂乱无章的室内场景中训练成功。我们相信，在这个评估中，在 ScanNet 上明确训练的 PlaneNet 可以作为一个强劲的对手。</p><p><strong><em>EN:</em></strong> As demonstrated in Fig. 4, PlaneRCNN significantly outperforms all other methods, except when the depth threshold is small and MWS-G can fit planes extremely accurately with the ground-truth depth values. Nonetheless, even with ground-truth depth information, MWS-G fails in extracting planar regions robustly, leading to lower recalls in general. Our results are superior also qualitatively as shown in Fig. 7<br><strong><em>CH:</em></strong> 如图四所示，PlaneRCNN 已经显著的优于其他所有的方法，除了当深度阈值非常小的时候 MWS-G 能利用真实实例深度值非常准确的拟合3D平面。但是，即使有真实样本的深度信息，MWS-G 还是失败在平面区域提取上，导致一般情况下召回率的降低。我们的结果是更优的，如图七所示。</p><img src="/2019/04/PlaneRCNN-translation/planercnn7.png"><p><strong><em>EN:</em></strong> Figure 7. Plane segmentation comparisons. From left to right: input image, MWS with inferred depths, MWS with ground-truth depths, PlaneNet, Ours, and ground-truth.<br><strong><em>CH:</em></strong> 图七：平面分割对比。</p><h5 id="5-3-Geometric-accuracy"><a href="#5-3-Geometric-accuracy" class="headerlink" title="5.3. Geometric accuracy"></a>5.3. Geometric accuracy</h5><h5 id="5-3-几何精确度"><a href="#5-3-几何精确度" class="headerlink" title="5.3. 几何精确度"></a>5.3. 几何精确度</h5><p><strong><em>EN:</em></strong> We propose a new metric in evaluating the quality of piecewise planar surface reconstruction by mixing the inferred depthmaps and the ground-truth plane segmentations. More precisely, we first generate a depthmap from our reconstruction by following the process in the warping loss evaluation (Sec. 3.3). Next, for every ground-truth planar segment, we convert depth values in the reconstructed depthmap to 3D points, fit a 3D plane by SVD, and normalize the plane coefficients to make the normal component into a unit vector. Finally, we compute the mean and the area-weighted mean of the parameter differences to serve as the evaluation metrics. Besides the plane parameter metrics, we also consider depthmap metrics commonly used in the literature. We evaluate over the NYU dataset for a fair comparison. Table 1 shows that, with more flexible detection network, PlaneRCNN generalizes much better without fine-tuning. PlaneRCNN also outperforms PlaneNet in every metric after fine tuning using the ground-truth depths from the NYU dataset.<br><strong><em>CH:</em></strong> 我们提出了一个新的方法，来评估分段平面重建的好坏，即通过联合检测的深度图和真实样本的平面分割。更准确的说，我们首先通过翘曲损失评估的过程，从重建中生成深度图；然后，对于每个真实样本平面片段，我们把重建的深度图中的深度值转换成3D点，通过 SVD 拟合3D平面，通过归一化平面系数将法向量变成单位向量；最后，我们计算参数差异的均值和面积加权均值作为评估指标。除了这个平面参数标准以外，我们也使用论文中常用的深度图评估标准。为了一个公平的比较环境，我们在 NYU 的数据集上进行评判。表1显示：通过更灵活的检测网络，PlaneRCNN 在不经过 fine-tuning 的情况下也有比较好的泛化性。PlaneRCNN 在经过 NYU 的数据集样本的 fine-tuning 训练后在每个评估指标上都要优于PlaneNet。</p><img src="/2019/04/PlaneRCNN-translation/planercnn8.png"><h5 id="5-4-Ablation-studies"><a href="#5-4-Ablation-studies" class="headerlink" title="5.4. Ablation studies"></a>5.4. Ablation studies</h5><h5 id="5-4-模型简化测试"><a href="#5-4-模型简化测试" class="headerlink" title="5.4. 模型简化测试"></a>5.4. 模型简化测试</h5><p><strong><em>EN:</em></strong> PlaneRCNN adds the following components to the Mask R-CNN backbone: 1) the pixel-wise depth estimation network; 2) the anchor based plane normal regression; 3) the warping loss module; and 4) the segmentation refinement network. Contribution of each component, we measure performance changes while adding the components one by one. Following, we evaluate the plane segmentation quality by three clustering metrics: variation of information (VOI), Rand index (RI), and segmentation covering ( SC). To further assess the geometric accuracy, we compute the average precision (AP) with IOU threshold 0.5 and three different depth error thresholds [0.4m, 0.6m, 0.9m]. A larger value means higher quality for all the metrics except For VOI.<br><strong><em>CH:</em></strong> PlaneRCNN 在 Mask R-CNN 的主干网络里添加了以下组件：1）像素点的深度估计网络，2）以锚点为基础的平面法线回归，3）翘曲损失模块，4）分割细化网络。为了明确每个组件的贡献程度，我们通过一个一个的添加这些组件，然后测量比较性能的改变。接着，我们评估这个分割的好坏通过三个指标：信息差异指标（VOI），兰德指数（RI）和分割覆盖（SC）。为了更进一步的评估几何精度，我们用 IOU 阈值为0.5和三个不同的深度误差阈值[0.4m, 0.6m, 0.9m]来计算平均精度（AP）。值越大意味着除了VOI之外所有的指标的可参考度越高。</p><img src="/2019/04/PlaneRCNN-translation/planercnn9.png"><p><strong><em>EN:</em></strong> Figure 8. Effects of the segmentation refinement network and the warping loss module. Top: the refinement network narrows the gap between adjacent planes. Bottom: the warping loss helps to correct erroneous plane geometries using the second view.<br><strong><em>CH:</em></strong> 图八：分割细化网络和翘曲损失模块的影响度</p><p><strong><em>EN:</em></strong> Table 2 shows that all the components have a positive contribution to the final performance. Fig. 8 further highlights the contributions of the warping loss module and the segmentation refinement network qualitatively. The first example shows that the segmentation refinement network fills in gaps between adjacent planar regions, while the second example shows that the warping loss module improves reconstruction accuracy with the help from the second view.<br><strong><em>CH:</em></strong> 表2展示了所有的组件对于模型最终性能的积极贡献。图八进一步突出了分割细化网络和翘曲损失模块的贡献。第一个例子表明分割细化网络可以更好的填充相邻平面之间的间隙，第二个例子表明翘曲损失模块在第二个视图的帮助下提高了重建任务的精度。</p><img src="/2019/04/PlaneRCNN-translation/planercnn10.png"><p><strong><em>EN:</em></strong> Table 2. Ablation studies on the contributions of the four components in PlaneRCNN. Plane segmentation and detection metrics are calculated over the ScanNet dataset. PlaneNet represents the competing state-of-the-art.<br><strong><em>CH:</em></strong> 表二。关于 PlaneRCNN 中四个组件的模型简化测试。平面分割和检测指标是在 ScanNet 数据集上计算的。PlaneNet 代表的是最先进的技术水平。</p><h5 id="5-5-Occlusion-reasoning"><a href="#5-5-Occlusion-reasoning" class="headerlink" title="5.5. Occlusion reasoning"></a>5.5. Occlusion reasoning</h5><h5 id="5-5-遮挡推理"><a href="#5-5-遮挡推理" class="headerlink" title="5.5. 遮挡推理"></a>5.5. 遮挡推理</h5><p><strong><em>EN:</em></strong> A simple modification allows PlaneRCNN to infer occluded/invisible surfaces and reconstruct layered depthmap models. We add one more mask prediction module to PlaneRCNN to infer the complete mask for each plane instance.<br><strong><em>CH:</em></strong> 一个简单的改变可以让 PlaneRCNN 推断出被遮挡的平面并且重建分层的深度图模型。我们向 PlaneRCNN 中添加了一个掩膜预测模块，为了推断出每个平面实例的完整掩膜。</p><p><strong><em>EN:</em></strong> The key challenge for training the network with occlu-sion reasoning is to generate ground-truth complete mask for supervision. In our original process, we fit planes to aligned 3D scans to obtain ground-truth 3D planar surfaces, then rasterize the planes to an image with a depth testing. We remove the depth testing and generate a “complete mask” for each plane. Besides disabling depth checking, we further complete the mask for layout structures based on the fact that layout planes are behind other geometries. First, we collect all planes which have layout labels (e.g., wall and floor), and compute the convexity and concavity between two planes in 3D space. Then for each combination of these planes, we compute the corresponding complete depthmap by using the greater depth value for two convex planes and using the smaller value for two concave ones. A complete depthmap is valid if 90% of the complete depthmap is behind the visible depthmap (with 0.2m tolerance to handle noise). We pick the valid complete depthmap which has the most support from visible regions of layout planes.<br><strong><em>CH:</em></strong> 训练含有遮挡推理网络的关键挑战是监督生成真实实例的完整掩膜。在我们的原始过程中，我们拟合被校准3D扫描的平面获得真实实例的3D平面，然后用深度测试将平面栅格化为图像。我们移除每个平面的深度测试并且生成一个完整的掩膜。除了禁用深度测试以外，我们基于布局平面落后于其他的几何体的事实进一步的完善了用于布局平面的掩膜。首先，我们收集所有具有布局标签的平面（e.g.墙和门），并且计算两个平面在3D空间的凹凸度。然后对于这些平面的每种组合，我们通过对凸面平面用较大的深度值，对凹面平面用较小的深度值， 来计算对应的完整的深度图。如果90%的完整深度图在可见深度图之后（噪声容差为0.2m），那么这个完整深度图是有效的。我们挑选的有效完整深度图是布局平面的可见区域最支持的。</p><p><strong><em>EN:</em></strong> Fig. 9 shows the new view synthesis examples, in which the modified PlaneRCNN successfully infers occluded surfaces, for example, floor surfaces behind tables and chairs. Note that a depthmap is rendered as a depth mesh model (i.e., a collection of small triangles) in the figure. The layered depthmap representation enables new applications such as artifacts-free view synthesis, better scene completion, and object removal. This experiment demonstrates yet another flexibility and potential of the proposed PlaneRCNN architecture.<br><strong><em>CH:</em></strong> 图九展示了新视图的合成例子，其中修改过后的 PlaneRCNN 成功的推测出了被遮挡的平面，例如桌子和椅子后面的地板表面。注意深度图被渲染成图中的深度网格模型（i.e.小三角形的集合）。这个分层的深度图显示可以实现新的应用，例如不掺杂人工的视图合成，更好的场景完善和目标移除。这个实验还验证了提出的 PlaneRCNN 架构的其他灵活性和潜能。</p><img src="/2019/04/PlaneRCNN-translation/planercnn11.png"><p><strong><em>EN:</em></strong> Figure 9. New view synthesis results with the layered depthmap models. A simple modification allows PlaneRCNN to also infer occluded surfaces and reconstruct layered depthmap models.<br><strong><em>CH:</em></strong> 图九：用分层深度图模型的新视图合成结果。一个简单的改变可以让 PlaneRCNN 推断出被遮挡的平面并且重建分层的深度图模型。</p><h5 id="6-Conclusion-and-future-work"><a href="#6-Conclusion-and-future-work" class="headerlink" title="6. Conclusion and future work"></a>6. Conclusion and future work</h5><h5 id="6-结论及未来的展望"><a href="#6-结论及未来的展望" class="headerlink" title="6. 结论及未来的展望"></a>6. 结论及未来的展望</h5><p><strong><em>EN:</em></strong> This paper proposes PlaneRCNN, the first detection based neural network for piecewise planar reconstruction from a single RGB image. PlaneRCNN learns to detect planar regions, regress plane parameters and instance masks, globally refine segmentation masks, and utilize a neighboring view during training for a performance boost. PlaneRCNN outperforms competing methods by a large margin based on our new benchmark with fine-grained plane annotations. An interesting future direction is to process an image sequence during inference which requires learning correspondences between plane detections.<br><strong><em>CH:</em></strong> 本篇论文提出的 PlaneRCNN，是第一个用于单张RGB图像检测并分段重建3D平面的神经网络。PlaneRCNN 学习检测平面区域，平面参数回归，实例化掩膜，全局细化分割掩膜，以及在训练中利用临近视图来提高最终的性能。PlaneRCNN 在我们基于细粒度平面注释的新基准上，大幅的超越其他的竞赛方法。未来一个有趣的方向是在平面检测中需要学习呼应的推断期间去处理图像序列。</p><h4 id="Appendices"><a href="#Appendices" class="headerlink" title="Appendices"></a>Appendices</h4><h4 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h4><p><strong><em>EN:</em></strong> Refinement of the Network Architecture In Figure 10, we illustrate the detailed architecture of the segmentation refinement network to support the descriptions shown in Figures 2 and 2. 3.2.<br><strong><em>CH:</em></strong> 在图十中详细说明了分割细化网络的详细体系结构。</p><img src="/2019/04/PlaneRCNN-translation/planercnn12.png"><p><strong><em>EN:</em></strong> Figure 10. Refinement network architecture. The network takes both global information (i.e., the input image, the reconstructed depthmap and the pixel-wise depthmap) and instance-specific information (i.e., the instance mask, the union of other masks, and the coordinate map of the instance) as input and refines instance mask with a U-Net rchitecture. Each convolution in the encoder is replaced by a ConvAccu module to accumulate features from other masks.<br><strong><em>CH:</em></strong> 图十：细化网络的结构。这个网络同时接受全局信息（i.e.输入图片，重建的深度图和像素级的深度图）和特定实例的信息（I.e.实例化掩膜，其他掩膜的并集，实例的坐标图）作为输入，并且用一个 U-Net 结构细化实例掩膜。编码器中的每个卷积都被 ConvAccu 模块所取代，为了从其他掩膜中积累特征。</p><p><strong><em>EN:</em></strong> More qualitative results. We show more qualitative results of our method, PlaneRCNN, on the test scenes from ScanNet in Fig. 11 and Fig. 12.<br><strong><em>CH:</em></strong> 更多的识别结果。</p><img src="/2019/04/PlaneRCNN-translation/planercnn13.png"><img src="/2019/04/PlaneRCNN-translation/planercnn14.png">]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1812.04072.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1812.04072.pdf&lt;/a&gt;&lt;br&gt;代码地址：&lt;a href=&quot;https://github.com/art-programmer/PlaneNet&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/art-programmer/PlaneNet&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;PlaneRCNN-3D-Plane-Detection-and-Reconstruction-from-a-Single-Image&quot;&gt;&lt;a href=&quot;#PlaneRCNN-3D-Plane-Detection-and-Reconstruction-from-a-Single-Image&quot; class=&quot;headerlink&quot; title=&quot;PlaneRCNN: 3D Plane Detection and Reconstruction from a Single Image&quot;&gt;&lt;/a&gt;PlaneRCNN: 3D Plane Detection and Reconstruction from a Single Image&lt;/h3&gt;&lt;h3 id=&quot;PlaneRCNN：单幅图像的三维平面检测与重建&quot;&gt;&lt;a href=&quot;#PlaneRCNN：单幅图像的三维平面检测与重建&quot; class=&quot;headerlink&quot; title=&quot;PlaneRCNN：单幅图像的三维平面检测与重建&quot;&gt;&lt;/a&gt;PlaneRCNN：单幅图像的三维平面检测与重建&lt;/h3&gt;&lt;img src=&quot;/2019/04/PlaneRCNN-translation/planercnn1.png&quot;&gt;
    
    </summary>
    
      <category term="计算机视觉" scheme="http://wgshun.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="平面重建" scheme="http://wgshun.github.io/tags/%E5%B9%B3%E9%9D%A2%E9%87%8D%E5%BB%BA/"/>
    
      <category term="平面检测" scheme="http://wgshun.github.io/tags/%E5%B9%B3%E9%9D%A2%E6%A3%80%E6%B5%8B/"/>
    
      <category term="深度图重建" scheme="http://wgshun.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%9B%BE%E9%87%8D%E5%BB%BA/"/>
    
      <category term="PlaneRCNN" scheme="http://wgshun.github.io/tags/PlaneRCNN/"/>
    
  </entry>
  
  <entry>
    <title>《PlaneNet-单幅RGB图像的分段平面重建》论文中英文对照解读</title>
    <link href="http://wgshun.github.io/2019/04/PlaneNet-translation/"/>
    <id>http://wgshun.github.io/2019/04/PlaneNet-translation/</id>
    <published>2019-04-10T16:03:00.000Z</published>
    <updated>2019-04-10T16:50:55.167Z</updated>
    
    <content type="html"><![CDATA[<p>论文地址：<a href="https://arxiv.org/pdf/1804.06278.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1804.06278.pdf</a><br>代码地址：<a href="https://github.com/art-programmer/PlaneNet" target="_blank" rel="noopener">https://github.com/art-programmer/PlaneNet</a></p><hr><h3 id="PlaneNet-Piece-wise-Planar-Reconstruction-from-a-Single-RGB-Image"><a href="#PlaneNet-Piece-wise-Planar-Reconstruction-from-a-Single-RGB-Image" class="headerlink" title="PlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image"></a>PlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image</h3><h3 id="PlaneNet：单幅RGB图像的分割平面重建"><a href="#PlaneNet：单幅RGB图像的分割平面重建" class="headerlink" title="PlaneNet：单幅RGB图像的分割平面重建"></a>PlaneNet：单幅RGB图像的分割平面重建</h3><img src="/2019/04/PlaneNet-translation/plane1.png"><a id="more"></a><h4 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h4><h4 id="论文摘要"><a href="#论文摘要" class="headerlink" title="论文摘要"></a>论文摘要</h4><p><strong><em>EN:</em></strong> This paper proposes a deep neural network (DNN) for piece-wise planar depthmap reconstruction from a single RGB image. While DNNs have brought remarkable progress to single-image depth prediction, piece-wise planar depthmap reconstruction requires a structured geometry representation, and has been a difficult task to master even for DNNs. The proposed end-to-end DNN learns to directly infer a set of plane parameters and corresponding plane segmentation masks from a single RGB image. We have generated more than 50,000 piece-wise planar depthmaps for training and testing from ScanNet, a largescale RGBD video database. Our qualitative and quantitative evaluations demonstrate that the proposed approach outperforms baseline methods in terms of both plane segmentation and depth estimation accuracy. To the best of our knowledge, this paper presents the first end-to-end neural architecture for piece-wise planar reconstruction from a single RGB image. Code and data are available at <a href="https://github.com/art-programmer/PlaneNet" target="_blank" rel="noopener">https://github.com/art-programmer/PlaneNet</a>.<br><strong><em>CH:</em></strong> 本篇论文提出了一种深度神经网络（DNN）去完成单幅图像的分割平面深度图重建任务。虽然DNN在单幅图像上的深度预测取得了显著的进步，但是分割平面深度图重建需要一个结构化的几何表示，即使对于DNN也是很难解决的一个任务。提出的这个端到端的DNN直接从单幅RGB图像中推算出一套平面参数和对应的平面分割掩膜。我们从 ScanNet 生成了超过50000张的分割平面深度图用于训练和测试，ScanNet 是一个大型的 RGBD 视频数据集。我们的定性和定量评估表明我们提出的这个方法在平面分割和深度估计的精度方面都比基础的方法效果要好。据我们所知，这篇论文提出的端到端神经网络结构是第一个用来解决单幅RGB图像的分割平面重建问题的神经网络。代码和数据均在GitHub：<a href="https://github.com/art-programmer/PlaneNet" target="_blank" rel="noopener">https://github.com/art-programmer/PlaneNet</a></p><h5 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h5><h5 id="1-前言"><a href="#1-前言" class="headerlink" title="1. 前言"></a>1. 前言</h5><p><strong><em>EN:</em></strong> Human vision has extraordinary perceptual power in understanding advanced scene structures. Looking at a typical indoor scene (for example, Figure 1), we can immediately parse the room into a few major planes (for example, floors, walls, and ceilings), sense the main surface of the furniture, or identify the surface of a horizontal tabletop. Segmental planar geometry understanding will be key to many applications in emerging areas such as robotics or augmented reality (AR). For example, the robot needs to identify the extent of the floor used to plan the move, or the desktop split for placing the object. In AR applications, planar surface inspection is becoming the basic building block for placing virtual objects on the desktop, replacing floor textures or hanging artwork on walls for internal remodeling. A fundamental problem in computer vision is the development of a computational algorithm that masters similar perceptions to implement such an application.<br><strong><em>CH:</em></strong> 人类视觉在理解高级别场景结构方面有着非凡的感知能力。看一个典型的室内场景（比如图一），我们能立即将这个房间分析成一些主要的平面（比如墙，地板，天花板），感知家具的主要表面和水平桌面的表面。分割平面的几何理解对一些新兴领域的许多应用起到了很关键的作用，比如机器人或虚拟现实（AR）。例如，机器人需要检测用于移动的地板的范围，或者在放置物体时需要分割桌面。在AR应用中，需要往桌面上放置虚拟的物体，更换地板的样式或对墙上的艺术品进行内部改建，这时的检测平面的表面就是一个基础的模块。计算机视觉中一个基础的问题是一个能解决相似感知问题的几何算法来实现这样的应用。</p><p><strong><em>EN:</em></strong> With the proliferation of deep neural networks, single image depth map inference and room layout estimation have been active areas of research. However, to our surprise, little attention has been paid to the study of segmental planar depth map reconstruction, which mimics this remarkable human perception in a general form. The main challenge is that segmented planar depth maps require a structured geometric representation (ie, a set of planar parameters and their segmentation masks). In particular, we do not know the number of planes to infer, and the order of the planes that are returned in the output feature vector, making the task even challenging for deep neural networks.<br><strong><em>CH:</em></strong> 随着深度神经网络的兴起，单幅图像的深度图和房间布局的推断一直是搞研究的活跃领域。然而，我们感到比较惊讶的是，分割平面深度图重建这一方面很少有人关注，这一方面一般来说是模仿了人类的这种非凡的感知能力。其中比较主要的挑战是分割平面深度图需要一个结构化的几何表示（i.e.平面参数的集合和它们的分割掩膜）。尤其是，我们不知道需要分割的平面数量，以及平面在输出特征向量中的顺序，完成这些任务对深度神经网络来说也很有挑战。</p><p><strong><em>EN:</em></strong> This paper proposes a novel deep neural architecture “PlaneNet” that learns to directly produce a set of plane parameters and probabilistic plane segmentation masks from a single RGB image. Following a recent work on point-setgeneration, we define a loss function that is agnostic to the order of planes. We further control the number of planes by allowing probabilistic plane segmentation masks to be all 0. The network also predicts a depthmap at non-planar surfaces, whose loss is defined through the probabilistic segmentation masks to allow back-propagation. We have generated more than 50,000 piece-wise planar depthmaps from ScanNet as ground-truth by fitting planes to 3D points and projecting them to images. Qualitative and quantitative evaluations show that our algorithm produces significantly better plane segmentation results than the current state-ofthe-art. Furthermore, our depth prediction accuracy is on-par or even superior to the existing single image depth inference techniques that are specifically trained for this task.<br><strong><em>CH:</em></strong> 本篇论文提出了一个新的深度神经网络结构“PlaneNet”，它通过学习训练直接从单幅RGB图像中得到一组平面参数和对应的平面分割掩膜。在最近的一项点集分割工作中，我们定义了一个跟平面顺序无关的损失函数。我们通过允许概率性的平面分割掩膜为0来进一步的控制平面的数量。这个网络结构还预测非平面处的深度图，这个损失是通过概率分割掩膜定义的，可以进行反向传播。我们通过拟合平面到3D点上，并且将它们投射到图像中，从 ScanNet 数据集中生成了超过50000张分段平面深度图作为真实样本。定性和定量的评估标准表明：我们的算法的平面分割结果相比当下流行的技术，有显著的提升。此外，我们的深度预测精度甚至要比当下专门针对此任务的算法更优秀。</p><h5 id="2-Related-work"><a href="#2-Related-work" class="headerlink" title="2. Related work"></a>2. Related work</h5><h5 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h5><p><strong><em>EN:</em></strong> <strong>Multi-view piece-wise planar reconstruction.</strong> Piece-wise planar depthmap reconstruction was once an active research topic in multi-view 3D reconstruction. The task is to infer a set of plane parameters and assign a plane ID to each pixel. Most existing methods first reconstruct precise 3D points, perform plane-fitting to generate plane hypotheses, then solve a global inference problem to reconstruct a piece-wise planar depthmap. Our approach learns to directly infer plane parameters and plane segmentations from a single RGB image.<br><strong><em>CH:</em></strong> 多视图分段平面重建。分段平面深度图重建曾经在多视图3D重建中的活跃研究领域。这个任务是推断一组平面参数并且给每个像素分配一个平面ID。目前大部分的算法都是首先重建精确的3D点集，拟合平面去生成假设平面，然后求解一个全局的推理问题去重建一个分段平面深度图。我们的方法通过学习训练直接从单幅RGB图像中得到一组平面参数和对应的平面分割掩膜。</p><p><strong><em>EN:</em></strong> <strong>Learning based depth reconstruction.</strong> Saxena et al.  pioneered a learning based approach for depthmap inference from a single image. With the surge of deep neural networks, numerous CNN based approaches have been proposed. However, most techniques simply produce an array of depth values (i.e., depthmap) without plane detection or segmentation. More recently, Wang et al. enforce planarity in depth (and surface normal) predictions by inferring pixels on planar surfaces. This is the closest work to ours. However, they only produce a binary segmentation mask (i.e., if a pixel is on a planar surface or not) without plane parameters or instance-level plane segmentation.<br><strong><em>CH:</em></strong> 基于自学习的深度重建。Saxena 等人针对单幅图像的深度图推断提出了一个基于自学习的方法。随着深度神经网络的兴起，出现了许多基于CNN的方法。但是，大部分的方法只是简单生成一组深度数值（i.e.深度图）而没有平面的检测与分割。最近，Wang等人通过计算平面上的像素信息，在深度信息（以及表面法线）预测中执行平面化操作。这是跟我们最接近的方法。然而，他们仅仅生成一个二进制的分割掩膜（i.e.一个像素是否在平面上），而没有平面参数或实例级别的平面分割。</p><p><strong><em>EN:</em></strong> <strong>Layout estimation.</strong> Room layout estimation also aims at predicting dominant planes in a scene (e.g., walls, floor, and ceiling). Most traditional approaches rely on image processing heuristics to estimate vanishing points of a scene, and aggregate low-level features by a global optimization procedure. Besides low-level features, high-level information has been utilized, such as human poses or semantics. Attempts have been made to go beyond room structure, and predict object geometry. However, the reliance on hand-crafted features makes those methods less robust, and the Manhattan World assumption limits their operating ranges. Recently, Lee et al. proposed an end-to-end deep neural network, RoomNet, which simultaneously classifies a room layout type and predicts corner locations. However, their framework is not applicable to general piece-wise planar scenes.<br><strong><em>CH:</em></strong> 房间布局估计。房间布局的估计也是针对一个场景中的主要平面进行预测的。（e.g.墙，地板和天花板）大部分传统的算法依靠图像的启发式处理去估算场景中的消隐点，并通过一个全局的优化程序聚合底层特征。除了底层特征，还使用到了一些高级信息，比如：人类的姿态和语义。尝试越过房间的结构来预测目标的几何结构。但是，人工选择的特征使得这些方法的稳健性比较低，曼哈顿世界的假设也限制了它们的操作范围。最近，Lee等人，提出了一个端到端的深度神经网络 RoomNet，它能同时分类房间的布局类型和预测角落的位置。但是，他们的框架不适用与一般情况下的分段平面场景。</p><p><strong><em>EN:</em></strong> <strong>Line analysis.</strong> Single image 3D reconstruction of line drawings date back to the 60s. The earliest attempt is probably the Robert’s system, which inspired many follow-up works. In real images, extraction of line drawings is challenging. Statistical analysis of line directions, junctions, or image segments have been used to enable 3D reconstruction for architectural scenes or indoor panoramas. Attributed grammar was used to parse an image into a hierarchical graph for 3D reconstruction. However, these approaches require hand-crafted features, grammar specification, or algorithmic rules. Our approach is purely data-driven harnessing the power of deep neural networks.<br><strong><em>CH:</em></strong> 线分析。单幅线条图像的3D重建可以追溯到60年代。最早的尝试大概是 Robert 的系统，它启发了许多后面的工作。在实际的图像中，线条图的提取有不小的挑战性。线向统计分析，交叉点和图像分割已经被用于建筑场景和室内全景图的3D重建。Attributed grammar 将图像解析成分层图用于3D重建。但是，这些传统的算法需要人工选取的特征，grammar specification, 或算法规则。我们的方法纯粹靠数据驱动的深度神经网络的力量。</p><h5 id="3-PlaneNet"><a href="#3-PlaneNet" class="headerlink" title="3. PlaneNet"></a>3. PlaneNet</h5><h5 id="3-PlaneNet-1"><a href="#3-PlaneNet-1" class="headerlink" title="3. PlaneNet"></a>3. PlaneNet</h5><p><strong><em>EN:</em></strong> We build our network on the Extended Residual Network (DRN) (see Figure 2), which is a flexible framework for global tasks (eg image classification) and pixel prediction tasks (eg semantic segmentation). Given the high-resolution final feature map from the DRN, we make three output branches for the three prediction tasks.<br><strong><em>CH:</em></strong> 我们基于 Extended Residual Network (DRN) 来构建我们的网络，（图二所示）DRN是针对全局性任务（e.g.图片分类）和像素预测任务（e.g.语义分割）的一个灵活框架。针对DRN最终输出的高分辨率的特征图，我们对于三个不同的预测任务提供了三个分支。</p><img src="/2019/04/PlaneNet-translation/plane2.png"><p><strong><em>EN:</em></strong> <strong>Plane parameters:</strong> For each scene, we predict a fixed number ($K$) of planar surfaces $S = {S_1, · · · S_K}$. Each surface $S_i$ is specified by the three plane parameters $P_i$ (i.e., encoding a normal and an offset). We use $D_i$ to denote a depth image, which can be inferred from the parameters $P_i$ .<br>The depth value calculation requires camera intrinsic parameters, which can be estimated via vanishing point analysis, for example. In our experiments, intrinsics are given for each image through the database information.<br><strong><em>CH:</em></strong> 平面参数。对于每个场景，我们预测的平面 $S = {S_1, · · · S_K}$ 数量是固定的 $K$。每个平面 $S_i$ 都通过三个平面参数 $P_i$ 指定。（i.e.编码法线和偏移量）我们用 $D_i$ 来表示深度图像，它能从参数 $P_i$ 中推算出来。深度值得推算需要相机内置参数，而相机内置参数可以通过消隐点分析来估算。但在我们的实验中相机内置参数是通过数据集每张图像的信息提供的。</p><p><strong><em>EN:</em></strong> <strong>Non-planar depthmap:</strong> We model non-planar structures and infer its geometry as a standard depthmap. With abuse of notation, we treat it as the $(K+1)^{th}$ surface and denote the depthmap as $D_{K+1}$. This does not explain planar surfaces.<br><strong><em>CH:</em></strong> 非平面深度图：我们对非平面结构进行建模处理，并将它的几何结构推断为标准的深度图。用符号表示的话，我们把平面表示为 $(K+1)^{th}$ ，把对应的深度图表示为 $D_{K+1}$。但是这个不能用来解释平面信息。</p><p><strong><em>EN:</em></strong> <strong>Segmentation masks:</strong> The last output is the probabilistic segmentation masks for the $K$ planes $(M_1, · · · M_K)$ and the non planar depthmap $(M_{K+1})$.<br><strong><em>CH:</em></strong> 分割掩膜：最后的输出是第 $K$ 个平面 $(M_1, · · · M_K)$ 的分割掩膜和对应的非平面深度图 $(M_{K+1})$ 。</p><p><strong><em>EN:</em></strong> In summary, the network predicts 1) plane parameters $(P1, ···, PK)$, 2) non-planar depth maps $(D_{K + 1})$, and 3) probability split masks $(M_1, ···, M_{K + 1})$. We now explain more details and loss functions for each task.<br><strong><em>CH:</em></strong> 概括起来，这个网络解决了三个任务：1）平面参数 $(P1, ···, PK)$，2）非平面深度图 $(D_{K + 1})$，3）概率分割掩膜 $(M_1, ···, M_{K + 1})$。下面详细说明每个任务的更多细节和损失函数。</p><h6 id="3-1-Plane-parameter-branch"><a href="#3-1-Plane-parameter-branch" class="headerlink" title="3.1. Plane parameter branch"></a>3.1. Plane parameter branch</h6><h6 id="3-1-平面参数分支"><a href="#3-1-平面参数分支" class="headerlink" title="3.1. 平面参数分支"></a>3.1. 平面参数分支</h6><p><strong><em>EN:</em></strong> The plane parameter branch starts with a global average pooling to reduce the feature map size to 1x1, followed by a fully connected layer to produce $K×3$ plane parameters. We do not know the number of planes as well as their order in this prediction task. By following prior works, we predict a constant number $(K)$ of planes, then allow some predictions to be invalid by letting the corresponding probabilistic segmentation masks to be 0. Our ground-truth generation process (See Sect. 4) produces at most 10 planes for most examples, thus we set $K = 10$ in our experiments. We define an order-agnostic loss function based on the Chamfer distance metric for the regressed plane parameters:</p><script type="math/tex; mode=display">L^P=\sum_{i=1}^{K^*}min_{j\in[1,K]}\Vert P_i^*-P_j \Vert_2^2</script><p>The parameterization $P_i$ is given by the 3D coordinate of the point that is closest to the camera center on the plane. $P^∗_i$ is the ground truth. $K^∗$ is the number of ground-truth planes.<br><strong><em>CH:</em></strong> 平面参数分支从一个全局平均 pooling 开始，将特征图的尺寸变成 1x1，紧接着，通过一个全连接层生成 $K×3$ 的平面参数。我们不知道平面的数量也不知道在这个预测任务中的顺序。通过遵循之前的工作，我们预测的平面数量为 $K$，然后通过使对应的概率分割掩膜为 0，让一些预测的平面无效。我们的大部分真实实例都可以生成十个左右的平面，（见第四节）因此在我们的实验中设置 $K=10$。我们基于倒角距离度量针对平面参数的回归定义了一个与顺序无关的损失函数：<script type="math/tex">L^P=\sum_{i=1}^{K^*}min_{j\in[1,K]}\Vert P_i^*-P_j \Vert_2^2</script>参数 $P_i$ 是根据平面上最靠近相机中心的点的3D坐标得到的。$P^∗_i$ 是真实实例。$K^∗$ 是真实实例中平面的数量。</p><h6 id="3-2-Plane-segmentation-branch"><a href="#3-2-Plane-segmentation-branch" class="headerlink" title="3.2. Plane segmentation branch"></a>3.2. Plane segmentation branch</h6><h6 id="3-2-平面分割分支"><a href="#3-2-平面分割分支" class="headerlink" title="3.2. 平面分割分支"></a>3.2. 平面分割分支</h6><p><strong><em>EN:</em></strong> The branch begins with a pyramid pool module followed by a convolutional layer to produce a $K + 1$ channel likelihood map for planar and non-planar surfaces. We added a dense conditional random field (DCRF) module based on the fast inference algorithm proposed by Krahenbuhl and Koltun, and jointly trained the DCRF module and the previous layer with Zheng et al. We set the average number of field iterations to 5 during training and set it to 10 during the test. For simplicity, the bandwidth of the bilateral filter is fixed. We use standard softmax cross entropy loss to supervise segmentation training: <script type="math/tex">L^M=\sum_{i=1}^{K+1}\sum_{p \in I}(1(M^{*(p)}=i)log(1-M_i^{(p)}))</script>The internal summation is over the image pixels $(I)$, where $M^{(p)}_i$ denotes the probability of pixel $p$ belonging to the $i^{th}$ plane. $M^{∗(p)}$ is the ground-truth plane-id for the pixel.<br><strong><em>CH:</em></strong> 这个分支以一个金字塔池化模块开始，紧接着通过一个卷积层生成平面和非平面表面 $K+1$ 通道的极大似然图。我们在 Krahenbuhl 和 Koltun 提出的快速推理算法的基础上添加了一个密集条件随机场（DCRF）模块，并且和 Zheng 等人共同训练这个 DCRF 模块和先前的层。我们在训练期间设置平均场迭代为5，在测试期间设置为10.为简单起见，双边滤波器的带宽是固定的。我们用标准的 softmax 交叉熵损失函数来监督分割训练：<script type="math/tex">L^M=\sum_{i=1}^{K+1}\sum_{p \in I}(1(M^{*(p)}=i)log(1-M_i^{(p)}))</script>当 $M^{(p)}_i$ 表示像素 $p$ 属于平面 $i^{th}$ 的概率时，里面的求和是对图像像素 $(I)$ 的求和。$M^{∗(p)}$ 是像素在真实实例上所属的平面 id。</p><h6 id="3-3-Non-planar-depth-branch"><a href="#3-3-Non-planar-depth-branch" class="headerlink" title="3.3. Non-planar depth branch"></a>3.3. Non-planar depth branch</h6><h6 id="3-3-非平面深度分支"><a href="#3-3-非平面深度分支" class="headerlink" title="3.3. 非平面深度分支"></a>3.3. 非平面深度分支</h6><p><strong><em>EN:</em></strong> The branch shares the same pyramid pooling module, followed by a convolution layer to produce a 1-channel depthmap. Instead of defining a loss specifically for non-planar regions, we found that exploiting the entire ground-truth depthmap makes the overall training more effective. Specifically, we define the loss as the sum of squared depth differences between the ground-truth and either a predicted plane or a non-planar depthmap, weighted by probabilities:<script type="math/tex">L^D=\sum_{i=1}^{K+1}\sum_{p\in I}(M_i^{(p)}(D_i^{(p)}-D^{*(p)})^2)$$$D_i^{(p)}$ denotes the depth value at pixel p, while $D^{*(p)}$ is the ground truth depth value.***CH:*** 这个分支与平面分割分支分享使用同一个金字塔池化模块，然后通过一个卷积层生成1通道的深度图。我们发现使用全部的真实实例深度图来进行训练比单独定义一个非平面区域的损失要更有效。因此，我们将损失定义为真实实例与预测平面或非平面深度图之间深度差的平方和，并由概率进行加权：</script>L^D=\sum_{i=1}^{K+1}\sum_{p\in I}(M_i^{(p)}(D_i^{(p)}-D^{<em>(p)})^2)$$当$D^{</em>(p)}$ 表示真实实例的深度值时，$D_i^{(p)}$ 表示在像素 $p$ 的深度值。</p><h5 id="4-Datasets-and-implemenation-details"><a href="#4-Datasets-and-implemenation-details" class="headerlink" title="4. Datasets and implemenation details"></a>4. Datasets and implemenation details</h5><h5 id="4-数据集和网络实现细节"><a href="#4-数据集和网络实现细节" class="headerlink" title="4. 数据集和网络实现细节"></a>4. 数据集和网络实现细节</h5><p><strong><em>EN:</em></strong> We have generated 51,000 ground-truth piece-wise planar depthmaps (50,000 training and 1,000 testing) from ScanNet, a large-scale indoor RGB-D video database. A depthmap in a single RGB-D frame contains holes and the quality deteriorates at far distances. Our approach for ground-truth generation is to directly fit planes to a consolidated mesh and project them back to individual frames, while also exploiting the associated semantic annotations.<br><strong><em>CH:</em></strong> 我们从 ScanNet（一个大型室内的 RGB-D 视频数据库）中生成了 51,000 张分段平面深度图作为真实样本（50,000张训练，1,000张预测）。单幅RGB-D图像的深度图包含 holes，而且图像内容距离比较远的效果也会变坏。我们生成真实实例的方法是将平面拟合到统一的网格中，并将他们投射回单个图像帧，同时还利用了相关的语义注释。</p><p><strong><em>EN:</em></strong> Specifically, for each sub mesh-models of the same semantic label, we treat mesh-vertices as points and repeat extracting planes by RANSAC with replacement. The inlier distance threshold is $5cm$, and the process continues until 90% of the points are covered. We merge two (not necessarily adjacent) planes that span different semantic labels if the plane normal difference is below $20^◦$ , and if the larger plane fits the smaller one with the mean distance error below $5cm$. We project each triangle to individual frames if the three vertices are fitted by the same plane. After projecting all the triangles, we keep only the planes whose projected area is larger than 1% of an image. We discard entire frames if the ratio of pixels covered by the planes is below 50%. For training samples, we randomly choose 90% of the scenes from ScanNet, subsample every 10 frames, compute piecewise planar depthmaps with the above procedure, then use the final random sampling to produce 50,000 examples. The same procedure generates 1,000 testing examples from the remaining 10% of the scenes.<br><strong><em>CH:</em></strong> 明确来说，对于相同语义标签的每个子网格模型，我们将网格顶点视为 points，并通过 RANSAC 算法重复提取平面。这个内部距离的阈值为 $5cm$，并且这个过程会持续到 points 的百分之九十被覆盖。如果两个跨越不同语义标签的平面的平面法线差异小于$20^◦$ 并且大平面拟合小平面时平均距离误差小于 $5cm$，就合并这两个平面。（不一定相邻）如果三个网格顶点拟合同一个平面，就把三个顶点投射到单独的坐标系中。投射完所有的顶点，只保留投射区域大于原图面积百分之一的平面。如果所有的平面像素覆盖比小于百分之五十，就丢弃所有的平面。我们从 ScanNet 中随机选取百分之九十的场景，每十帧采样一次，使用上述流程生成分段平面深度图，然后用随机采样选出50,000个样本作为训练集。相同的流程从 ScanNet 剩余的百分之十场景中选出1,000个样本作为测试集。</p><p><strong><em>EN:</em></strong> We have implemented PlaneNet using TensorFlow based on DeepLab . Our system is a 101-layer ResNet with Dilated Convolution, while we have followed a prior work and modified the first few layers to deal with the degridding issue. The final feature map of the DRN contains 2096 channels. We use the Adam optimizer with the initial learning rate set to 0.0003. The input image, the output plane segmentation masks, and the non-planar depthmap have a resolution of 256x192. We train our network for 50 epochs on the 50,000 training samples.<br><strong><em>CH:</em></strong> 我们基于 DeepLab 的 TensorFlow 实现了 PlaneNet 。我们的方法是有着 Dilated Convolution（扩张卷积）的 101层 Resnet，我们复现了先前的工作，并修改了前几层，为了处理 degridding 的问题。最后输出的 DRN 特征图包括 2096 个通道。我们使用 Adam 优化器设置学习率为 0.0003 来进行网络训练优化。输入图像，输出的平面分割掩膜和非平面深度图的尺寸都为 256x192。我们在 50,000 个训练样本上训练了 50 轮我们的网络。</p><h5 id="5-Experimental-results"><a href="#5-Experimental-results" class="headerlink" title="5. Experimental results"></a>5. Experimental results</h5><h5 id="5-实验结果"><a href="#5-实验结果" class="headerlink" title="5. 实验结果"></a>5. 实验结果</h5><p><strong><em>EN:</em></strong> Figure 3 shows the reconstruction results for various scenarios. Our end-to-end learning framework has successfully restored segmented planar and semantically meaningful structures from a single RGB image, such as a floor, wall, desktop or computer screen. We have included more examples in the supplements. We now provide a quantitative assessment of the accuracy of planar segmentation and depth reconstruction for competitive baselines, and then analyze our results more.<br><strong><em>CH:</em></strong> 图三展示了各种场景的平面重建结果。我们的端到端的学习架构成功的从单幅RGB图像中重建了分段的平面结构和有意义的语义结构，比如：地板，墙面，桌面或电脑屏幕。在补充材料里有更多的实例结果。我们提出了一个针对平面分割和深度图重建的定量评估标准，然后对我们的结果进行更多的分析。</p><img src="/2019/04/PlaneNet-translation/plane3.png"><p><strong><em>EN:</em></strong> Figure 3: Piece-wise planar depthmap reconstruction results by PlaneNet. From left to right: input image, plane segmentation, depthmap reconstruction, and 3D rendering of our depthmap. In the plane segmentation results, the black color shows non-planar surface regions.<br><strong><em>CH:</em></strong> 图三：PlaneNet 的分段平面深度图重建结果。从左到右：输入图像，平面分割结果，深度图重建结果和深度图的3D渲染结果。在平面分割结果中，黑色显示非平面表面区域。</p><h6 id="5-1-Plane-segmentation-accuracy"><a href="#5-1-Plane-segmentation-accuracy" class="headerlink" title="5.1. Plane segmentation accuracy"></a>5.1. Plane segmentation accuracy</h6><h6 id="5-1-平面分割准确率"><a href="#5-1-平面分割准确率" class="headerlink" title="5.1. 平面分割准确率"></a>5.1. 平面分割准确率</h6><p><strong><em>EN:</em></strong> Piece-wise planar reconstruction from a single RGB image is a challenging problem. While existing approaches have produced encouraging results, they are based on hand-crafted features and algorithmic designs, and may not match against big-data and deep neural network (DNN) based systems. Much better baselines would then be piece-wise planar depthmap reconstruction techniques from 3D points, where input 3D points are either given by the ground truth depthmaps or inferred by a state-of-the-art DNN-based system.<br><strong><em>CH:</em></strong> 单幅RGB图像的分段平面重建是一个有挑战的问题。虽然现有的方法在这方面已经有了不错的结果，但它们都是基于手动设计的特征的算法，并且可能和基于大数据和深度神经网络的系统不匹配。更好的基准线将来自于3D点的分段平面深度图重建技术，输入3D点，然后将由真实实例的深度图或最先进的DNN系统推测输出。</p><p><strong><em>EN:</em></strong> In particular, to infer depthmaps, we have used a variant of PlaneNet which only has the pixel-wise depthmap branch, while following Eigen et al. to change the loss. Table 1 shows that this network, PlaneNet (Depth rep.), outperforms the current top-performers on the NYU benchmark.<br><strong><em>CH:</em></strong> 特别是，为了推算深度图，我们使用了 PlaneNet 的变种网络，只保留像素级的深度图分支，然后参考 Eigen 等人的思想去改变损失函数。图一显示 PlaneNet 在 NYU 的基准上是目前最佳的网络。</p><p><strong><em>EN:</em></strong> For piece-wise planar depthmap reconstruction, we have used the following three baselines from the literature.<br>“NYU-Toolbox” is a plane extraction algorithm from the official NYU toolbox that extracts plane hypotheses using RANSAC, and optimizes the plane segmentation via a Markov Random Field (MRF) optimization.<br>Manhattan World Stereo (MWS) is very similar to NYU-Toolbox except that MWS employs the Manhattan World assumption in extracting planes and exploits vanishing lines in the pairwise terms to improve results.<br>Piecewise Planar Stereo (PPS) relaxes the Manhattan World assumption of MWS, and uses vanishing lines to generate better plane proposals. Please see the supplementary document for more algorithmic details on the baselines.<br><strong><em>CH:</em></strong> 为了对比分段平面深度图重建，我们使用了文献中的三个方法作为比较基准。<br>NYU-Toolbox 是 NYU 官方工具箱中的平面提取算法，使用了 RANSAC 算法提取平面候选区域，然后通过马尔可夫随机场（MRF）来优化平面分割。<br>Manhattan World Stereo (MWS) 与 NYU-Toolbox 很相似，不同之处在于 MWS 在提取平面时用了曼哈顿世界的假设（Manhattan World assumption），并且用成对项中的消失线来改善结果。<br>Piecewise Planar Stereo (PPS) 放宽了曼哈顿世界假设（Manhattan World assumption）对 MWS 的影响，并使用消失线来生成更好的平面候选区域。</p><p><strong><em>EN:</em></strong> Figure 4 shows the evaluation results on two recall metrics. The first metric is the percentage of correctly predicted ground-truth planes. We consider a ground-truth plane being correctly predicted, if one of the inferred planes has 1) more than 0.5 Intersection over Union (IOU) score and 2) the mean depth difference over the overlapping region is less than a threshold. We vary this threshold from 0 to 0.6m with an increment of 0.05m to plot graphs. The second recall metric is simply the percentage of pixels that are in such overlapping regions where planes are correctly predicted. The figure shows that PlaneNet is significantly better than all the competing methods when inferred depthmaps are used. PlaneNet is even better than some competing methods that use ground-truth depthmaps. This demonstrates the effectiveness of our approach, learning to infer piece-wise planar structures from many examples.<br><strong><em>CH:</em></strong> 图四显示了两个召回指标的评估结果。第一个指标是正确预测的真实实例平面的百分比。我们判断一个真实实例平面预测是否正确的标准是：1）是否有IOU分数大于0.5的平面，2）重叠区域的平均深度差是否小于阈值。我们让这个阈值从0 - 0.6m以0.05m的速度递增来画图。第二个指标是正确预测平面中重叠区域所占的像素百分比。该图显示，在推算深度图指标中 PlaneNet 要优于其他的方法。证明了我们的方法的有效性，从许多实例中学习推算分段平面结构。</p><img src="/2019/04/PlaneNet-translation/plane4.png"><p><strong><em>EN:</em></strong> Figure 4: Plane segmentation accuracy against competing baselines that use 3D points as input. Either ground-truth depthmaps or inferred depthmaps (by a DNN-based system) are used as their inputs. PlaneNet outperforms all the other methods that use inferred depthmaps. Surprisingly, PlaneNet is even better than many other methods that use ground-truth depthmaps.<br><strong><em>CH:</em></strong> 图四：使用3D点作为输入，平面分割准确率的对比。或者使用真实实例深度图和基于DNN系统推算的深度图作为输入。PlaneNet 要优于其他的方法。出人意料的是，PlaneNet 比一些使用真实实例深度图的方法还要好。</p><p><strong><em>EN:</em></strong> Figure 5 shows qualitative comparisons against existing methods with inferred depthmaps. PlaneNet produces significantly better plane segmentation results, while existing methods often generate many redundant planes where depthmaps are noisy, and fail to capture precise boundaries where the intensity edges are weak.<br><strong><em>CH:</em></strong> 图五显示了与现有的方法推算出的深度图的定性比较。PlaneNet 生成了更好的平面分割结果，现有的方法会有一些冗余的平面而且深度图会有很多噪音，不能精确的捕捉到平面的边界。</p><img src="/2019/04/PlaneNet-translation/plane5.png"><p><strong><em>EN:</em></strong> Figure 5: Qualitative comparisons between PlaneNet and existing methods that use inferred depthmaps as the inputs. From left to right: an input image, plane segmentation results for existing methods, and PlaneNet, respectively, and the ground-truth.<br><strong><em>CH:</em></strong> 图五：使用推算的深度图作为输入，PlaneNet 与现有的其他方法的定性比较。从左往右：第一列为输入图像，第二三四列为现有其他方法的平面分割结果，第五列为PlaneNet 的平面分割结果，第六列为真实实例。</p><h6 id="5-2-Depth-reconstruction-accuracy"><a href="#5-2-Depth-reconstruction-accuracy" class="headerlink" title="5.2. Depth reconstruction accuracy"></a>5.2. Depth reconstruction accuracy</h6><h6 id="5-2-深度重建的准确率"><a href="#5-2-深度重建的准确率" class="headerlink" title="5.2. 深度重建的准确率"></a>5.2. 深度重建的准确率</h6><p><strong><em>EN:</em></strong> While the capability to infer a plane segmentation mask and precise plane parameters is the key contribution of the work, it is also interesting to compare against depth prediction methods. This is to ensure that our structured depth prediction does not compromise per-pixel depth prediction accuracy. PlaneNet makes $(K+1)$ depth value predictions at each pixel. We pick the depth value with the maximum probability in the segmentation mask to define our depthmap.<br><strong><em>CH:</em></strong> 虽然这个工作的关键是预测平面分割掩膜和精确的平面参数，但也能与深度预测方法进行比较。可以确保我们的深度结构化预测不会对每个像素的深度预测精度造成影响。PlaneNet 对每个像素进行了 $(K+1)$ 深度值预测。我们选择分割掩膜中最大概率的深度值来定义深度图。</p><p><strong><em>EN:</em></strong> Depth accuracies are evaluated on the NYUv2 dataset at 1) planar regions, 2) boundary regions, and 3) the entire image, against three competing baselines. Eigen-VGG is a convolutional architecture to predict both depths and surface normals. SURGE is a more recent depth inference network that optimizes planarity. FCRN is the current state-of-the-art single-image depth inference network .<br><strong><em>CH:</em></strong> 深度精度评估基于 NYUv2 数据集的平面区域，边界区域和整个图像。三个对比网络分别是：Eigen-VGG 是用来预测深度值和平面法线的卷积结构。SURGE 是最新的深度推算网络可以优化平面的。FCRN 是目前最好的单图像推算网络。</p><p><strong><em>EN:</em></strong> Depthmaps in NYUv2 are very noisy and ground-truth plane extraction does not work well. Thus, we fine-tune our network using only the depth loss. Note that the key factor in this training is that the network is trained to generate a depthmap through our piece-wise planar depthmap represen-tation. To further verify the effects of this representation, we have also fine-tuned our network in the standard per-pixel depthmap representation by disabling the plane parameter and the plane segmentation branches. In this version, denoted as “PlaneNet (Depth rep.)”, the entire depthmap is predicted in the $(K + 1)^{th}$ depthmap $(D_{K+1})$.<br><strong><em>CH:</em></strong> NYUv2 的深度图有很多噪音，并且真实实例的平面提取效果不好。因此，我们只使用深度损失来 fine-tune 我们的网络。注意，训练时候的关键因素是网络经过训练可以通过我们分段平面深度信息表示生成深度图。为了进一步验证这种表示的效果，我们禁用了平面参数和平面分割掩膜两个分支，只 fine-tune 像素的深度图网络分支，这个版本表示为 PlaneNet (Depth rep.)。</p><p><strong><em>EN:</em></strong> Table 1 shows the depth prediction accuracy on various metrics introduced in the prior work. The left five metrics provide different error statistics such as relative difference (Rel) or rooted-mean-square-error (RMSE) on the average per-pixel depth errors. The right three metrics provide the ratio of pixels, for which the relative difference between the predicted and the ground-truth depths is below a threshold. The table demonstrates that PlaneNet outperforms the state of-the-art of single-image depth inference techniques. As observed in prior works, the planarity constraint makes differences in the depth prediction task, and the improvements are more significant when our piece-wise planar representation is enforced by our network.<br><strong><em>CH:</em></strong> 表一展示了先前工作中用的各种指标的深度预测准确度。左边五个是不同的误差统计，比如：平均像素深度误差的相对偏差（Rel）和均方根误差（RMSE）右边三个是像素所占的比例，对于那些预测的和实际的深度相对误差小于阈值的。该表表明 PlaneNet 要优于目前单图像深度信息推算的最新方法。之前的工作中有观察到，在深度预测任务中，平面约束可以产生积极的影响，当我们的网络强制性执行分段平面表示时，这种影响更加的明显了。</p><img src="/2019/04/PlaneNet-translation/plane6.png"><h6 id="5-3-Plane-ordering-consistency"><a href="#5-3-Plane-ordering-consistency" class="headerlink" title="5.3. Plane ordering consistency"></a>5.3. Plane ordering consistency</h6><h6 id="5-3-平面顺序的一致性"><a href="#5-3-平面顺序的一致性" class="headerlink" title="5.3. 平面顺序的一致性"></a>5.3. 平面顺序的一致性</h6><p><strong><em>EN:</em></strong> For segment depth map inference, sorting ambiguity is a challenge. We found that PlaneNet automatically learns consistent sorting without supervision, for example, the floor is always returned to the second plane. In Figure 3, the colors in the planar segmentation results are defined by the order of the planes in the network output. While ordering loses consistency for small objects or extreme camera angles, in most cases, major common surfaces such as floors and walls have a consistent ordering.<br><strong><em>CH:</em></strong> 对于分割深度图的推算，平面的顺序是一个挑战。我们发现 PlaneNet 在没有干预的情况下会自动进行平面排序，例如：识别出来的地板总是被分到第二个平面。图三中，平面分割结果的颜色就由输出的平面顺序决定的。一般情况下，对于墙面，地板这些大的平面，顺序是一致的，只有在一些小平面上会失去一致性。</p><p><strong><em>EN:</em></strong> We have taken advantage of this property and implemented a simple room layout estimation algorithm. More specifically, we look at the reconstruction example and manually select the plane entries that correspond to the ceiling, floor, and left/middle/right walls. For each possible room layout configuration (for example, a configuration with floor, left and middle walls visible), we build a 3D concave shell based on the plane parameters and project it back into the image to generate the room-layout. We measure the configured score by the number of pixels, where the constructed room layout is consistent with the inferred plane segmentation (determined by the winner). We chose the constructed room layout with the best score as our prediction. Figure 6 shows that our algorithm can generate reasonable room layout estimates even if the scene is confusing and contains many occlusion objects. Table 2 shows a quantitative assessment of the NYUv2 303 data set, where our method is comparable to the prior art designed specifically for this task.<br><strong><em>CH:</em></strong> 根据这样一个特点，我们实现了一个房间布局估计算法，具体来说，我们在重建的实例中手动选择对应的天花板，墙面，地板等平面。对于每个可能的房间布局配置，我们都根据推算的平面参数构建一个3D结构，然后将这个3D结构投影到原图像生成房间的布局配置。在构建的房间布局和推断的平面分割一致时，我们通过像素的数量来衡量预测布局的效果。最后选择具有最佳效果的房间布局作为输出的预测结果。图六显示即使场景很复杂，有许多遮挡对象，我们的算法也能够生成合理的房间布局。表二显示在 NYUv2 303 数据集上，我们的方法与专门针对此任务的方法效果相当。</p><img src="/2019/04/PlaneNet-translation/plane7.png"><p><strong><em>EN:</em></strong> Figure 6: Room layout estimations. We have exploited the ordering consistency in the predicted planes to infer room layouts.<br><strong><em>CH:</em></strong> 图六：房间布局估计。我们利用预测平面的顺序一致性来预测房间布局。</p><img src="/2019/04/PlaneNet-translation/plane8.png"><p><strong><em>EN:</em></strong> Table 2: Room layout estimations. Quantitative evaluations against the top-performers over the NYUv2 303 dataset.<br><strong><em>CH:</em></strong> 表二：房间布局估计。在 NYUv2 303 数据集上与其他算法的定性效果比较。</p><h6 id="5-4-Failure-modes"><a href="#5-4-Failure-modes" class="headerlink" title="5.4. Failure modes"></a>5.4. Failure modes</h6><h6 id="5-4-不足之处"><a href="#5-4-不足之处" class="headerlink" title="5.4. 不足之处"></a>5.4. 不足之处</h6><p><strong><em>EN:</em></strong> While achieving promising results on most images, PlaneNet has some failure modes as shown in Fig. 7. In the first example, PlaneNet generates two nearly co-planar vertical surfaces in the low-light region below the sink. In the second example, it cannot distinguish a white object on the floor from a white wall. In the third example, it misses a column structure on a wall due to the presence of object clutter. While the capability to infer precise plane parameters is already super-human, there is a lot of room for improvement on the planar segmentation, especially in the absence of texture information or at the presence of clutter.<br><strong><em>CH:</em></strong> 虽然在很多图像上有不错的效果，但是 PlaneNet 还是有许多不足之处，如图七所示。在第一个例子中，PlaneNet 在一个低光区域产生了两个几乎共面的垂直表面，第二个例子中，没有把白色墙壁和白色物体区分开来，第三个例子中，由于杂乱物体的影响，错过了墙上的列结构。虽然 PlaneNet 在推算平面参数的能力已经时很优秀了，但是在平面分割精度方面还有待提升，尤其是在没有纹理和有杂物的情况下。</p><img src="/2019/04/PlaneNet-translation/plane9.png"><p><strong><em>EN:</em></strong> Figure 7: Typical failure modes occur in the absence of enough image texture cues or at the presence of small objects and clutter.<br><strong><em>CH:</em></strong> 图七：不足之处在于缺乏纹理或者有小物体遮挡的情况下。</p><h5 id="6-Applications"><a href="#6-Applications" class="headerlink" title="6. Applications"></a>6. Applications</h5><h5 id="6-应用"><a href="#6-应用" class="headerlink" title="6. 应用"></a>6. 应用</h5><p><strong><em>EN:</em></strong> Structured geometry reconstruction is important for many application in Augmented Reality. We demonstrate two image editing pplications enabled by our piece-wise planar representation: texture insertion and replacement (see Fig. 8). We first extract Manhattan directions by using the predicted plane normals through a standard voting scheme . Given a piece-wise planar region, we define an axis of its UV coordinate by the Manhattan direction that is the most parallel to the plane, while the other axis is simply the cross product of the first axis and the plane normal. Given a UV coordinate, we insert a new texture by alpha-blending or completely replace a texture with a new one. Please see the supplementary material and the video for more AR application examples.<br><strong><em>CH:</em></strong> 结构化几何重建对于增强现实中的许多应用都非常重要。通过使用我们的分段平面表示：纹理插入和替换，做了两个图像编辑的应用。（见图八）我们首先用一个标准的表决方法通过预测的平面法线来提取曼哈顿方向。给定分段平面分割的区域，我们通过最平行与平面的曼哈顿方向来定义它的 UV 坐标轴，另一个轴是第一个轴和其平面法线的叉乘。给定 UV 坐标轴，我们通过 alpha-blending 插入新的纹理或者完全替换旧纹理。更多实例请参阅补充材料及视频。</p><img src="/2019/04/PlaneNet-translation/plane10.png"><p><strong><em>EN:</em></strong> Figure 8: Texture editing applications. From top to bottom, an input image, a plane segmentation result, and an edited image.<br><strong><em>CH:</em></strong> 图八：图片纹理编辑应用。</p><h5 id="7-Conclusion-and-future-work"><a href="#7-Conclusion-and-future-work" class="headerlink" title="7. Conclusion and future work"></a>7. Conclusion and future work</h5><h5 id="7-结论及未来的工作"><a href="#7-结论及未来的工作" class="headerlink" title="7. 结论及未来的工作"></a>7. 结论及未来的工作</h5><p><strong><em>EN:</em></strong> This paper proposes PlaneNet, the first deep neural architecture for piece-wise planar depthmap reconstruction from a single RGB image. PlaneNet learns to directly infer a set of plane parameters and their probabilistic segmentation masks. The proposed approach significantly outperforms competing baselines in the plane segmentation task. It also advances the state-of-the-art in the single image depth prediction task. An interesting future direction is to go beyond the depthmap framework and tackle structured geometry prediction problems in a full 3D space.<br><strong><em>CH:</em></strong> 本论文提出了第一个用于单幅图像重建分段平面深度图的深度神经网络-PlaneNet。PlaneNet 直接推断平面参数及其分割掩膜。这个方法不仅在此任务中明显的优于目前的其他方法，还推动了单一图像深度预测任务的发展。在未来一个有趣的方向是超越深度图，直接在3D空间处理几何结构化预测问题。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/pdf/1804.06278.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://arxiv.org/pdf/1804.06278.pdf&lt;/a&gt;&lt;br&gt;代码地址：&lt;a href=&quot;https://github.com/art-programmer/PlaneNet&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/art-programmer/PlaneNet&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&quot;PlaneNet-Piece-wise-Planar-Reconstruction-from-a-Single-RGB-Image&quot;&gt;&lt;a href=&quot;#PlaneNet-Piece-wise-Planar-Reconstruction-from-a-Single-RGB-Image&quot; class=&quot;headerlink&quot; title=&quot;PlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image&quot;&gt;&lt;/a&gt;PlaneNet: Piece-wise Planar Reconstruction from a Single RGB Image&lt;/h3&gt;&lt;h3 id=&quot;PlaneNet：单幅RGB图像的分割平面重建&quot;&gt;&lt;a href=&quot;#PlaneNet：单幅RGB图像的分割平面重建&quot; class=&quot;headerlink&quot; title=&quot;PlaneNet：单幅RGB图像的分割平面重建&quot;&gt;&lt;/a&gt;PlaneNet：单幅RGB图像的分割平面重建&lt;/h3&gt;&lt;img src=&quot;/2019/04/PlaneNet-translation/plane1.png&quot;&gt;
    
    </summary>
    
      <category term="计算机视觉" scheme="http://wgshun.github.io/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
    
      <category term="planenet" scheme="http://wgshun.github.io/tags/planenet/"/>
    
      <category term="平面重建" scheme="http://wgshun.github.io/tags/%E5%B9%B3%E9%9D%A2%E9%87%8D%E5%BB%BA/"/>
    
      <category term="平面检测" scheme="http://wgshun.github.io/tags/%E5%B9%B3%E9%9D%A2%E6%A3%80%E6%B5%8B/"/>
    
      <category term="深度图重建" scheme="http://wgshun.github.io/tags/%E6%B7%B1%E5%BA%A6%E5%9B%BE%E9%87%8D%E5%BB%BA/"/>
    
  </entry>
  
  <entry>
    <title>Windows 使用 VNC 远程连接 Ubuntu 桌面版</title>
    <link href="http://wgshun.github.io/2019/04/windows-connect-ubuntu-with-vnc/"/>
    <id>http://wgshun.github.io/2019/04/windows-connect-ubuntu-with-vnc/</id>
    <published>2019-04-10T14:26:59.000Z</published>
    <updated>2019-04-10T17:04:04.377Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>工作需要使用 Windows 远程桌面版的 Ubuntu，原来使用的 TeamViewer 现在经常被检测为商业用途，就很麻烦。因此，现在转战使用 VNC 进行远程。使用步骤参考地址：<a href="https://www.cnblogs.com/xuliangxing/p/7642650.html" target="_blank" rel="noopener">法号阿兴</a>，下面是自己的记录，以备不时之需。</p><h4 id="使用步骤"><a href="#使用步骤" class="headerlink" title="使用步骤"></a>使用步骤</h4><h5 id="1-设置-Ubuntu-的桌面共享"><a href="#1-设置-Ubuntu-的桌面共享" class="headerlink" title="1. 设置 Ubuntu 的桌面共享"></a>1. 设置 Ubuntu 的桌面共享</h5><p>首先在 Ubuntu 系统中找到桌面共享的应用，如下图：</p><img src="/2019/04/windows-connect-ubuntu-with-vnc/vnc1.png"><a id="more"></a><p>设置远程相关的选项，如：允许远程控制此电脑，远程连接此电脑的时候不需要进行确认，远程连接此电脑的时候需要输入的密码等，如下图：</p><img src="/2019/04/windows-connect-ubuntu-with-vnc/vnc2.png"><h5 id="2-安装-VNC-Server-和-dconf-editor-并设置远程权限"><a href="#2-安装-VNC-Server-和-dconf-editor-并设置远程权限" class="headerlink" title="2. 安装 VNC Server 和 dconf-editor 并设置远程权限"></a>2. 安装 VNC Server 和 dconf-editor 并设置远程权限</h5><p>打开终端，输入如下命令安装 VNC Server ：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install xrdp vnc4server xbase-clients</span><br></pre></td></tr></table></figure></p><p>Ubuntu 的 <code>dconf</code> 类似于 Windows 下的注册表，有 Ubuntu 系统的相关配置。<code>dconf-editor</code> 就是调整相关配置的一个工具。<br>打开终端，输入如下命令安装 dconf-editor ：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install dconf-editor</span><br></pre></td></tr></table></figure></p><p>安装完成后打开此工具，依次进入菜单：<code>org --- gnome --- desktop --- remote-access</code>，然后取消 <code>require-encryption</code>（请求加密）的勾选状态，保存设置。如下图：</p><img src="/2019/04/windows-connect-ubuntu-with-vnc/vnc3.png"><p>至此 Ubuntu 桌面系统的 VNC Server 设置告一段落。</p><h5 id="3-Windows-远程连接-Ubuntu"><a href="#3-Windows-远程连接-Ubuntu" class="headerlink" title="3. Windows 远程连接 Ubuntu"></a>3. Windows 远程连接 Ubuntu</h5><p>可以通过在 Windows 系统上安装 VNC Viewer 客户端来远程 Ubuntu 桌面系统，<a href="https://www.realvnc.com/en/connect/download/viewer/" target="_blank" rel="noopener">下载地址</a>，注意下载对应的<code>Standalone</code> 免安装版本。下载完成就可以打开下载的 VNC Viewer 客户端输入 VNC Server address（即Ubuntu 端的 IP 地址）进行远程连接了，如下图：</p><img src="/2019/04/windows-connect-ubuntu-with-vnc/vnc4.png"><p>连接成功就可以看到 Ubuntu 的桌面了。<br>除了使用 VNC Viewer 客户端来远程 Ubuntu 桌面系统，还可以使用 Windows 自带的远程桌面连接应用进行连接，同样是输入服务端的 IP 地址。<br>通过 Windows键+R键 打开运行界面输入 <code>mstsc</code> 即可打开远程桌面应用，如下图：</p><img src="/2019/04/windows-connect-ubuntu-with-vnc/vnc5.png"><p>或者通过 Windows键 搜索远程连接应用打开也可，界面显示如下图：</p><img src="/2019/04/windows-connect-ubuntu-with-vnc/vnc6.png"><p>输入 IP 以后会转到下图界面，选择模式：<code>vnc-any</code>，输入对应的 IP 和密码即可连接。</p><img src="/2019/04/windows-connect-ubuntu-with-vnc/vnc7.png">]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h4&gt;&lt;p&gt;工作需要使用 Windows 远程桌面版的 Ubuntu，原来使用的 TeamViewer 现在经常被检测为商业用途，就很麻烦。因此，现在转战使用 VNC 进行远程。使用步骤参考地址：&lt;a href=&quot;https://www.cnblogs.com/xuliangxing/p/7642650.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;法号阿兴&lt;/a&gt;，下面是自己的记录，以备不时之需。&lt;/p&gt;
&lt;h4 id=&quot;使用步骤&quot;&gt;&lt;a href=&quot;#使用步骤&quot; class=&quot;headerlink&quot; title=&quot;使用步骤&quot;&gt;&lt;/a&gt;使用步骤&lt;/h4&gt;&lt;h5 id=&quot;1-设置-Ubuntu-的桌面共享&quot;&gt;&lt;a href=&quot;#1-设置-Ubuntu-的桌面共享&quot; class=&quot;headerlink&quot; title=&quot;1. 设置 Ubuntu 的桌面共享&quot;&gt;&lt;/a&gt;1. 设置 Ubuntu 的桌面共享&lt;/h5&gt;&lt;p&gt;首先在 Ubuntu 系统中找到桌面共享的应用，如下图：&lt;/p&gt;
&lt;img src=&quot;/2019/04/windows-connect-ubuntu-with-vnc/vnc1.png&quot;&gt;
    
    </summary>
    
      <category term="环境搭建" scheme="http://wgshun.github.io/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="ubuntu" scheme="http://wgshun.github.io/tags/ubuntu/"/>
    
      <category term="windows" scheme="http://wgshun.github.io/tags/windows/"/>
    
      <category term="vnc" scheme="http://wgshun.github.io/tags/vnc/"/>
    
      <category term="远程连接" scheme="http://wgshun.github.io/tags/%E8%BF%9C%E7%A8%8B%E8%BF%9E%E6%8E%A5/"/>
    
  </entry>
  
  <entry>
    <title>Windows 10 安装 Anaconda3 &amp; GPU版Tensorflow &amp; Cuda9 &amp; cudnn7</title>
    <link href="http://wgshun.github.io/2018/10/windows-install-tensorflow-gpu/"/>
    <id>http://wgshun.github.io/2018/10/windows-install-tensorflow-gpu/</id>
    <published>2018-10-27T15:19:17.000Z</published>
    <updated>2018-10-27T15:21:51.551Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>为了更好的学习工作，清理了下电脑，重做了下系统，因此需要重新装深度学习相关的运行环境。在这里记录一下安装过程，以备后用。<br>环境：Windows 10 &amp; vs-2015 &amp; cuda9 &amp; cudnn7 &amp; anaconda3 &amp; tensorflow-gpu</p><h4 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h4><h5 id="1-安装-vs2015"><a href="#1-安装-vs2015" class="headerlink" title="1. 安装 vs2015"></a>1. 安装 vs2015</h5><p>首先在安装 cuda 之前需要安装 vs，在这选择安装 vs2015，安装需要的 iso 文件在我的网盘里。</p><blockquote><p>网盘链接：<a href="https://pan.baidu.com/s/10KQn-1yBqZ1vzkJnbzNL2A" target="_blank" rel="noopener">https://pan.baidu.com/s/10KQn-1yBqZ1vzkJnbzNL2A</a><br>提取码：1aiq</p></blockquote><p>下载完成之后，打开其中的 <code>vs_community.exe</code> 文件进行安装，安装期间所有的选项均默认即可。安装路径也可以单独指定。<br>如果之前电脑里有 vs 的可以直接跳过此步骤，或者可以重新更新一下。<br><a id="more"></a></p><h5 id="2-安装-cuda9-0"><a href="#2-安装-cuda9-0" class="headerlink" title="2. 安装 cuda9.0"></a>2. 安装 cuda9.0</h5><p>安装完 vs2015 之后就可以安装 cuda 了，在这里我安装的是 cuda9.0 (因为目前 tensorflow 还不支持最新的 cuda10)，cuda9.0 安装文件的下载路径：<a href="https://developer.nvidia.com/cuda-90-download-archive" target="_blank" rel="noopener">cuda9.0 安装文件</a>，当然，如果想下载 cuda 的其他可用版本也是可以的。<br>cuda9.0 的安装文件下载好了以后会是类似于：<code>cuda_9.0.176_win10.exe</code> 的一个文件，然后就是打开安装文件，一步步的按照默认的选项进行安装。使用安装文件安装完成之后，打开 <strong>cmd</strong>命令窗口 输入：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> nvcc -V</span></span><br></pre></td></tr></table></figure></p><p>就可以看到类似的以下 cuda9.0 的版本信息：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nvcc: NVIDIA (R) Cuda compiler driver</span><br><span class="line">Copyright (c) 2005-2017 NVIDIA Corporation</span><br><span class="line">Built on Fri_Sep__1_21:08:32_Central_Daylight_Time_2017</span><br><span class="line">Cuda compilation tools, release 9.0, V9.0.176</span><br></pre></td></tr></table></figure></p><p>到此，cuda9.0 算是安装完成。</p><h5 id="3-安装-cudnn7"><a href="#3-安装-cudnn7" class="headerlink" title="3. 安装 cudnn7"></a>3. 安装 cudnn7</h5><p>安装好 cuda9.0 之后，需要安装与之版本相对应的性能优化库：cudnn7，cudnn7 的下载路径：<a href="https://developer.nvidia.com/cudnn" target="_blank" rel="noopener">下载 cudnn7</a>，NVIDIA要求用户登录之后才开放下载 cudnn7 的下载通道，不想登录下载的可以去我的网盘下载。</p><blockquote><p>网盘链接：<a href="https://pan.baidu.com/s/1FUl4xEvt-SFeE3saso6-xw" target="_blank" rel="noopener">https://pan.baidu.com/s/1FUl4xEvt-SFeE3saso6-xw</a><br>提取码：p55p<br>这个版本的 cudnn7 是针对 cuda9.0 的</p></blockquote><p>这是一个压缩包文件，下载好之后解压会出现一个 <code>cuda</code> 文件夹，将文件夹中的东西复制到 <strong>NVIDIA GPU Computing Toolkit</strong> 的安装路径下。路径默认会在：<code>C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v9.0</code>，复制完成，cudnn 也算安装完成了。</p><h5 id="4-安装-anaconda3"><a href="#4-安装-anaconda3" class="headerlink" title="4. 安装 anaconda3"></a>4. 安装 anaconda3</h5><p>gpu 的相关软件设置安装完成，下面就是安装代码所需要的 python 环境了。<br>Anaconda 是一个开源的 python发行版本，其中包括了许多常用的 python 第三方库，相对于传统的 python2&amp;python3 个人感觉更为方便，所以在这里 python 环境使用了 anaconda3 ，默认 python 版本是 3.6 。anaconda3 安装文件的下载路径：<a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">下载 anaconda</a>，可能一部分网络在下载的时候会很慢，所以我也放上我的网盘链接：</p><blockquote><p>链接：<a href="https://pan.baidu.com/s/15MSq0irNJ9HgVEPRftQy7A" target="_blank" rel="noopener">https://pan.baidu.com/s/15MSq0irNJ9HgVEPRftQy7A</a><br>提取码：23gv</p></blockquote><p>下载完成之后，打开安装文件，一步步的按照默认选项进行安装，也可以自定义安装路径。<br>安装完成之后按下 <code>Windows键</code>，就能再软件列表看到安装好的 Anaconda3 的相关信息了。</p><h5 id="5-安装-tensorflow-gpu"><a href="#5-安装-tensorflow-gpu" class="headerlink" title="5. 安装 tensorflow-gpu"></a>5. 安装 tensorflow-gpu</h5><p>按下 <code>Windows键</code>，在 Anaconda3 的列表下找到 <code>Anaconda Prompt</code> ,点击就会出现 anaconda 环境下的命令行窗口，在这里使用命令<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> pip install tensorflow-gpu</span></span><br></pre></td></tr></table></figure></p><p>即可安装最新版本的 tensorflow-gpu ，成功安装之后，使用以下命令进行检验：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> python</span></span><br><span class="line">Python 3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 13:32:41) [MSC v.1900 64 bit (AMD64)] on win32</span><br><span class="line">Type "help", "copyright", "credits" or "license" for more information.</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import tensorflow as tf</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; <span class="built_in">print</span>(tf.__version__)</span></span><br><span class="line">1.11.0</span><br></pre></td></tr></table></figure></p><p>可以成功的导入，并输出 tensorflow-gpu 的相关版本信息就算安装成功了。</p><h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>如果在导入 tensorflow 的过程中出现下面的错误：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ImportError: DLL load failed: 找不到指定的模块</span><br></pre></td></tr></table></figure></p><p>可以尝试重新安装 cuda9.0 和 cudnn7，即能解决问题。</p><p>如果在导入 tensorflow 的过程中出现下面的错误：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ModuleNotFoundError: No module named &apos;absl&apos;</span><br></pre></td></tr></table></figure></p><p>可以通过执行下面命令重新安装 absl-py 库来解决问题。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash"> pip uninstall absl-py</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash"> pip install absl-py</span></span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h4&gt;&lt;p&gt;为了更好的学习工作，清理了下电脑，重做了下系统，因此需要重新装深度学习相关的运行环境。在这里记录一下安装过程，以备后用。&lt;br&gt;环境：Windows 10 &amp;amp; vs-2015 &amp;amp; cuda9 &amp;amp; cudnn7 &amp;amp; anaconda3 &amp;amp; tensorflow-gpu&lt;/p&gt;
&lt;h4 id=&quot;安装步骤&quot;&gt;&lt;a href=&quot;#安装步骤&quot; class=&quot;headerlink&quot; title=&quot;安装步骤&quot;&gt;&lt;/a&gt;安装步骤&lt;/h4&gt;&lt;h5 id=&quot;1-安装-vs2015&quot;&gt;&lt;a href=&quot;#1-安装-vs2015&quot; class=&quot;headerlink&quot; title=&quot;1. 安装 vs2015&quot;&gt;&lt;/a&gt;1. 安装 vs2015&lt;/h5&gt;&lt;p&gt;首先在安装 cuda 之前需要安装 vs，在这选择安装 vs2015，安装需要的 iso 文件在我的网盘里。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;网盘链接：&lt;a href=&quot;https://pan.baidu.com/s/10KQn-1yBqZ1vzkJnbzNL2A&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://pan.baidu.com/s/10KQn-1yBqZ1vzkJnbzNL2A&lt;/a&gt;&lt;br&gt;提取码：1aiq&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;下载完成之后，打开其中的 &lt;code&gt;vs_community.exe&lt;/code&gt; 文件进行安装，安装期间所有的选项均默认即可。安装路径也可以单独指定。&lt;br&gt;如果之前电脑里有 vs 的可以直接跳过此步骤，或者可以重新更新一下。&lt;br&gt;
    
    </summary>
    
      <category term="环境搭建" scheme="http://wgshun.github.io/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="cuda" scheme="http://wgshun.github.io/tags/cuda/"/>
    
      <category term="cudnn" scheme="http://wgshun.github.io/tags/cudnn/"/>
    
      <category term="windows" scheme="http://wgshun.github.io/tags/windows/"/>
    
      <category term="anaconda" scheme="http://wgshun.github.io/tags/anaconda/"/>
    
      <category term="tensorflow" scheme="http://wgshun.github.io/tags/tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04 下 Python3 虚拟环境安装 OpenCV</title>
    <link href="http://wgshun.github.io/2018/10/ubuntu-install-python3-opencv/"/>
    <id>http://wgshun.github.io/2018/10/ubuntu-install-python3-opencv/</id>
    <published>2018-10-25T13:51:41.000Z</published>
    <updated>2019-04-10T17:02:21.527Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>最近在看 Light Head Rcnn 的 Tensorflow 源码，在 Python3 下需要用到 OpenCV ，鉴于我的 Ubuntu 下已经装了 Python2.7 的 OpenCV2.4.13.7 <a href="https://wgshun.top/2018/06/ubuntu-install-opencv/" target="_blank" rel="noopener">点击查看安装教程</a>。所以这次在虚拟环境下配置一下 Python3 的 OpenCV，按照以下步骤直接装了最新的 OpenCV4.0.0-pre。</p><h4 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h4><h5 id="1-各种依赖包的安装"><a href="#1-各种依赖包的安装" class="headerlink" title="1. 各种依赖包的安装"></a>1. 各种依赖包的安装</h5><p>升级一些预安装的软件包：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get update</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get upgrade</span></span><br></pre></td></tr></table></figure></p><p>安装一些编译 OpenCV 所需要的开发工具：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install build-essential cmake git pkg-config</span></span><br></pre></td></tr></table></figure></p><p>安装一些用于从磁盘中读取各种图片格式所需要的依赖包：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install libjpeg8-dev libtiff4-dev libjasper-dev libpng12-dev</span></span><br></pre></td></tr></table></figure></p><a id="more"></a><p>安装一些用于从磁盘中读取视频所需要的依赖包：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install libavcodec-dev libavformat-dev libswscale-dev libv4l-dev</span></span><br></pre></td></tr></table></figure></p><p>安装以后使用 OpenCV 的 GUI 时所需要的 GTK：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install libgtk2.0-dev</span></span><br></pre></td></tr></table></figure></p><p>安装用于优化 OpenCV 内部各种功能的依赖包，例如矩阵操作：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install libatlas-base-dev gfortran</span></span><br></pre></td></tr></table></figure></p><h5 id="2-设置-Python3-的相关操作"><a href="#2-设置-Python3-的相关操作" class="headerlink" title="2. 设置 Python3 的相关操作"></a>2. 设置 Python3 的相关操作</h5><p>首先安装 Python 的包管理工具 <code>pip</code>。安装 Python3 的 <code>pip</code> ：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> wget https://bootstrap.pypa.io/get-pip.py</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo python3 get-pip.py</span></span><br></pre></td></tr></table></figure></p><p><strong>注意：</strong> 在安装 <code>pip</code> 的时候使用的是 python3，如果不用 python3 的话，默认安装的将是 python2 的。</p><p>本文前言中说了：是在虚拟环境下安装 Python3 的 OpenCV。所以在这里搭建 Python3 的虚拟环境需要用到 <strong>virtualenv</strong> 和 <strong>virtualenvwrapper</strong> ，下面先使用 <code>pip</code> 安装 <strong>virtualenv</strong> 和 <strong>virtualenvwrapper</strong> 。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo pip3 install virtualenv virtualenvwrapper</span></span><br></pre></td></tr></table></figure></p><p><strong>注意：</strong> 在这里使用的也是 <code>pip3</code> ，而不是 <code>pip</code> ，使用 <code>pip3</code> 安装确保将这两个包装在 Python3 的环境下。</p><p>下面就要更新环境变量了，在 <code>~/.bashrc</code> 文件的最后添加上以下环境变量信息：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># virtualenv and virtualenvwrapper</span><br><span class="line">export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3</span><br><span class="line">export WORKON_HOME=$HOME/.virtualenvs</span><br><span class="line">source /usr/local/bin/virtualenvwrapper.sh</span><br></pre></td></tr></table></figure></p><p><strong>注意一下</strong>在这里是怎么将 <code>VIRTUALENVWRAPPER_PYTHON</code> 指向 Python3 所在的 Ubuntu 系统路径的。</p><p>为了确保刚刚配置的环境变量有效，执行以下命令：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> ~/.bashrc</span></span><br></pre></td></tr></table></figure></p><p>然后，创建下面安装 Python3 的 OpenCV 需要使用 <strong>cv</strong> 虚拟环境：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkvirtualenv cv</span></span><br></pre></td></tr></table></figure></p><p>这时可以看到终端中用户名的前面有了 <strong>(cv)</strong> 的标识。目前算是完成了 Python 设置的一半了，下面需要安装 Python3 的开发文件：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install python3.5-dev</span></span><br></pre></td></tr></table></figure></p><p>我这里的 Python3 的版本是 3.5 ，所以我装的是 <code>python3.5-dev</code>。<br>OpenCV 还需要用到 Numpy ，下一步安装 Numpy ：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pip install numpy</span></span><br></pre></td></tr></table></figure></p><p><strong>注意：</strong> 这一步出错的朋友请移步本文开头的参考链接，那里有解决方案。</p><h5 id="3-安装-OpenCV"><a href="#3-安装-OpenCV" class="headerlink" title="3. 安装 OpenCV"></a>3. 安装 OpenCV</h5><p>到此，安装 OpenCV 所需要设置的环境都设置完毕，下面开始下载 OpenCV 的源码：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> git <span class="built_in">clone</span> https://github.com/Itseez/opencv.git</span></span><br></pre></td></tr></table></figure></p><p><strong>注意：</strong> 本文开头的参考文章中有安装 OpenCV 的 contrib库 ，目前我还用不到，所以就不占用那个空间去装了，需要的还请移步自取。</p><p>下载好源码以后就是编译了：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> opencv</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir release</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> release</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/<span class="built_in">local</span> -D INSTALL_C_EXAMPLES=ON -D INSTALL_PYTHON_EXAMPLES=ON -D BUILD_EXAMPLES=ON ..</span></span><br></pre></td></tr></table></figure></p><p>cmake 命令结束以后就可以看到以下 python3 的信息了：</p><img src="/2018/10/ubuntu-install-python3-opencv/1.png"><p><strong>注意：</strong> 在 cmake 以后看到 python3 的信息才能进行下面的操作。</p><p>下面开始 OpenCV 的编译操作：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> make -j4</span></span><br></pre></td></tr></table></figure></p><p><strong>注意：</strong> 上述命令中的 <strong>4</strong> 根据处理器的内核数来设置，可以加快编译速度。</p><p>在编译的时候遇到错误：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">fatal error: Eigen/Eigenvalues: No such file or directory</span><br><span class="line">compilation terminated.</span><br></pre></td></tr></table></figure></p><p>解决方法：<br>重新 cmake ，在 cmake 的时候加入参数 <code>-D WITH_EIGEN=OFF</code>，然后重新编译就通过了。</p><p>在没有报错编译完成之后，可以执行下列命令将 OpenCV 装到系统里：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo make install</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo ldconfig</span></span><br></pre></td></tr></table></figure></p><h5 id="4-设置-OpenCV-的系统链接"><a href="#4-设置-OpenCV-的系统链接" class="headerlink" title="4. 设置 OpenCV 的系统链接"></a>4. 设置 OpenCV 的系统链接</h5><p>在没有错误的情况下执行完以上步骤，执行命令：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls -l /usr/<span class="built_in">local</span>/lib/python3.5/site-packages/</span></span><br></pre></td></tr></table></figure></p><p>就可以看到 OpenCV 的相关信息了：</p><img src="/2018/10/ubuntu-install-python3-opencv/2.png"><p>在这里 OpenCV 的名字是 <code>cv2.cpython-35m-x86_64-linux-gnu.so</code> 。<br>下面为了能在创建的 <strong>cv</strong> 虚拟环境中使用 OpenCV ，需要将这个 <code>site-packages</code> 文件夹下的 <code>cv2.cpython-35m-x86_64-linux-gnu.so</code> 文件链接到 <strong>cv</strong> 虚拟环境中：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> ~/.virtualenvs/cv/lib/python3.5/site-packages/</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ln -s /usr/<span class="built_in">local</span>/lib/python3.5/site-packages/cv2.cpython-35m-x86_64-linux-gnu.so cv2.so</span></span><br></pre></td></tr></table></figure></p><p>在上述命令中将 <code>cv2.cpython-35m-x86_64-linux-gnu.so</code> 的名字改成了 <code>cv2.so</code>，这样在 Python3 中就能直接导入 <strong>cv2</strong> 了。这时候使用 <code>ls</code> 命令就可以看到  <code>cv2.so</code> 文件了。<br>如果没有  <code>cv2.so</code> 文件，将不能导入 <strong>cv2</strong> 。</p><h5 id="5-测试-Python3-安装的-OpenCV"><a href="#5-测试-Python3-安装的-OpenCV" class="headerlink" title="5. 测试 Python3 安装的 OpenCV"></a>5. 测试 Python3 安装的 OpenCV</h5><p>打开终端，进入创建的 <strong>cv</strong> 虚拟环境，在 Python 中导入 cv2 查看 OpenCV 的版本：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> workon cv</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> python</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; import cv2</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; cv2.__version__</span></span><br><span class="line">'4.0.0-pre'</span><br></pre></td></tr></table></figure></p><p>结束，安装成功。</p><p>参考：<a href="https://www.pyimagesearch.com/2015/07/20/install-opencv-3-0-and-python-3-4-on-ubuntu/" target="_blank" rel="noopener">https://www.pyimagesearch.com/2015/07/20/install-opencv-3-0-and-python-3-4-on-ubuntu/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h4&gt;&lt;p&gt;最近在看 Light Head Rcnn 的 Tensorflow 源码，在 Python3 下需要用到 OpenCV ，鉴于我的 Ubuntu 下已经装了 Python2.7 的 OpenCV2.4.13.7 &lt;a href=&quot;https://wgshun.top/2018/06/ubuntu-install-opencv/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;点击查看安装教程&lt;/a&gt;。所以这次在虚拟环境下配置一下 Python3 的 OpenCV，按照以下步骤直接装了最新的 OpenCV4.0.0-pre。&lt;/p&gt;
&lt;h4 id=&quot;安装步骤&quot;&gt;&lt;a href=&quot;#安装步骤&quot; class=&quot;headerlink&quot; title=&quot;安装步骤&quot;&gt;&lt;/a&gt;安装步骤&lt;/h4&gt;&lt;h5 id=&quot;1-各种依赖包的安装&quot;&gt;&lt;a href=&quot;#1-各种依赖包的安装&quot; class=&quot;headerlink&quot; title=&quot;1. 各种依赖包的安装&quot;&gt;&lt;/a&gt;1. 各种依赖包的安装&lt;/h5&gt;&lt;p&gt;升级一些预安装的软件包：&lt;br&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; sudo apt-get update&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; sudo apt-get upgrade&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;安装一些编译 OpenCV 所需要的开发工具：&lt;br&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; sudo apt-get install build-essential cmake git pkg-config&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;安装一些用于从磁盘中读取各种图片格式所需要的依赖包：&lt;br&gt;&lt;figure class=&quot;highlight shell&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;bash&quot;&gt; sudo apt-get install libjpeg8-dev libtiff4-dev libjasper-dev libpng12-dev&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="环境搭建" scheme="http://wgshun.github.io/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="ubuntu" scheme="http://wgshun.github.io/tags/ubuntu/"/>
    
      <category term="python3" scheme="http://wgshun.github.io/tags/python3/"/>
    
      <category term="opencv" scheme="http://wgshun.github.io/tags/opencv/"/>
    
      <category term="linux" scheme="http://wgshun.github.io/tags/linux/"/>
    
  </entry>
  
  <entry>
    <title>Windows 10 安装 Desktop Ubuntu</title>
    <link href="http://wgshun.github.io/2018/10/windows10-install-desktop-ubuntu/"/>
    <id>http://wgshun.github.io/2018/10/windows10-install-desktop-ubuntu/</id>
    <published>2018-10-15T10:06:01.000Z</published>
    <updated>2019-04-10T17:03:42.860Z</updated>
    
    <content type="html"><![CDATA[<h5 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h5><p>前一段时间在 <strong>Windows 10</strong> 上装了 <strong>Bash</strong> 的 <strong>Ubuntu</strong> ，所以就想是不是也能装 <strong>Desktop</strong> 版本的。经过一番搜索，果断找到了。下面记录下过程：</p><h5 id="安装前提"><a href="#安装前提" class="headerlink" title="安装前提"></a>安装前提</h5><ul><li>新版本的 Windows 10（装有Windows Subsystem for Linux）</li><li>提前装了 Bash 版本的 Ubuntu 安装步骤可以参考：<a href="https://wgshun.top/2018/10/windows10-install-bash-ubuntu/" target="_blank" rel="noopener">Windows 10 安装 Bash Ubuntu</a></li></ul><h5 id="下载-windows-下的-X-service"><a href="#下载-windows-下的-X-service" class="headerlink" title="下载 windows 下的 X-service"></a>下载 windows 下的 X-service</h5><p>下载地址：<a href="https://sourceforge.net/projects/vcxsrv/" target="_blank" rel="noopener">https://sourceforge.net/projects/vcxsrv/</a><br><a id="more"></a><br>下载好，安装好以后，打开 <strong>XLaunch</strong> ，首次使用会弹出设置窗口，选择 <strong>One large window</strong>，设置 <strong>Display number</strong> 为 0 ，如下图</p><img src="/2018/10/windows10-install-desktop-ubuntu/desktop1.png"><p>下面其余的设置全部按默认走。</p><h5 id="在-Bash-版本-Ubuntu-里下载桌面"><a href="#在-Bash-版本-Ubuntu-里下载桌面" class="headerlink" title="在 Bash 版本 Ubuntu 里下载桌面"></a>在 Bash 版本 Ubuntu 里下载桌面</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get update</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo apt-get install ubuntu-desktop unity compiz-core compizconfig-settings-manager</span></span><br></pre></td></tr></table></figure><p>会下载六百多M的包，解压完有两个多G。</p><p><strong>注：</strong>在下载过程中网络要绝对的稳定，不然很容易卡着不动。卡着不动了不要惊慌，不要失措，可以选择继续等（经过一段时间有可能会再次动起来），也可以选择干掉重新来过。</p><h5 id="配置-compiz-窗口管理器"><a href="#配置-compiz-窗口管理器" class="headerlink" title="配置 compiz 窗口管理器"></a>配置 compiz 窗口管理器</h5><p>启动刚才安装的 <strong>XLaunch</strong> ，然后在 Bash 版的 Ubuntu 中执行命令：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">export</span> DISPLAY=localhost:0</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo ccsm</span></span><br></pre></td></tr></table></figure></p><p>然后在打开的  <strong>XLaunch</strong> 界面会出现 <strong>ccsm</strong> 的配置界面，勾选 <strong>Ubuntu Unity Plugin</strong> 选项，其余的默认就行不用管。然后关闭 <strong>ccsm</strong> ，关闭的时候会弹出几个窗口全部选确定。在 Bash 版的 Ubuntu 中执行命令：<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> sudo compiz</span></span><br></pre></td></tr></table></figure></p><p>看启动的  <strong>XLaunch</strong> 界面就会出现 <strong>Ubuntu</strong> 的 Desktop 界面了。</p><p>转载自：<a href="https://blog.csdn.net/u011138447/article/details/78262369?locationNum=4&amp;fps=1" target="_blank" rel="noopener">https://blog.csdn.net/u011138447/article/details/78262369?locationNum=4&amp;fps=1</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h5&gt;&lt;p&gt;前一段时间在 &lt;strong&gt;Windows 10&lt;/strong&gt; 上装了 &lt;strong&gt;Bash&lt;/strong&gt; 的 &lt;strong&gt;Ubuntu&lt;/strong&gt; ，所以就想是不是也能装 &lt;strong&gt;Desktop&lt;/strong&gt; 版本的。经过一番搜索，果断找到了。下面记录下过程：&lt;/p&gt;
&lt;h5 id=&quot;安装前提&quot;&gt;&lt;a href=&quot;#安装前提&quot; class=&quot;headerlink&quot; title=&quot;安装前提&quot;&gt;&lt;/a&gt;安装前提&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;新版本的 Windows 10（装有Windows Subsystem for Linux）&lt;/li&gt;
&lt;li&gt;提前装了 Bash 版本的 Ubuntu 安装步骤可以参考：&lt;a href=&quot;https://wgshun.top/2018/10/windows10-install-bash-ubuntu/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Windows 10 安装 Bash Ubuntu&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&quot;下载-windows-下的-X-service&quot;&gt;&lt;a href=&quot;#下载-windows-下的-X-service&quot; class=&quot;headerlink&quot; title=&quot;下载 windows 下的 X-service&quot;&gt;&lt;/a&gt;下载 windows 下的 X-service&lt;/h5&gt;&lt;p&gt;下载地址：&lt;a href=&quot;https://sourceforge.net/projects/vcxsrv/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://sourceforge.net/projects/vcxsrv/&lt;/a&gt;&lt;br&gt;
    
    </summary>
    
      <category term="环境搭建" scheme="http://wgshun.github.io/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="Windows" scheme="http://wgshun.github.io/tags/Windows/"/>
    
      <category term="desktop ubuntu" scheme="http://wgshun.github.io/tags/desktop-ubuntu/"/>
    
  </entry>
  
  <entry>
    <title>Windows 10 安装 Bash Ubuntu</title>
    <link href="http://wgshun.github.io/2018/10/windows10-install-bash-ubuntu/"/>
    <id>http://wgshun.github.io/2018/10/windows10-install-bash-ubuntu/</id>
    <published>2018-10-15T09:59:11.000Z</published>
    <updated>2019-04-10T17:03:26.707Z</updated>
    
    <content type="html"><![CDATA[<h5 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h5><p>听闻微软大哥在新版本的 <strong>Windows 10</strong> 里添加了个新功能，可以直接在  <strong>Windows 10</strong> 下直接安装使用 Bash 版本的 <strong>Ubuntu</strong> 。</p><h5 id="安装前提"><a href="#安装前提" class="headerlink" title="安装前提"></a>安装前提</h5><ul><li>新版本的 64位的Windows 10（装有Windows Subsystem for Linux）</li></ul><h5 id="安装-Ubuntu"><a href="#安装-Ubuntu" class="headerlink" title="安装 Ubuntu"></a>安装 Ubuntu</h5><p>打开 win10 的 Windows 设置，如下图<br><a id="more"></a></p><img src="/2018/10/windows10-install-bash-ubuntu/bash1.png"><p>在 <code>查找设置</code> 输入框输入 <strong>Windows 功能</strong> 会出现 <code>启用或关闭 Windows 功能</code>，如下图</p><img src="/2018/10/windows10-install-bash-ubuntu/bash2.png"><p>点击进入会出现 Windows 功能窗口，勾选 <code>适用于 Linux 的 Windows 子系统</code>。如下图</p><img src="/2018/10/windows10-install-bash-ubuntu/bash3.png"><p>然后打开微软应用商店，如下图</p><img src="/2018/10/windows10-install-bash-ubuntu/bash4.png"><p>搜索 <strong>Ubuntu</strong>，点击安装。如下图</p><img src="/2018/10/windows10-install-bash-ubuntu/bash5.png"><p>安装完成后就可以在应用列表搜索 <strong>Ubuntu</strong> 找到安装好的 Bash 版 <strong>Ubuntu</strong> ，点击就可以开始使用了。如下图</p><img src="/2018/10/windows10-install-bash-ubuntu/bash6.png"><p>首次使用会让你设置 <code>username</code> 和 <code>password</code> 。以后使用效果如下图：</p><img src="/2018/10/windows10-install-bash-ubuntu/bash7.png"><p>完。</p>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h5&gt;&lt;p&gt;听闻微软大哥在新版本的 &lt;strong&gt;Windows 10&lt;/strong&gt; 里添加了个新功能，可以直接在  &lt;strong&gt;Windows 10&lt;/strong&gt; 下直接安装使用 Bash 版本的 &lt;strong&gt;Ubuntu&lt;/strong&gt; 。&lt;/p&gt;
&lt;h5 id=&quot;安装前提&quot;&gt;&lt;a href=&quot;#安装前提&quot; class=&quot;headerlink&quot; title=&quot;安装前提&quot;&gt;&lt;/a&gt;安装前提&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;新版本的 64位的Windows 10（装有Windows Subsystem for Linux）&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&quot;安装-Ubuntu&quot;&gt;&lt;a href=&quot;#安装-Ubuntu&quot; class=&quot;headerlink&quot; title=&quot;安装 Ubuntu&quot;&gt;&lt;/a&gt;安装 Ubuntu&lt;/h5&gt;&lt;p&gt;打开 win10 的 Windows 设置，如下图&lt;br&gt;
    
    </summary>
    
      <category term="环境搭建" scheme="http://wgshun.github.io/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="bash ubuntu" scheme="http://wgshun.github.io/tags/bash-ubuntu/"/>
    
      <category term="Windows" scheme="http://wgshun.github.io/tags/Windows/"/>
    
  </entry>
  
  <entry>
    <title>Hexo 博客绑定个人域名</title>
    <link href="http://wgshun.github.io/2018/07/custom-domain/"/>
    <id>http://wgshun.github.io/2018/07/custom-domain/</id>
    <published>2018-07-05T06:43:04.000Z</published>
    <updated>2019-04-10T17:00:55.188Z</updated>
    
    <content type="html"><![CDATA[<p>前段时间用 hexo 搭建的 gitpage 个人博客，服务器用的是 github 的，然后域名默认也是 github 下的二级域名：<code>username.github.io</code>, 现在为了提升格调准备将自己的博客指向一个新的域名。下面记录下过程。</p><h4 id="购买域名"><a href="#购买域名" class="headerlink" title="购买域名"></a>购买域名</h4><p>国内的域名服务商有<a href="http://www.xinnet.com/domain/domain.html" target="_blank" rel="noopener">新网</a>，<a href="https://dnspod.cloud.tencent.com/" target="_blank" rel="noopener">腾讯云</a>，还有阿里云的<a href="https://wanwang.aliyun.com/" target="_blank" rel="noopener">万网</a>等。下面以阿里云的万网为例：</p><p>在万网购买了自己心仪的域名后，进入阿里云的<code>管理控制台</code>-<code>域名与网站</code>-<code>域名</code>就可以看到购买的域名此时的域名状态是未实名认证的，然后就是实名认证（一般需要2小时左右）。</p><h4 id="域名解析"><a href="#域名解析" class="headerlink" title="域名解析"></a>域名解析</h4><p>首先获取自己 github 的二级域名的 IP地址，windows 下直接在 cmd 里 Ping 一下自己的博客就会得到 IP 地址：<br><a id="more"></a></p><img src="/2018/07/custom-domain/Customdomain1.jpg"><p>下面通过 DNS域名解析将购买的域名指向 github 的二级域名：<code>username.github.io</code>，进入阿里云的<code>管理控制台</code>-<code>域名与网站</code>-<code>云解析 DNS</code>，进入域名的解析设置，点击新手指导，将得到的 IP 地址填到记录值一栏，点击确定就 OK 了。填完以后的解析列表会出现：</p><img src="/2018/07/custom-domain/Customdomain2.jpg"><p>记录值就是自己 github 的二级域名的 IP地址。</p><h4 id="设置CNAME"><a href="#设置CNAME" class="headerlink" title="设置CNAME"></a>设置CNAME</h4><p>在 hexo 项目下，<code>source</code> 文件夹下面创建 CNAME 文件（没有后缀名的），在里面写上购买的域名。比如：</p><img src="/2018/07/custom-domain/Customdomain3.jpg"><p>在 github 上面，打开 username.github.io 项目的（Settings）设置，然后在 <code>GitHub Pages</code>的 <code>Custom domain</code>设置里填上购买的域名。比如：</p><img src="/2018/07/custom-domain/Customdomain4.jpg"><p>好了，新域名配置完成，可以访问了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;前段时间用 hexo 搭建的 gitpage 个人博客，服务器用的是 github 的，然后域名默认也是 github 下的二级域名：&lt;code&gt;username.github.io&lt;/code&gt;, 现在为了提升格调准备将自己的博客指向一个新的域名。下面记录下过程。&lt;/p&gt;
&lt;h4 id=&quot;购买域名&quot;&gt;&lt;a href=&quot;#购买域名&quot; class=&quot;headerlink&quot; title=&quot;购买域名&quot;&gt;&lt;/a&gt;购买域名&lt;/h4&gt;&lt;p&gt;国内的域名服务商有&lt;a href=&quot;http://www.xinnet.com/domain/domain.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;新网&lt;/a&gt;，&lt;a href=&quot;https://dnspod.cloud.tencent.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;腾讯云&lt;/a&gt;，还有阿里云的&lt;a href=&quot;https://wanwang.aliyun.com/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;万网&lt;/a&gt;等。下面以阿里云的万网为例：&lt;/p&gt;
&lt;p&gt;在万网购买了自己心仪的域名后，进入阿里云的&lt;code&gt;管理控制台&lt;/code&gt;-&lt;code&gt;域名与网站&lt;/code&gt;-&lt;code&gt;域名&lt;/code&gt;就可以看到购买的域名此时的域名状态是未实名认证的，然后就是实名认证（一般需要2小时左右）。&lt;/p&gt;
&lt;h4 id=&quot;域名解析&quot;&gt;&lt;a href=&quot;#域名解析&quot; class=&quot;headerlink&quot; title=&quot;域名解析&quot;&gt;&lt;/a&gt;域名解析&lt;/h4&gt;&lt;p&gt;首先获取自己 github 的二级域名的 IP地址，windows 下直接在 cmd 里 Ping 一下自己的博客就会得到 IP 地址：&lt;br&gt;
    
    </summary>
    
      <category term="博客技术" scheme="http://wgshun.github.io/categories/%E5%8D%9A%E5%AE%A2%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="hexo" scheme="http://wgshun.github.io/tags/hexo/"/>
    
      <category term="绑定域名" scheme="http://wgshun.github.io/tags/%E7%BB%91%E5%AE%9A%E5%9F%9F%E5%90%8D/"/>
    
  </entry>
  
  <entry>
    <title>Hexo 的 Next 主题中渲染 MathJax 数学公式</title>
    <link href="http://wgshun.github.io/2018/07/mathjax-in-hexo/"/>
    <id>http://wgshun.github.io/2018/07/mathjax-in-hexo/</id>
    <published>2018-07-05T03:32:22.000Z</published>
    <updated>2018-10-25T13:13:27.886Z</updated>
    
    <content type="html"><![CDATA[<p>在用markdown写技术文档时，免不了会碰到数学公式。常用的Markdown编辑器都会集成Mathjax，用来渲染文档中的类Latex格式书写的数学公式。基于Hexo搭建的个人博客，默认情况下渲染数学公式却会出现各种各样的问题。</p><p>这个问题搞了好久才找到解决方案，感谢@<a href="http://xudongyang.coding.me/math-in-hexo/" target="_blank" rel="noopener">小毛驴</a></p><h4 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h4><p>Hexo 默认使用 ”hexo-renderer-marked” 引擎渲染网页，该引擎会把一些特殊的 markdown 符号转换为相应的 html 标签，比如在 markdown 语法中，下划线<code>_</code>代表斜体，会被渲染引擎处理为<code>&lt;em&gt;</code>标签。</p><p>因为类 Latex 格式书写的数学公式下划线<code>_</code>表示下标，有特殊的含义，如果被强制转换为<code>&lt;em&gt;</code>标签，那么 MathJax 引擎在渲染数学公式的时候就会出错。</p><p>类似的语义冲突的符号还包括<code>*</code>, <code>{</code>, <code>}</code>, <code>\\</code>等。<br><a id="more"></a></p><h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><p>更换 Hexo 的 markdown 渲染引擎，hexo-renderer-kramed 引擎是在默认的渲染引擎 hexo-renderer-marked 的基础上修改了一些 bug ，两者比较接近，也比较轻量级。<br><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> npm uninstall hexo-renderer-marked --save</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> npm install hexo-renderer-kramed --save</span></span><br></pre></td></tr></table></figure></p><p>执行上面的命令即可，先卸载原来的渲染引擎，再安装新的。<br>然后，跟换引擎后行间公式可以正确渲染了，但是这样还没有完全解决问题，行内公式的渲染还是有问题，因为 hexo-renderer-kramed 引擎也有语义冲突的问题。接下来到博客根目录下，找到node_modules\kramed\lib\rules\inline.js，把第11行的 escape 变量的值做相应的修改：<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//escape: /^\\([<span class="string">\\`*&#123;&#125;\[\</span>](<span class="link"></span>)#$+\-.!_&gt;])/,</span><br><span class="line">escape: /^\\([<span class="string">`*\[\</span>](<span class="link"></span>)#$+\-.!_&gt;])/,</span><br></pre></td></tr></table></figure></p><p>这一步是在原基础上取消了对\\,\{,\}的转义(escape)。<br>同时把第20行的em变量也要做相应的修改。<br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//em: /^\b<span class="emphasis">_((?:_</span><span class="emphasis">_|[\s\S])+?)_</span>\b|^\<span class="emphasis">*((?:\*</span>\<span class="emphasis">*|[\s\S])+?)\*</span>(?!\*)/,</span><br><span class="line">em: /^\<span class="emphasis">*((?:\*</span>\<span class="emphasis">*|[\s\S])+?)\*</span>(?!\*)/,</span><br></pre></td></tr></table></figure></p><p>重新启动hexo（先clean再generate）,问题完美解决。哦，如果不幸还没解决的话，看看是不是还需要在使用的主题中配置mathjax开关。</p><h5 id="在-Next-主题中开启-MathJax-开关"><a href="#在-Next-主题中开启-MathJax-开关" class="headerlink" title="在 Next 主题中开启 MathJax 开关"></a>在 Next 主题中开启 MathJax 开关</h5><p>如何使用了主题了，别忘了在主题（Theme）中开启 MathJax 开关，下面以 next 主题为例，介绍下如何打开 MathJax 开关。</p><p>进入到主题目录，找到 _config.yml 配置问题，把 math 默认的 false 修改为true，具体如下： </p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Math Equations Render Support</span></span><br><span class="line"><span class="attr">math:</span></span><br><span class="line"><span class="attr">  enable:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Default(true) will load mathjax/katex script on demand</span></span><br><span class="line">  <span class="comment"># That is it only render those page who has 'mathjax: true' in Front Matter.</span></span><br><span class="line">  <span class="comment"># If you set it to false, it will load mathjax/katex srcipt EVERY PAGE.</span></span><br><span class="line"><span class="attr">  per_page:</span> <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="attr">  engine:</span> <span class="string">mathjax</span></span><br><span class="line">  <span class="comment">#engine: katex</span></span><br></pre></td></tr></table></figure><p>还需要在文章的Front-matter里打开mathjax开关，如下：<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">index.html</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2018</span><span class="bullet">-07</span><span class="bullet">-05</span> <span class="number">12</span><span class="string">:01:30</span></span><br><span class="line"><span class="attr">tags:</span></span><br><span class="line"><span class="attr">mathjax:</span> <span class="literal">true</span></span><br><span class="line"><span class="bullet">-</span><span class="bullet">-</span></span><br></pre></td></tr></table></figure></p><p>之所以要在文章头里设置开关，是因为考虑只有在用到公式的页面才加载 Mathjax，这样不需要渲染数学公式的页面的访问速度就不会受到影响了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在用markdown写技术文档时，免不了会碰到数学公式。常用的Markdown编辑器都会集成Mathjax，用来渲染文档中的类Latex格式书写的数学公式。基于Hexo搭建的个人博客，默认情况下渲染数学公式却会出现各种各样的问题。&lt;/p&gt;
&lt;p&gt;这个问题搞了好久才找到解决方案，感谢@&lt;a href=&quot;http://xudongyang.coding.me/math-in-hexo/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;小毛驴&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&quot;原因&quot;&gt;&lt;a href=&quot;#原因&quot; class=&quot;headerlink&quot; title=&quot;原因&quot;&gt;&lt;/a&gt;原因&lt;/h4&gt;&lt;p&gt;Hexo 默认使用 ”hexo-renderer-marked” 引擎渲染网页，该引擎会把一些特殊的 markdown 符号转换为相应的 html 标签，比如在 markdown 语法中，下划线&lt;code&gt;_&lt;/code&gt;代表斜体，会被渲染引擎处理为&lt;code&gt;&amp;lt;em&amp;gt;&lt;/code&gt;标签。&lt;/p&gt;
&lt;p&gt;因为类 Latex 格式书写的数学公式下划线&lt;code&gt;_&lt;/code&gt;表示下标，有特殊的含义，如果被强制转换为&lt;code&gt;&amp;lt;em&amp;gt;&lt;/code&gt;标签，那么 MathJax 引擎在渲染数学公式的时候就会出错。&lt;/p&gt;
&lt;p&gt;类似的语义冲突的符号还包括&lt;code&gt;*&lt;/code&gt;, &lt;code&gt;{&lt;/code&gt;, &lt;code&gt;}&lt;/code&gt;, &lt;code&gt;\\&lt;/code&gt;等。&lt;br&gt;
    
    </summary>
    
      <category term="博客技术" scheme="http://wgshun.github.io/categories/%E5%8D%9A%E5%AE%A2%E6%8A%80%E6%9C%AF/"/>
    
    
      <category term="hexo" scheme="http://wgshun.github.io/tags/hexo/"/>
    
      <category term="machjax" scheme="http://wgshun.github.io/tags/machjax/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04 安装 TensorRT</title>
    <link href="http://wgshun.github.io/2018/06/ubuntu-install-tensorrt/"/>
    <id>http://wgshun.github.io/2018/06/ubuntu-install-tensorrt/</id>
    <published>2018-06-07T03:43:59.000Z</published>
    <updated>2018-12-12T19:04:37.327Z</updated>
    
    <content type="html"><![CDATA[<h4 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h4><p>首先下载<code>tar</code>版本的安装包，<a href="https://developer.nvidia.com/tensorrt" target="_blank" rel="noopener">下载地址</a>需要登陆<code>NVIDIA</code>。<br>安装<code>TensorRT</code>前需要安装<code>Cuda</code>和<code>cudnn</code>，安装步骤可以参考<a href="https://http://www.wgshun.top/2018/06/ubuntu-install-cuda-and-cudnn/" target="_blank" rel="noopener"> ubuntu安装cuda和cudnn</a>。<br>打开下载的<code>TensorRT</code>所在路径，解压下载的<code>tar</code>文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tar -xzvf TensorRT-XXX.tar.gz</span><br></pre></td></tr></table></figure></p><p>解压好添加环境变量：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ vim ~/.bashrc <span class="comment"># 打开环境变量文件</span></span><br><span class="line"><span class="comment"># 将下面三个环境变量写入环境变量文件并保存</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=TensorRT解压路径/lib:<span class="variable">$LD_LIBRARY_PATH</span></span><br><span class="line"><span class="built_in">export</span> CUDA_INSTALL_DIR=/usr/<span class="built_in">local</span>/cuda-9.0</span><br><span class="line"><span class="built_in">export</span> CUDNN_INSTALL_DIR=/usr/<span class="built_in">local</span>/cuda-9.0</span><br><span class="line"><span class="comment"># 使刚刚修改的环境变量文件生效</span></span><br><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure></p><a id="more"></a><p>下面是安装<code>Python</code>的<code>TensorRT</code>包：<br>进到解压的<code>TensorRT</code>目录下的<code>Python</code>目录：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于python2</span></span><br><span class="line">$ sudo pip2 install tensorrt-XXX-cp27-cp27mu-linux_x86_64.whl</span><br><span class="line"><span class="comment"># 对于python3</span></span><br><span class="line">$ sudo pip3 install tensorrt-XXX-cp35-cp35m-linux_x86_64.whl</span><br></pre></td></tr></table></figure></p><p><strong>如安装失败请参考文章末尾的解决方案。</strong><br>测试<code>TensorRT</code>是否安装成功，进入Python编辑器加载<code>tensorrt</code>：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import tensorrt</span><br><span class="line">&gt;&gt;&gt; tensorrt.__version__</span><br></pre></td></tr></table></figure></p><p>会输出<code>TensorRT</code>的版本号，即安装成功。<br>然后转到<code>uff</code>目录下安装<code>uff</code>包：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于python2</span></span><br><span class="line">$ sudo pip2 install uff-0.1.0rc0-py2.py3-none-any.whl</span><br><span class="line"><span class="comment"># 对于python3</span></span><br><span class="line">$ sudo pip3 install uff-0.1.0rc0-py2.py3-none-any.whl</span><br></pre></td></tr></table></figure></p><p>测试：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">which</span> convert-to-uff</span><br></pre></td></tr></table></figure></p><p>会输出<code>uff</code>的安装路径。<br>拷贝<code>lenet5.uff</code>到<code>python</code>相关目录进行验证：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sudo cp TensorRT-XXX/data/mnist/lenet5.uff TensorRT-XXX/python/data/mnist/lenet5.uff</span><br><span class="line">$ <span class="built_in">cd</span> TensorRT-XXX/samples/sampleMNIST</span><br><span class="line">$ make clean</span><br><span class="line">$ make</span><br><span class="line">$ <span class="built_in">cd</span> /TensorRT-XXX/bin（转到bin目录下面，make后的可执行文件在此目录下）</span><br><span class="line">$ ./sample_mnist</span><br></pre></td></tr></table></figure></p><p>命令执行顺利即安装成功。</p><h5 id="错误"><a href="#错误" class="headerlink" title="错误"></a>错误</h5><p>在安装<code>Python</code>的<code>TensorRT</code>包时可能出现的错误：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In file included from src/cpp/cuda.cpp:1:0:</span><br><span class="line">    src/cpp/cuda.hpp:14:18: fatal error: cuda.h: No such file or directory</span><br><span class="line">    compilation terminated.</span><br><span class="line">    error: <span class="built_in">command</span> <span class="string">'x86_64-linux-gnu-gcc'</span> failed with <span class="built_in">exit</span> status 1</span><br></pre></td></tr></table></figure></p><h5 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h5><p>显示是找不到cuda.h，根据网上分析是因为用了sudo之后环境变量用的是root的环境变量。<br><a href="https://codeyarns.com/2015/07/31/pip-install-error-with-pycuda/" target="_blank" rel="noopener">参考</a></p><h5 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h5><p>将<code>cuda</code>的安装路径添加到<code>root</code>的环境变量中，在<code>root</code>角色下安装<code>Python</code>的<code>TensorRT</code>包<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo vim /etc/profile.d/cuda.sh</span><br><span class="line">添加：<span class="built_in">export</span> PATH=/usr/<span class="built_in">local</span>/cuda-9.0/bin:<span class="variable">$PATH</span></span><br><span class="line">$ sudo su -</span><br><span class="line">$ pip2 install tensorrt-XXX-cp27-cp27mu-linux_x86_64.whl </span><br><span class="line">$ <span class="built_in">exit</span></span><br></pre></td></tr></table></figure></p><p>参考：<a href="https://blog.csdn.net/xll_bit/article/details/78376320" target="_blank" rel="noopener">https://blog.csdn.net/xll_bit/article/details/78376320</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;安装步骤&quot;&gt;&lt;a href=&quot;#安装步骤&quot; class=&quot;headerlink&quot; title=&quot;安装步骤&quot;&gt;&lt;/a&gt;安装步骤&lt;/h4&gt;&lt;p&gt;首先下载&lt;code&gt;tar&lt;/code&gt;版本的安装包，&lt;a href=&quot;https://developer.nvidia.com/tensorrt&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;下载地址&lt;/a&gt;需要登陆&lt;code&gt;NVIDIA&lt;/code&gt;。&lt;br&gt;安装&lt;code&gt;TensorRT&lt;/code&gt;前需要安装&lt;code&gt;Cuda&lt;/code&gt;和&lt;code&gt;cudnn&lt;/code&gt;，安装步骤可以参考&lt;a href=&quot;https://http://www.wgshun.top/2018/06/ubuntu-install-cuda-and-cudnn/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt; ubuntu安装cuda和cudnn&lt;/a&gt;。&lt;br&gt;打开下载的&lt;code&gt;TensorRT&lt;/code&gt;所在路径，解压下载的&lt;code&gt;tar&lt;/code&gt;文件：&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ tar -xzvf TensorRT-XXX.tar.gz&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;解压好添加环境变量：&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ vim ~/.bashrc &lt;span class=&quot;comment&quot;&gt;# 打开环境变量文件&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 将下面三个环境变量写入环境变量文件并保存&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; LD_LIBRARY_PATH=TensorRT解压路径/lib:&lt;span class=&quot;variable&quot;&gt;$LD_LIBRARY_PATH&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; CUDA_INSTALL_DIR=/usr/&lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt;/cuda-9.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;export&lt;/span&gt; CUDNN_INSTALL_DIR=/usr/&lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt;/cuda-9.0&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 使刚刚修改的环境变量文件生效&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ &lt;span class=&quot;built_in&quot;&gt;source&lt;/span&gt; ~/.bashrc&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="环境搭建" scheme="http://wgshun.github.io/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="ubuntu" scheme="http://wgshun.github.io/tags/ubuntu/"/>
    
      <category term="cuda" scheme="http://wgshun.github.io/tags/cuda/"/>
    
      <category term="cudnn" scheme="http://wgshun.github.io/tags/cudnn/"/>
    
      <category term="tensorrt" scheme="http://wgshun.github.io/tags/tensorrt/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu 系统查询 cuda 和 cudnn 版本号</title>
    <link href="http://wgshun.github.io/2018/06/ubuntu-inquire-cuda-and-cudnn-version/"/>
    <id>http://wgshun.github.io/2018/06/ubuntu-inquire-cuda-and-cudnn-version/</id>
    <published>2018-06-06T07:56:06.000Z</published>
    <updated>2018-10-25T13:13:59.989Z</updated>
    
    <content type="html"><![CDATA[<h5 id="查询-cuda-版本号"><a href="#查询-cuda-版本号" class="headerlink" title="查询 cuda 版本号"></a>查询 cuda 版本号</h5><p>命令行输入：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cat /usr/<span class="built_in">local</span>/cuda/version.txt</span><br></pre></td></tr></table></figure></p><p>会输出如下信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; CUDA Version 8.0.61</span><br></pre></td></tr></table></figure></p><p>显示<code>cuda</code>版本号为：<code>8.0.61</code></p><h5 id="查询-cudnn-版本号"><a href="#查询-cudnn-版本号" class="headerlink" title="查询 cudnn 版本号"></a>查询 cudnn 版本号</h5><p>命令行输入：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cat /usr/<span class="built_in">local</span>/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</span><br></pre></td></tr></table></figure></p><a id="more"></a><p>会输出如下信息：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#define CUDNN_MAJOR 7</span><br><span class="line">#define CUDNN_MINOR 0</span><br><span class="line">#define CUDNN_PATCHLEVEL 1</span><br><span class="line">--</span><br><span class="line">#define CUDNN_VERSION (CUDNN_MAJOR * 1000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)</span><br><span class="line">#include &quot;driver_types.h&quot;</span><br></pre></td></tr></table></figure></p><p>显示<code>cudnn</code>版本号为：<code>7.0.1</code></p>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;查询-cuda-版本号&quot;&gt;&lt;a href=&quot;#查询-cuda-版本号&quot; class=&quot;headerlink&quot; title=&quot;查询 cuda 版本号&quot;&gt;&lt;/a&gt;查询 cuda 版本号&lt;/h5&gt;&lt;p&gt;命令行输入：&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ cat /usr/&lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt;/cuda/version.txt&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;会输出如下信息：&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&amp;gt;&amp;gt;&amp;gt; CUDA Version 8.0.61&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;显示&lt;code&gt;cuda&lt;/code&gt;版本号为：&lt;code&gt;8.0.61&lt;/code&gt;&lt;/p&gt;
&lt;h5 id=&quot;查询-cudnn-版本号&quot;&gt;&lt;a href=&quot;#查询-cudnn-版本号&quot; class=&quot;headerlink&quot; title=&quot;查询 cudnn 版本号&quot;&gt;&lt;/a&gt;查询 cudnn 版本号&lt;/h5&gt;&lt;p&gt;命令行输入：&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ cat /usr/&lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt;/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="环境搭建" scheme="http://wgshun.github.io/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="ubuntu" scheme="http://wgshun.github.io/tags/ubuntu/"/>
    
      <category term="cuda" scheme="http://wgshun.github.io/tags/cuda/"/>
    
      <category term="cudnn" scheme="http://wgshun.github.io/tags/cudnn/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04 安装 Caffe</title>
    <link href="http://wgshun.github.io/2018/06/ubuntu-install-caffe/"/>
    <id>http://wgshun.github.io/2018/06/ubuntu-install-caffe/</id>
    <published>2018-06-06T06:27:35.000Z</published>
    <updated>2018-10-25T13:19:17.063Z</updated>
    
    <content type="html"><![CDATA[<h4 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h4><p>首先安装各种依赖包：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev python-dev libgflags-dev libatlas-base-dev libhdf5-serial-dev protobuf-compiler</span><br><span class="line">$ sudo apt-get install --no-install-recommends libboost-all-dev</span><br></pre></td></tr></table></figure></p><p>从<code>github</code>上面拷贝下来<code>caffe</code>项目：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/BVLC/caffe.git</span><br><span class="line">$ <span class="built_in">cd</span> caffe</span><br></pre></td></tr></table></figure></p><p>安装<code>caffe</code>版的<code>SSD</code>拷贝步骤为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/weiliu89/caffe.git</span><br><span class="line">$ <span class="built_in">cd</span> caffe</span><br><span class="line">$ git checkout ssd</span><br></pre></td></tr></table></figure></p><p>然后将<code>caffe</code>主目录下面的<code>Makefile.config.example</code>拷贝更名为<code>Makefile.config</code>，打开操作：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ cp Makefile.config.example Makefile.config</span><br><span class="line">$ gedit Makefile.config</span><br></pre></td></tr></table></figure></p><a id="more"></a><p>将其中的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#USE_CUDNN := 1</span><br><span class="line">#WITH_PYTHON_LAYER := 1</span><br><span class="line">INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include</span><br><span class="line">LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib</span><br></pre></td></tr></table></figure></p><p>分别更改为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">USE_CUDNN := 1</span><br><span class="line">WITH_PYTHON_LAYER := 1</span><br><span class="line">INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial</span><br><span class="line">LIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial</span><br></pre></td></tr></table></figure></p><p>打开 <code>Makefile</code>文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ gedit Makefile</span><br></pre></td></tr></table></figure></p><p>将其中的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NVCCFLAGS += -ccbin=$(CXX) -Xcompiler-fPIC $(COMMON_FLAGS)</span><br></pre></td></tr></table></figure></p><p>更改为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NVCCFLAGS += -D_FORCE_INLINES -ccbin=$(CXX) -Xcompiler -fPIC $(COMMON_FLAGS)</span><br></pre></td></tr></table></figure></p><p>打开<code>/usr/local/cuda/include/crt/host_config.h</code>文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo gedit /usr/<span class="built_in">local</span>/cuda/include/crt/host_config.h</span><br></pre></td></tr></table></figure></p><p>将其中的：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">#error-- unsupported GNU version! gcc versions later than 4.9 are not supported!</span><br></pre></td></tr></table></figure></p><p>更改为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">//#error-- unsupported GNU version! gcc versions later than 4.9 are not supported!</span><br></pre></td></tr></table></figure></p><p>下面就是编译<code>caffe</code>并测试：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ make clean -j8</span><br><span class="line">$ make all -j8 </span><br><span class="line">$ make runtest -j8</span><br></pre></td></tr></table></figure></p><p>最后输出<code>PASS</code>说明测试成功。<br>配置环境变量：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim ~/.bashrc</span><br></pre></td></tr></table></figure></p><p>在文件末尾写入<code>caffe-pathon</code>的安装路径：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PYTHONPATH=caffe安装路径/caffe/python:$PYTHONPATH</span><br></pre></td></tr></table></figure></p><p>上述语句中的<code>~</code>表示caffe所在的根目录。<br>是环境变量生效：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure></p><p>然后执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ make pycaffe</span><br></pre></td></tr></table></figure></p><h4 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h4><h5 id="CUDA9错误"><a href="#CUDA9错误" class="headerlink" title="CUDA9错误"></a>CUDA9错误</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">NVCC src/caffe/layers/bnll_layer.cu</span><br><span class="line">nvcc fatal   : Unsupported gpu architecture <span class="string">'compute_20'</span></span><br><span class="line">Makefile:594: recipe <span class="keyword">for</span> target <span class="string">'.build_release/cuda/src/caffe/layers/bnll_layer.o'</span> failed</span><br><span class="line">make: *** [.build_release/cuda/src/caffe/layers/bnll_layer.o] Error 1</span><br><span class="line">make: *** Waiting <span class="keyword">for</span> unfinished <span class="built_in">jobs</span>....</span><br></pre></td></tr></table></figure><h5 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h5><p>cuda9不支持‘ compute-20 ’，需要修改<code>Makefile.config</code>文件中<code>CUDA_ARCH</code>设置，将<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CUDA architecture setting: going with all of them.</span></span><br><span class="line"><span class="comment"># For CUDA &lt; 6.0, comment the *_50 through *_61 lines for compatibility.</span></span><br><span class="line"><span class="comment"># For CUDA &lt; 8.0, comment the *_60 and *_61 lines for compatibility.</span></span><br><span class="line"><span class="comment"># For CUDA &gt;= 9.0, comment the *_20 and *_21 lines for compatibility.</span></span><br><span class="line">CUDA_ARCH := -gencode arch=compute_20,code=sm_20 \</span><br><span class="line">                -gencode arch=compute_20,code=sm_21 \</span><br><span class="line">                -gencode arch=compute_30,code=sm_30 \</span><br><span class="line">                -gencode arch=compute_35,code=sm_35 \</span><br><span class="line">                -gencode arch=compute_50,code=sm_50 \</span><br><span class="line">                -gencode arch=compute_52,code=sm_52 \</span><br><span class="line">                -gencode arch=compute_60,code=sm_60 \</span><br><span class="line">                -gencode arch=compute_61,code=sm_61 \</span><br><span class="line">                -gencode arch=compute_61,code=compute_61</span><br></pre></td></tr></table></figure></p><p>中的<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-gencode arch=compute_20,code=sm_20 \</span><br><span class="line">-gencode arch=compute_20,code=sm_21 \</span><br></pre></td></tr></table></figure></p><p>删除即可重新编译。</p><h5 id="HDF5错误"><a href="#HDF5错误" class="headerlink" title="HDF5错误"></a>HDF5错误</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">src/caffe/net.cpp:8:18: fatal error: hdf5.h: No such file or directory</span><br><span class="line">compilation terminated.</span><br><span class="line">Makefile:581: recipe <span class="keyword">for</span> target <span class="string">'.build_release/src/caffe/net.o'</span> failed</span><br><span class="line">make: *** [.build_release/src/caffe/net.o] Error 1</span><br></pre></td></tr></table></figure><p>或者<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">AR -o .build_release/lib/libcaffe.a</span><br><span class="line">LD -o .build_release/lib/libcaffe.so.1.0.0</span><br><span class="line">/usr/bin/ld: cannot find -lhdf5_hl</span><br><span class="line">/usr/bin/ld: cannot find -lhdf5</span><br><span class="line">collect2: error: ld returned 1 <span class="built_in">exit</span> status</span><br><span class="line">Makefile:572: recipe <span class="keyword">for</span> target <span class="string">'.build_release/lib/libcaffe.so.1.0.0'</span> failed</span><br><span class="line">make: *** [.build_release/lib/libcaffe.so.1.0.0] Error 1</span><br></pre></td></tr></table></figure></p><h5 id="解决方案-1"><a href="#解决方案-1" class="headerlink" title="解决方案"></a>解决方案</h5><p>执行命令安装<code>libhdf5-dev</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install libhdf5-dev</span><br></pre></td></tr></table></figure></p><p>然后再重新编译。</p><h5 id="gflags错误"><a href="#gflags错误" class="headerlink" title="gflags错误"></a>gflags错误</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In file included from src/caffe/net.cpp:10:0:</span><br><span class="line">./include/caffe/common.hpp:5:27: fatal error: gflags/gflags.h: No such file or directory</span><br><span class="line">compilation terminated.</span><br><span class="line">Makefile:581: recipe <span class="keyword">for</span> target <span class="string">'.build_release/src/caffe/net.o'</span> failed</span><br><span class="line">make: *** [.build_release/src/caffe/net.o] Error 1</span><br></pre></td></tr></table></figure><h5 id="解决方案-2"><a href="#解决方案-2" class="headerlink" title="解决方案"></a>解决方案</h5><p>执行命令安装<code>gflags</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install libgflags-dev</span><br></pre></td></tr></table></figure></p><p>然后再重新编译。</p><h5 id="glog错误"><a href="#glog错误" class="headerlink" title="glog错误"></a>glog错误</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In file included from src/caffe/net.cpp:10:0:</span><br><span class="line">./include/caffe/common.hpp:6:26: fatal error: glog/logging.h: No such file or directory</span><br><span class="line">compilation terminated.</span><br><span class="line">Makefile:581: recipe <span class="keyword">for</span> target <span class="string">'.build_release/src/caffe/net.o'</span> failed</span><br><span class="line">make: *** [.build_release/src/caffe/net.o] Error 1</span><br></pre></td></tr></table></figure><h5 id="解决方案-3"><a href="#解决方案-3" class="headerlink" title="解决方案"></a>解决方案</h5><p>执行命令安装<code>glog</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install libgoogle-glog-dev</span><br></pre></td></tr></table></figure></p><p>然后再重新编译。</p><h5 id="LMDB错误"><a href="#LMDB错误" class="headerlink" title="LMDB错误"></a>LMDB错误</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In file included from src/caffe/util/db.cpp:3:0:</span><br><span class="line">./include/caffe/util/db_lmdb.hpp:8:18: fatal error: lmdb.h: No such file or directory</span><br><span class="line">compilation terminated.</span><br><span class="line">Makefile:581: recipe <span class="keyword">for</span> target <span class="string">'.build_release/src/caffe/util/db.o'</span> failed</span><br><span class="line">make: *** [.build_release/src/caffe/util/db.o] Error 1</span><br></pre></td></tr></table></figure><h5 id="解决方案-4"><a href="#解决方案-4" class="headerlink" title="解决方案"></a>解决方案</h5><p>执行命令安装<code>lmdb</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install liblmdb-dev</span><br></pre></td></tr></table></figure></p><p>然后再重新编译。</p><h5 id="opencv-imgcodecs-opencv-videoio错误"><a href="#opencv-imgcodecs-opencv-videoio错误" class="headerlink" title="opencv_imgcodecs opencv_videoio错误"></a>opencv_imgcodecs opencv_videoio错误</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/ld: cannot find -lopencv_imgcodecs</span><br><span class="line">/usr/bin/ld: cannot find -lopencv_videoio</span><br><span class="line">collect2: error: ld returned 1 <span class="built_in">exit</span> status</span><br><span class="line">Makefile:579: recipe <span class="keyword">for</span> target <span class="string">'.build_release/lib/libcaffe.so.1.0.0-rc5'</span> failed</span><br><span class="line">make: *** [.build_release/lib/libcaffe.so.1.0.0-rc5] Error 1</span><br></pre></td></tr></table></figure><h5 id="解决方案-5"><a href="#解决方案-5" class="headerlink" title="解决方案"></a>解决方案</h5><p>打开<code>Makefile</code>文件，在164行（我的文件）加上<code>opencv_imgcodecs</code>，如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">LIBRARIES += glog gflags protobuf leveldb snappy \</span><br><span class="line">  lmdb boost_system hdf5_hl hdf5 m \</span><br><span class="line">  opencv_core opencv_highgui opencv_imgproc opencv_imgcodecs</span><br></pre></td></tr></table></figure></p><p>然后再重新编译。</p><h5 id="numpy路径错误"><a href="#numpy路径错误" class="headerlink" title="numpy路径错误"></a>numpy路径错误</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">python/caffe/_caffe.cpp:10:31: fatal error: numpy/arrayobject.h: No such file or</span><br><span class="line">directory</span><br><span class="line">compilation terminated.</span><br><span class="line">make: *** [python/caffe/_caffe.so] Error 1`</span><br></pre></td></tr></table></figure><h5 id="解决方案-6"><a href="#解决方案-6" class="headerlink" title="解决方案"></a>解决方案</h5><p>打开<code>python</code>编辑器，通过命令得到<code>numpy</code>的安装路径：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>dirs = numpy.get_include()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(dirs)</span><br></pre></td></tr></table></figure></p><p>然后就能看到<code>numpy</code>的安装路径，打开<code>caffe</code>目录下的<code>Makefile.config</code>文件，将65行（我的文件）的路径：<code>/usr/lib/python2.7/dist-packages/numpy/core/include</code>换成刚刚得到<code>numpy</code>的安装路径，然后重新编译。</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;安装步骤&quot;&gt;&lt;a href=&quot;#安装步骤&quot; class=&quot;headerlink&quot; title=&quot;安装步骤&quot;&gt;&lt;/a&gt;安装步骤&lt;/h4&gt;&lt;p&gt;首先安装各种依赖包：&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev python-dev libgflags-dev libatlas-base-dev libhdf5-serial-dev protobuf-compiler&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ sudo apt-get install --no-install-recommends libboost-all-dev&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;从&lt;code&gt;github&lt;/code&gt;上面拷贝下来&lt;code&gt;caffe&lt;/code&gt;项目：&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ git &lt;span class=&quot;built_in&quot;&gt;clone&lt;/span&gt; https://github.com/BVLC/caffe.git&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ &lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; caffe&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;安装&lt;code&gt;caffe&lt;/code&gt;版的&lt;code&gt;SSD&lt;/code&gt;拷贝步骤为：&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ git &lt;span class=&quot;built_in&quot;&gt;clone&lt;/span&gt; https://github.com/weiliu89/caffe.git&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ &lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; caffe&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ git checkout ssd&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;然后将&lt;code&gt;caffe&lt;/code&gt;主目录下面的&lt;code&gt;Makefile.config.example&lt;/code&gt;拷贝更名为&lt;code&gt;Makefile.config&lt;/code&gt;，打开操作：&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ cp Makefile.config.example Makefile.config&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ gedit Makefile.config&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="环境搭建" scheme="http://wgshun.github.io/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="ubuntu" scheme="http://wgshun.github.io/tags/ubuntu/"/>
    
      <category term="caffe" scheme="http://wgshun.github.io/tags/caffe/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04 安装 cuda9 和 cudnn7</title>
    <link href="http://wgshun.github.io/2018/06/ubuntu-install-cuda-and-cudnn/"/>
    <id>http://wgshun.github.io/2018/06/ubuntu-install-cuda-and-cudnn/</id>
    <published>2018-06-06T04:20:12.000Z</published>
    <updated>2018-10-25T13:19:52.999Z</updated>
    
    <content type="html"><![CDATA[<h4 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h4><h5 id="安装-cuda"><a href="#安装-cuda" class="headerlink" title="安装 cuda"></a>安装 cuda</h5><p>首先下载 cuda9.0 <a href="https://developer.nvidia.com/cuda-toolkit-archive" target="_blank" rel="noopener">下载地址</a>的 (runfile) 安装文件，下载完成进到文件下载的目录下，给安装文件赋予权限：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo chmod 777 XXX.run <span class="comment"># XXX为安装文件的文件名</span></span><br></pre></td></tr></table></figure></p><p>执行安装文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sh XXX.run</span><br></pre></td></tr></table></figure></p><p>在看完协议选择 <code>Install NVIDIA Accelerated Graphics Driver for nvidia</code> 时，选择 <em>no</em>，其他的可全部选择 <em>yes</em> 和默认回车；并且在查看协议时有快捷键 <code>Ctrl+D</code> 进行翻页。<br>下面安装完成配置环境变量：<br>打开环境变量文件<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ gedit ~/.bashrc</span><br></pre></td></tr></table></figure></p><a id="more"></a><p>在文件末尾加上安装的 cuda9.0 路径<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=/usr/<span class="built_in">local</span>/cuda-9.0/bin:<span class="variable">$PATH</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda-9.0/lib64:<span class="variable">$LD_LIBRARY_PATH</span></span><br></pre></td></tr></table></figure></p><p>配置完环境变量，使其生效：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure></p><p>然后进行测试：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /usr/<span class="built_in">local</span>/cuda-9.0/samples/1_Utilities/deviceQuery</span><br><span class="line">$ sudo make</span><br><span class="line">$ sudo ./deviceQuery</span><br></pre></td></tr></table></figure></p><p>测试结果有输出<code>PASS</code>，即为正确安装。</p><h5 id="安装-cudnn"><a href="#安装-cudnn" class="headerlink" title="安装 cudnn"></a>安装 cudnn</h5><p>首先下载 cudnn7 <a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="noopener">下载路径</a>（<em>下载 cudnn7 需要登陆 NVIDIA ，没有 NVIDIA 账号的注册一个就行了。</em>）下载 tgz 压缩文件。<br>首先解压 tgz 压缩文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tar -xzvf cudnn-XXX.tgz</span><br></pre></td></tr></table></figure></p><p>接着复制文件到 cuda 路径下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo cp cuda/include/cudnn.h /usr/<span class="built_in">local</span>/cuda/include</span><br><span class="line">$ sudo cp cuda/lib64/libcudnn* /usr/<span class="built_in">local</span>/cuda/lib64</span><br><span class="line">$ sudo chmod a+r /usr/<span class="built_in">local</span>/cuda/include/cudnn.h /usr/<span class="built_in">local</span>/cuda/lib64/libcudnn*</span><br></pre></td></tr></table></figure></p><p>复制完成然后使用命令，可以看到输出相应的 cuda 和 cudnn 的版本信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nvcc -V</span><br></pre></td></tr></table></figure></p><hr><p>如果提示库缺失错误，可参考以下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo cp /usr/<span class="built_in">local</span>/cuda-9.0/lib64/libcudart.so.9.0 /usr/<span class="built_in">local</span>/lib/libcudart.so.9.0 &amp;&amp; sudo ldconfig</span><br><span class="line">$ sudo cp /usr/<span class="built_in">local</span>/cuda-9.0/lib64/libcublas.so.9.0 /usr/<span class="built_in">local</span>/lib/libcublas.so.9.0 &amp;&amp; sudo ldconfig</span><br><span class="line">$ sudo cp /usr/<span class="built_in">local</span>/cuda-9.0/lib64/libcurand.so.9.0 /usr/<span class="built_in">local</span>/lib/libcurabd.so.9.0 &amp;&amp; sudo ldconfig</span><br><span class="line">$ sudo cp /usr/<span class="built_in">local</span>/cuda-9.0/lib64/libcudnn.so.7 /usr/<span class="built_in">local</span>/lib/libcudnn.so.7 &amp;&amp; sudo ldconfig</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;安装步骤&quot;&gt;&lt;a href=&quot;#安装步骤&quot; class=&quot;headerlink&quot; title=&quot;安装步骤&quot;&gt;&lt;/a&gt;安装步骤&lt;/h4&gt;&lt;h5 id=&quot;安装-cuda&quot;&gt;&lt;a href=&quot;#安装-cuda&quot; class=&quot;headerlink&quot; title=&quot;安装 cuda&quot;&gt;&lt;/a&gt;安装 cuda&lt;/h5&gt;&lt;p&gt;首先下载 cuda9.0 &lt;a href=&quot;https://developer.nvidia.com/cuda-toolkit-archive&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;下载地址&lt;/a&gt;的 (runfile) 安装文件，下载完成进到文件下载的目录下，给安装文件赋予权限：&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ sudo chmod 777 XXX.run &lt;span class=&quot;comment&quot;&gt;# XXX为安装文件的文件名&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;执行安装文件：&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ sudo sh XXX.run&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;在看完协议选择 &lt;code&gt;Install NVIDIA Accelerated Graphics Driver for nvidia&lt;/code&gt; 时，选择 &lt;em&gt;no&lt;/em&gt;，其他的可全部选择 &lt;em&gt;yes&lt;/em&gt; 和默认回车；并且在查看协议时有快捷键 &lt;code&gt;Ctrl+D&lt;/code&gt; 进行翻页。&lt;br&gt;下面安装完成配置环境变量：&lt;br&gt;打开环境变量文件&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ gedit ~/.bashrc&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="环境搭建" scheme="http://wgshun.github.io/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="ubuntu" scheme="http://wgshun.github.io/tags/ubuntu/"/>
    
      <category term="cuda" scheme="http://wgshun.github.io/tags/cuda/"/>
    
      <category term="cudnn" scheme="http://wgshun.github.io/tags/cudnn/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04 python2 安装 OpenCV2.4.13.6</title>
    <link href="http://wgshun.github.io/2018/06/ubuntu-install-opencv/"/>
    <id>http://wgshun.github.io/2018/06/ubuntu-install-opencv/</id>
    <published>2018-06-04T02:31:46.000Z</published>
    <updated>2019-01-16T13:17:51.996Z</updated>
    
    <content type="html"><![CDATA[<p>事先装有 OpenCV 需要重新安装的，先执行卸载步骤，再安装。</p><h4 id="卸载步骤"><a href="#卸载步骤" class="headerlink" title="卸载步骤"></a>卸载步骤</h4><p>1.找到当初安装 OpenCV 的 release 或 build 目录，执行以下命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo make uninstall</span><br><span class="line"><span class="built_in">cd</span> ..</span><br><span class="line">sudo rm -r build</span><br><span class="line">sudo rm -r /usr/<span class="built_in">local</span>/include/opencv2 /usr/<span class="built_in">local</span>/include/opencv /usr/include/opencv /usr/include/opencv2 /usr/<span class="built_in">local</span>/share/opencv /usr/<span class="built_in">local</span>/share/OpenCV /usr/share/opencv /usr/share/OpenCV /usr/<span class="built_in">local</span>/bin/opencv* /usr/<span class="built_in">local</span>/lib/libopencv*</span><br></pre></td></tr></table></figure></p><p>2.删除 usr 文件夹中所有 opencv 相关项<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/</span><br><span class="line">find . -name <span class="string">"*opencv*"</span> | xargs sudo rm -rf</span><br></pre></td></tr></table></figure></p><p>3.移除 Python 相关<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apt-get remove opencv-doc opencv-data python-opencv</span><br></pre></td></tr></table></figure></p><h4 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h4><p>1.通过命令安装各种软件包<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install build-essential</span><br><span class="line">$ sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev</span><br><span class="line">$ sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev</span><br></pre></td></tr></table></figure></p><p>2.进到安装路径下拷贝 OpenCV 源码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git <span class="built_in">clone</span> https://github.com/opencv/opencv.git</span><br></pre></td></tr></table></figure></p><p>3.选择安装的 OpenCV 版本号<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> opencv</span><br><span class="line">$ git checkout -b 2.4 origin/2.4</span><br></pre></td></tr></table></figure></p><a id="more"></a><p>4.使用 Cmake 编译 OpenCV 源码<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir release</span><br><span class="line">$ <span class="built_in">cd</span> release</span><br><span class="line">$ cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/<span class="built_in">local</span> ..</span><br></pre></td></tr></table></figure></p><p>5.make 安装 OpenCV<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ make -j4</span><br><span class="line">$ sudo make install</span><br></pre></td></tr></table></figure></p><p>安装到此结束。</p><h5 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h5><p>命令进入 Python 编辑器<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> cv2</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(cv2.__version__)</span><br></pre></td></tr></table></figure></p><p>会输出 OpenCV 的版本号。</p><h4 id="cuda-9-0-与-opencv-版本匹配问题："><a href="#cuda-9-0-与-opencv-版本匹配问题：" class="headerlink" title="cuda 9.0 与 opencv 版本匹配问题："></a>cuda 9.0 与 opencv 版本匹配问题：</h4><p>Cmake 过程中会出现以下问题：</p><h5 id="错误"><a href="#错误" class="headerlink" title="错误"></a>错误</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CMake Error: The following variables are used <span class="keyword">in</span> this project, but they are <span class="built_in">set</span> to NOTFOUND.</span><br><span class="line">Please <span class="built_in">set</span> them or make sure they are <span class="built_in">set</span> and tested correctly <span class="keyword">in</span> the CMake files:</span><br><span class="line">CUDA_nppi_LIBRARY (ADVANCED)</span><br></pre></td></tr></table></figure><h5 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h5><p>在cuda9里面，NVIDIA把 <code>libnppi.so</code>换成<code>libnppc.solibnppial.solibnppicc.solibnppicom.solibnppidei.solibnppif.solibnppig.solibnppim.solibnppist.solibnppisu.solibnppitc.solibnpps.so</code></p><h5 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h5><p>修改opencv/cmake/FindCUDA.cmake 文件，将其中的<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">unset</span>(CUDA_nppi_LIBRARY CACHE)</span><br></pre></td></tr></table></figure></p><p>替换为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">unset</span>(CUDA_nppial_LIBRARY CACHE)</span><br><span class="line"><span class="built_in">unset</span>(CUDA_nppicc_LIBRARY CACHE)</span><br><span class="line"><span class="built_in">unset</span>(CUDA_nppicom_LIBRARY CACHE)</span><br><span class="line"><span class="built_in">unset</span>(CUDA_nppidei_LIBRARY CACHE)</span><br><span class="line"><span class="built_in">unset</span>(CUDA_nppif_LIBRARY CACHE)</span><br><span class="line"><span class="built_in">unset</span>(CUDA_nppig_LIBRARY CACHE)</span><br><span class="line"><span class="built_in">unset</span>(CUDA_nppim_LIBRARY CACHE)</span><br><span class="line"><span class="built_in">unset</span>(CUDA_nppist_LIBRARY CACHE)</span><br><span class="line"><span class="built_in">unset</span>(CUDA_nppisu_LIBRARY CACHE)</span><br><span class="line"><span class="built_in">unset</span>(CUDA_nppitc_LIBRARY CACHE)</span><br></pre></td></tr></table></figure></p><p>将<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">find_cuda_helper_libs(nppi)</span><br><span class="line"><span class="built_in">set</span>(CUDA_npp_LIBRARY<span class="string">"<span class="variable">$&#123;CUDA_nppc_LIBRARY&#125;</span>;<span class="variable">$&#123;CUDA_nppi_LIBRARY&#125;</span>;<span class="variable">$&#123;CUDA_npps_LIBRARY&#125;</span>"</span>)</span><br></pre></td></tr></table></figure></p><p>替换为<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">find_cuda_helper_libs(nppial)</span><br><span class="line">find_cuda_helper_libs(nppicc)</span><br><span class="line">find_cuda_helper_libs(nppicom)</span><br><span class="line">find_cuda_helper_libs(nppidei)</span><br><span class="line">find_cuda_helper_libs(nppif)</span><br><span class="line">find_cuda_helper_libs(nppig)</span><br><span class="line">find_cuda_helper_libs(nppim)</span><br><span class="line">find_cuda_helper_libs(nppist)</span><br><span class="line">find_cuda_helper_libs(nppisu)</span><br><span class="line">find_cuda_helper_libs(nppitc)</span><br><span class="line"><span class="built_in">set</span>(CUDA_npp_LIBRARY <span class="string">"<span class="variable">$&#123;CUDA_nppc_LIBRARY&#125;</span>;<span class="variable">$&#123;CUDA_nppial_LIBRARY&#125;</span>;<span class="variable">$&#123;CUDA_nppicc_LIBRARY&#125;</span>;<span class="variable">$&#123;CUDA_nppicom_LIBRARY&#125;</span>;<span class="variable">$&#123;CUDA_nppidei_LIBRARY&#125;</span>;<span class="variable">$&#123;CUDA_nppif_LIBRARY&#125;</span>;<span class="variable">$&#123;CUDA_nppig_LIBRARY&#125;</span>;<span class="variable">$&#123;CUDA_nppim_LIBRARY&#125;</span>;<span class="variable">$&#123;CUDA_nppist_LIBRARY&#125;</span>;<span class="variable">$&#123;CUDA_nppisu_LIBRARY&#125;</span>;<span class="variable">$&#123;CUDA_nppitc_LIBRARY&#125;</span>;<span class="variable">$&#123;CUDA_npps_LIBRARY&#125;</span>"</span>)</span><br></pre></td></tr></table></figure></p><h5 id="错误-1"><a href="#错误-1" class="headerlink" title="错误"></a>错误</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">opencv nvcc fatal   : Unsupported gpu architecture <span class="string">'compute_20'</span></span><br></pre></td></tr></table></figure><h5 id="原因-1"><a href="#原因-1" class="headerlink" title="原因"></a>原因</h5><p>cuda9不支持‘ compute-20 ’</p><h5 id="解决方案："><a href="#解决方案：" class="headerlink" title="解决方案："></a>解决方案：</h5><p>更改 OpenCVDetectCUDA.cmake 文件，把有关 ‘ compute-20 ’ 的全删掉<br>将<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(CUDA_GENERATION STREQUAL <span class="string">"Fermi"</span>)</span><br><span class="line">    <span class="built_in">set</span>(__cuda_arch_bin <span class="string">"3.0 3.5"</span>)</span><br><span class="line">  elseif(CUDA_GENERATION STREQUAL <span class="string">"Kepler"</span>)</span><br><span class="line">    <span class="keyword">if</span>(<span class="variable">$&#123;CUDA_VERSION&#125;</span> VERSION_LESS <span class="string">"5.0"</span>)</span><br><span class="line">      <span class="built_in">set</span>(__cuda_arch_bin <span class="string">"2.0 2.1"</span>)</span><br><span class="line">    <span class="keyword">else</span>()</span><br><span class="line">      <span class="built_in">set</span>(__cuda_arch_bin <span class="string">"3.0 3.5"</span>)</span><br></pre></td></tr></table></figure></p><p>替换为<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(CUDA_GENERATION STREQUAL <span class="string">"Fermi"</span>)</span><br><span class="line">    <span class="built_in">set</span>(__cuda_arch_bin <span class="string">"3.0 3.5"</span>)</span><br><span class="line">  elseif(CUDA_GENERATION STREQUAL <span class="string">"Kepler"</span>)</span><br><span class="line">    <span class="keyword">if</span>(<span class="variable">$&#123;CUDA_VERSION&#125;</span> VERSION_LESS <span class="string">"5.0"</span>)</span><br><span class="line">      <span class="built_in">set</span>(__cuda_arch_bin <span class="string">"3.0"</span>)</span><br><span class="line">    <span class="keyword">else</span>()</span><br><span class="line">      <span class="built_in">set</span>(__cuda_arch_bin <span class="string">"3.0 3.5"</span>)</span><br></pre></td></tr></table></figure></p><p>将：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(<span class="variable">$&#123;CUDA_VERSION&#125;</span> VERSION_LESS <span class="string">"5.0"</span>)</span><br><span class="line">     <span class="built_in">set</span>(__cuda_arch_bin <span class="string">"1.1 1.2 1.3 2.0 2.1(2.0) 3.0"</span>)</span><br><span class="line">elseif(<span class="variable">$&#123;CUDA_VERSION&#125;</span> VERSION_GREATER <span class="string">"6.5"</span>)</span><br><span class="line">     <span class="built_in">set</span>(__cuda_arch_bin <span class="string">"2.0 2.1(2.0) 3.0 3.5"</span>)</span><br></pre></td></tr></table></figure></p><p>替换为：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>(<span class="variable">$&#123;CUDA_VERSION&#125;</span> VERSION_LESS <span class="string">"5.0"</span>)</span><br><span class="line">        <span class="built_in">set</span>(__cuda_arch_bin <span class="string">"1.1 1.2 1.3 2.0 2.1(2.0) 3.0"</span>)</span><br><span class="line">elseif(<span class="variable">$&#123;CUDA_VERSION&#125;</span> VERSION_GREATER <span class="string">"6.5"</span>)</span><br><span class="line">        <span class="built_in">set</span>(__cuda_arch_bin <span class="string">"3.0 3.5"</span>)</span><br></pre></td></tr></table></figure></p><p>然后 cmake 成功。</p><h5 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h5><p><a href="https://docs.opencv.org/2.4/doc/tutorials/introduction/linux_install/linux_install.html" target="_blank" rel="noopener">https://docs.opencv.org/2.4/doc/tutorials/introduction/linux_install/linux_install.html</a></p><p><a href="https://stackoverflow.com/questions/46584000/cmake-error-variables-are-set-to-notfound" target="_blank" rel="noopener">https://stackoverflow.com/questions/46584000/cmake-error-variables-are-set-to-notfound</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;事先装有 OpenCV 需要重新安装的，先执行卸载步骤，再安装。&lt;/p&gt;
&lt;h4 id=&quot;卸载步骤&quot;&gt;&lt;a href=&quot;#卸载步骤&quot; class=&quot;headerlink&quot; title=&quot;卸载步骤&quot;&gt;&lt;/a&gt;卸载步骤&lt;/h4&gt;&lt;p&gt;1.找到当初安装 OpenCV 的 release 或 build 目录，执行以下命令&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;sudo make uninstall&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; ..&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo rm -r build&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;sudo rm -r /usr/&lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt;/include/opencv2 /usr/&lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt;/include/opencv /usr/include/opencv /usr/include/opencv2 /usr/&lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt;/share/opencv /usr/&lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt;/share/OpenCV /usr/share/opencv /usr/share/OpenCV /usr/&lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt;/bin/opencv* /usr/&lt;span class=&quot;built_in&quot;&gt;local&lt;/span&gt;/lib/libopencv*&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;2.删除 usr 文件夹中所有 opencv 相关项&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; /usr/&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;find . -name &lt;span class=&quot;string&quot;&gt;&quot;*opencv*&quot;&lt;/span&gt; | xargs sudo rm -rf&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;3.移除 Python 相关&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;apt-get remove opencv-doc opencv-data python-opencv&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;h4 id=&quot;安装步骤&quot;&gt;&lt;a href=&quot;#安装步骤&quot; class=&quot;headerlink&quot; title=&quot;安装步骤&quot;&gt;&lt;/a&gt;安装步骤&lt;/h4&gt;&lt;p&gt;1.通过命令安装各种软件包&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ sudo apt-get install build-essential&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ sudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ sudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;2.进到安装路径下拷贝 OpenCV 源码&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ git &lt;span class=&quot;built_in&quot;&gt;clone&lt;/span&gt; https://github.com/opencv/opencv.git&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;3.选择安装的 OpenCV 版本号&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ &lt;span class=&quot;built_in&quot;&gt;cd&lt;/span&gt; opencv&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ git checkout -b 2.4 origin/2.4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="环境搭建" scheme="http://wgshun.github.io/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="ubuntu" scheme="http://wgshun.github.io/tags/ubuntu/"/>
    
      <category term="cuda" scheme="http://wgshun.github.io/tags/cuda/"/>
    
      <category term="OpenCV" scheme="http://wgshun.github.io/tags/OpenCV/"/>
    
  </entry>
  
  <entry>
    <title>Ubuntu16.04 安装 NVIDIA 驱动</title>
    <link href="http://wgshun.github.io/2018/06/ubuntu-install-nvidia-drive/"/>
    <id>http://wgshun.github.io/2018/06/ubuntu-install-nvidia-drive/</id>
    <published>2018-06-04T02:31:46.000Z</published>
    <updated>2018-10-25T13:22:20.251Z</updated>
    
    <content type="html"><![CDATA[<h5 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h5><p>   在安装了NVIDIA驱动后出现了进入Ubuntu循环登录的问题。</p><h5 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h5><p>1.进入命令行界面<br><code>Ctrl+Alt+F1</code><br>打开编辑配置文件：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim /etc/modprobe.d/blacklist.conf</span><br></pre></td></tr></table></figure></p><p>在最后一行添加：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">blacklist nouveau</span><br></pre></td></tr></table></figure></p><p>2.禁用 nouveau 第三方驱动，之后也不需要改回来<br>执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo update-initramfs -u</span><br><span class="line">$ lsmod | grep nouveau</span><br></pre></td></tr></table></figure></p><a id="more"></a><p>没有输出即屏蔽好了。</p><p>3.禁用X服务，执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo /etc/init.d/lightdm stop</span><br></pre></td></tr></table></figure></p><p>4.给驱动run文件赋予执行权限<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo chmod a+x NVIDIA***.run</span><br></pre></td></tr></table></figure></p><p>安装(注意 参数)<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ./NVIDIA***.run –no-opengl-files</span><br></pre></td></tr></table></figure></p><p>5.启用X服务，执行：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo /etc/init.d/lightdm stop</span><br></pre></td></tr></table></figure></p><p>如果还无法进入桌面，这是因为驱动修改了xorg的配置，可执行一下命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /usr/share/X11/xorg.conf.d/ </span><br><span class="line">$ sudo mv nvidia-drm-outputclass.conf nvidia-drm-outputclass.conf.bak</span><br></pre></td></tr></table></figure></p><h5 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h5><p><a href="http://blog.csdn.net/u012759136/article/details/53355781" target="_blank" rel="noopener">http://blog.csdn.net/u012759136/article/details/53355781</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h5 id=&quot;问题描述&quot;&gt;&lt;a href=&quot;#问题描述&quot; class=&quot;headerlink&quot; title=&quot;问题描述&quot;&gt;&lt;/a&gt;问题描述&lt;/h5&gt;&lt;p&gt;   在安装了NVIDIA驱动后出现了进入Ubuntu循环登录的问题。&lt;/p&gt;
&lt;h5 id=&quot;解决方案&quot;&gt;&lt;a href=&quot;#解决方案&quot; class=&quot;headerlink&quot; title=&quot;解决方案&quot;&gt;&lt;/a&gt;解决方案&lt;/h5&gt;&lt;p&gt;1.进入命令行界面&lt;br&gt;&lt;code&gt;Ctrl+Alt+F1&lt;/code&gt;&lt;br&gt;打开编辑配置文件：&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ vim /etc/modprobe.d/blacklist.conf&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;在最后一行添加：&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;blacklist nouveau&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;2.禁用 nouveau 第三方驱动，之后也不需要改回来&lt;br&gt;执行：&lt;br&gt;&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;$ sudo update-initramfs -u&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;$ lsmod | grep nouveau&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="环境搭建" scheme="http://wgshun.github.io/categories/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="ubuntu" scheme="http://wgshun.github.io/tags/ubuntu/"/>
    
      <category term="nvidia驱动" scheme="http://wgshun.github.io/tags/nvidia%E9%A9%B1%E5%8A%A8/"/>
    
  </entry>
  
</feed>
